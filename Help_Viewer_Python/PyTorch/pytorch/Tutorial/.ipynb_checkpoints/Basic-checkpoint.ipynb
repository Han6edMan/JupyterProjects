{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "PyTorch 中张量与 NumPy 的 ndarray 类似，只是张量可以利用 gpu 或其他专用硬件上来加速计算；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tensor Initialization\n",
    "```python\n",
    "# Directly from data\n",
    "x = torch.tensor([[1, 2], [3, 4]], dtype=torch.int32, device=\"cpu\")\n",
    "# Using built-in initializers\n",
    "x = torch.empty(3, 4, dtype=torch.float)\n",
    "x = torch.zeros(5, 3, dtype=torch.int8)\n",
    "x = torch.ones(3, 4, requires_grad=True)\n",
    "x = torch.rand(2, 3)  # x ~ U(0, 1)\n",
    "x = torch.randn(3, 2)  # x ~ N(0, 1)\n",
    "x = torch.randint(low=1, high=10, size=(2, 3))\n",
    "# From another tensor\n",
    "x = torch.ones_like(x)\n",
    "x = torch.zeros_like(x)\n",
    "x = torch.randn_like(x)\n",
    "x = torch.rand_like(x)\n",
    "x = torch.randint_like(x)\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tensor Attributes\n",
    "\n",
    "**! TODO**\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored.\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "Out:\n",
    "\n",
    "Shape of tensor: torch.Size([3, 4])\n",
    "Datatype of tensor: torch.float32\n",
    "Device tensor is stored on: cpu\n",
    "### Tensor Operations\n",
    "PyTorch 中对张量操作有超过 100 多种，包括转置、数学操作、线性代数、随机抽样，更多内容参见[这里](https://pytorch.org/docs/stable/torch.html)\n",
    "\n",
    "```python\n",
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "Try out some of the operations from the list. If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.\n",
    "\n",
    "Standard numpy-like indexing and slicing:\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining op that is subtly different from torch.cat.\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "Multiplying tensors\n",
    "\n",
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")\n",
    "Out:\n",
    "\n",
    "tensor.mul(tensor)\n",
    " tensor([[1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.]])\n",
    "\n",
    "tensor * tensor\n",
    " tensor([[1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.]])\n",
    "This computes the matrix multiplication between two tensors\n",
    "\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n",
    "Out:\n",
    "\n",
    "tensor.matmul(tensor.T)\n",
    " tensor([[3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.]])\n",
    "\n",
    "tensor @ tensor.T\n",
    " tensor([[3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.],\n",
    "        [3., 3., 3., 3.]])\n",
    "In-place operations Operations that have a _ suffix are in-place. For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "Out:\n",
    "\n",
    "tensor([[1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.],\n",
    "        [1., 0., 1., 1.]])\n",
    "\n",
    "tensor([[6., 5., 6., 6.],\n",
    "        [6., 5., 6., 6.],\n",
    "        [6., 5., 6., 6.],\n",
    "        [6., 5., 6., 6.]])\n",
    "NOTE\n",
    "\n",
    "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.\n",
    "```\n",
    "\n",
    "### Bridge with NumPy\n",
    "利用`<tensor>.numpy()`函数即可将张量转换为 NumPy 数组，利用`torch.from_numpy()`函数即可将 NumPy 数组转换为张量；两种情况下得到的数组和张量绑定在同一数值上，但它们并不共享内存地址；示例如下：\n",
    "```python\n",
    "tentor_ = torch.ones(5)\n",
    "ndarray_ = torch.numpy()\n",
    "tentor_.add_(1)\n",
    "assert np.all(ndarray_ == [1, 1, 1, 1, 1])\n",
    "\n",
    "ndarray_ = np.zeros(5)\n",
    "tentor_ = torch.from_numpy(ndarray_)\n",
    "np.add(ndarray_, 1, out=ndarray_)\n",
    "assert \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770347248064 1770351449744\n"
     ]
    }
   ],
   "source": [
    "tentor_ = torch.ones(5, dtype=torch.int32)\n",
    "ndarray_ = tentor_.numpy()\n",
    "tentor_.add_(1)\n",
    "assert np.all(ndarray_ == [2, 2, 2, 2, 2])\n",
    "\n",
    "ndarray_ = np.zeros(3, dtype=np.int32)\n",
    "tentor_ = torch.from_numpy(ndarray_)\n",
    "np.add(ndarray_, 1, out=ndarray_)\n",
    "assert (tentor_ == torch.tensor([1, 1, 1])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=10>初始化</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.empty(3, 4, dtype=torch.float)\n",
    "x = torch.zeros(5, 3, dtype=torch.int8)\n",
    "x = torch.ones_like(x)\n",
    "x = torch.ones(3, 4)\n",
    "x = torch.tensor([3, 4, 6], dtype=torch.int32, device=\"cpu\")\n",
    "x = x.new_ones(3, 4)\n",
    "# print(x)\n",
    "x = torch.rand(4, 4)  # U(0, 1)\n",
    "x = torch.randn(4, 4)\n",
    "x = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: `x = x.new_ones()`\n",
    "\n",
    "new_ones(size, dtype=None, device=None, requires_grad=False) $\\rightarrow$ Tensor\n",
    "\n",
    "By default, the returned Tensor has the same `torch.dtype` and `torch.device` as this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_usqz_1 = torch.unsqueeze(x, dim=1)\n",
    "x_usqz_0 = torch.unsqueeze(x, dim=0)\n",
    "x_usqz_m1 = torch.unsqueeze(x, dim=-1)\n",
    "x_usqz_m2 = torch.unsqueeze(x, dim=-2)\n",
    "# 可以从shape看出不同dim的区别\n",
    "print(x.shape)\n",
    "print(x_usqz_1.size())\n",
    "print(x_usqz_0.shape)\n",
    "print(x_usqz_m1.size())\n",
    "print(x_usqz_m2.size())\n",
    "print(x)\n",
    "print(x_usqz_1)\n",
    "print(x_usqz_0)\n",
    "print(x_usqz_m1)\n",
    "print(x_usqz_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.FloatTensor(x)  # 转换成float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((3, 4))\n",
    "y = torch.ones_like(x)\n",
    "# print(x + y)\n",
    "# print(torch.add(x, y))\n",
    "# print(x.add_(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.sum()\n",
    "`sumsum(input, dim, keepdim=False, dtype=None)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "- input, dtype: 略\n",
    "- dim: 求和时，求和索引所在的维度标号\n",
    "- keepdim: 为``True``时，输出张量与原张量形状维度相同，否则会比原张量低至少 1 维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((3, 4, 5))\n",
    "print(torch.sum(x, dim=0, keepdim=True).shape)\n",
    "print(torch.sum(x, dim=1, keepdim=True).shape)\n",
    "\n",
    "print(x.sum((0, 1), keepdim=True))\n",
    "print(x.sum((0, 2), keepdim=True))\n",
    "print(x.sum((1, 0), keepdim=True))\n",
    "print(x.sum((1, 2), keepdim=True))\n",
    "print(x.sum((2, 0), keepdim=True))\n",
    "print(x.sum((2, 1), keepdim=True))\n",
    "\n",
    "print(x.sum(2, keepdim=True).sum(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch & NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_data = np.arange(16).reshape((4, 4))\n",
    "\n",
    "# np to torch\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "\n",
    "# torch to np\n",
    "torch_2_np = torch_data.numpy()\n",
    "\n",
    "print(\n",
    "    \"np_data:\\n\", np_data,\n",
    "    \"\\n\\ntorch_data:\\n\", torch_data,\n",
    "    \"\\n\\ntorch_2_np:\\n\", torch_2_np\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch_data = torch_data.flatten()\n",
    "print(\n",
    "    \"np:\\n\", np_data.dot(np_data),\n",
    "    \"\\n\\ntorch:\\n\", torch_data.dot(torch_data)  # Torch的dot只能用于一维数组\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=8>常用函数</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `x = y.view(*shape)`\n",
    "\n",
    "返回一个与 y 数值相同，但形状为指定形状的 tensor x，而 x 必须与 y 的形状和 stride 兼容，Otherwise, `contiguous` needs to be called before the tensor can be\n",
    "viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(range(24)).reshape(3, 8)\n",
    "print(x.view(3, 2, 4))\n",
    "print(x.view(3, 2, 4, 1))\n",
    "print(x.view(-1, 6))  # ‘-1’可自动推断维度\n",
    "print(x.view(1, -1))  # .view(1, n) 返回拉直的二维数组\n",
    "print(x.view(-1))  # 仅有 -1 时返回拉直的一维数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### `contiguous(memory_format=torch.contiguous_format)`\n",
    "\n",
    "Returns a contiguous in memory tensor containing the same data as `self` tensor. If `self` tensor is already in the specified memory format, this function returns the `self` tensor.\n",
    "\n",
    "Args:\n",
    "    memory_format (class:`torch.memory_format`): the desired memory format of returned Tensor.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.expand()\n",
    "`<tensor>.expand(*sizes)`\n",
    "\n",
    "返回一个由`<tensor>`张成的新张量。\n",
    "\n",
    "当`<tensor>`有一个维度为 1 时，例如 dim=1 的维度上，则可以在 dim=1 的纬度上扩展为更**大**的维度，即原 dim=1 维度值可以由 1 增大至任何整数；此外可以将`<tensor>`扩展为更**高**维的张量，但新张量的 -1, ..., -n, ( n=`len(<tensor>.shape)` )维上的维度值中非 1 的维度值应与 `<tensor>` 的维度中对应值相等\n",
    "\n",
    "当`<tensor>`没有维度为 1 时，其只能被扩展为更**高**维的张量，其原则与上相同；\n",
    "\n",
    "`*sizes`某一维为 -1 时，表示不改变该维的值，当把`<tensor>`扩展为更**高**维的张量时，第一个维数的值不能为 -1；\n",
    "\n",
    "扩展一个张量不会分配新的内存，而只会在现有张量上创建一个新的视图(new view)，在这个视图中通过将`stride`设为0，来将尺寸为1的维度扩展为更大的尺寸。大小为 1 的任何维度都可以扩展为任意值，而不需要分配新的内存，故在对变量进行赋值等操作时，**注意名词绑定的问题**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([17])\n",
    "print(x.expand(7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1]])\n",
    "print(x.expand(7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 3, 4])\n",
    "print(x.expand(7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 3, 4])\n",
    "print(x.expand(2, 7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2], [3], [4]])\n",
    "print(x.expand(2, 2, 3, 7))  # 必须为 expand(n1, n1, 3, n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "print(x.expand(2, 2, 3, 5))  # 必须为 expand(n1, n1, 3, 5)\n",
    "print(x.expand(3, -1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.expand_as()\n",
    "`expand_as(other)`\n",
    "\n",
    "**说明**\n",
    "\n",
    "返回一个和`other`形状相同的tensor，更多有关``expand``详见`torch.Tensor.expand`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3, 4], [3, 4, 5, 6], [4, 5, 7, 7]])\n",
    "b = torch.tensor([[1, 3, 3, 1], [4, 4, 5, 6], [4, 5, 6, 9]])\n",
    "c = a.eq(b)\n",
    "# c = c.view(-1).float().sum(0, keepdim=True)\n",
    "c = c.view(-1).float()\n",
    "print(type(c))\n",
    "print(c[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA张量\n",
    "可以利用`x.to(<device>)`来制定将张量移动至特定设备上运行\n",
    "\n",
    "以下代码在GPU可用时的预期输出为：\n",
    "```\n",
    "tensor([-0.4181], device='cuda:0')\n",
    "tensor([-0.4181], dtype=torch.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)  # or to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))  # `.to()` can also change dtype together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = [[[[1, 0.23], [4, 0.24]], 0.6],\n",
    "     [[[1, 0.23], [4, 0.24]], 0.8],\n",
    "     [[[1, 0.23], [4, 0.24]], 0.8]]\n",
    "x = np.array(x)\n",
    "print(type(x[:2, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
