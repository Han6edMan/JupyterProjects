{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd\n",
    "\n",
    "**Desciption**\n",
    "\n",
    "``torch.autograd``提供了能够对任意标量函数自动求导的类和函数；目前支持自动求导的`Tensor`数据类型有浮点型 (half, float, double, bfloat16) 和复数型 (cfloat, cdouble)\n",
    "\n",
    "**PACKAGE CONTENTS**\n",
    "\n",
    "- \\_functions (package)\n",
    "- anomaly_mode\n",
    "- function\n",
    "- functional\n",
    "- grad_mode\n",
    "- gradcheck\n",
    "- profiler\n",
    "- variable\n",
    "\n",
    "**CLASSES**\n",
    "- autograd.function.\\_ContextMethodMixin(builtins.object)\n",
    "- autograd.function.\\_HookMixin(builtins.object)\n",
    "- torch.\\_C.\\_FunctionBase(builtins.object)\n",
    "    - autograd.function.Function(<br>\n",
    "        torch.\\_C.\\_FunctionBase,<br>\n",
    "        autograd.function.\\_ContextMethodMixin,<br>\n",
    "        autograd.function.\\_HookMixin)\n",
    "- torch.\\_C.\\_LegacyVariableBase(builtins.object)\n",
    "    - autograd.variable.Variable\n",
    "\n",
    "\n",
    "**FUNCTIONS**\n",
    "\n",
    "- backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n",
    "\n",
    "**DATA**\n",
    "- \\_\\_all__ = ['Variable', 'Function', 'backward', 'grad_mode']\n",
    "\n",
    "**FILE**: \\torch\\autograd\\\\\\_\\_init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  autograd包\n",
    "这是一个 define-by-run framework，这意味着反向传播由 how your code is run 来定义，并且每一轮迭代都可能不同。\n",
    "#### Tensor\n",
    "`torch.Tensor`是这个包的 central package，当设定某变量的属性`requires_grad=True` 时，pytorch 便会追踪该变量上的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5000, grad_fn=<MeanBackward0>)\n",
      "<class 'MeanBackward0'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "x.requires_grad_(True)\n",
    "out = torch.mean(x * x)\n",
    "print(out)\n",
    "print(out.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用`<输出变量>.backward()`函数以自动完成backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于中间某一变量，可用`<变量名>.grad`属性来得到该变量处的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于网络输出值为一大于1维的张量的情况，如$y=(y_1,y_2,\\cdots,y_m)^T$，`out.backward()`执行计算\n",
    "$$v_T\\cdot J$$\n",
    "其中 $v$ 为人为取定，\n",
    "$$J=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)$$\n",
    "其意义为，若$v$巧好是某个标量函数$l=g(\\vec{y})$的梯度，即 $v=\\left(\\frac{\\partial l}{\\partial y_{1}} \\cdots \\frac{\\partial l}{\\partial y_{m}}\\right)^{T}$，则 $$J^{T} \\cdot v=\\left(\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_{1}}{\\partial x n} & \\cdots & \\frac{\\partial y_{m}}{\\partial x n}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial y_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial y_{m}}\n",
    "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial l}{\\partial x_{1}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial x n}\n",
    "\\end{array}\\right)$$\n",
    "即为函数 $l$ 对 $\\vec{x}$ 的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x*x\n",
    "v = torch.tensor([1., 1/2., 1/3.])\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若不需要某变量追踪历史，可以通过 `<变量名>.detach()`to detach it from the computation history，也可以用`with torch.no_grad():`实现。该操作在评估网络性能很有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.requires_grad)\n",
    "print(x.eq(y).all())\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "在 PyTorch 中，自动求导机制主要依靠`torch.autograd` API 来完成；利用自动求导机制，模型会在前向传播时自动记录每个单位模块处的导数值；模型在反向传播时，只需将先前计算的所有梯度值按照求导的链式法则进行计算，便可得到目标参数处的导数，以用于参数更新，进而完成反向传播整个过程；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
