{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch -- Overview\n",
    "\n",
    "DESCRIPTION\n",
    "The torch package contains data structures for multi-dimensional\n",
    "tensors and mathematical operations over these are defined.\n",
    "Additionally, it provides many utilities for efficient serializing of\n",
    "Tensors and arbitrary types, and other useful utilities.\n",
    "\n",
    "It has a CUDA counterpart, that enables you to run your tensor computations\n",
    "on an NVIDIA GPU with compute capability >= 3.0.\n",
    "\n",
    "PACKAGE CONTENTS\n",
    "    \\_C\n",
    "    \\_VF\n",
    "    \\_\\_config__\n",
    "    \\_\\_future__\n",
    "    \\_appdirs\n",
    "    \\_classes\n",
    "    \\_jit_internal\n",
    "    \\_linalg_utils\n",
    "    \\_lobpcg\n",
    "    \\_lowrank\n",
    "    \\_namedtensor_internals\n",
    "    \\_ops\n",
    "    \\_overrides\n",
    "    \\_six\n",
    "    \\_storage_docs\n",
    "    \\_tensor_docs\n",
    "    \\_tensor_str\n",
    "    \\_torch_docs\n",
    "    \\_utils\n",
    "    \\_utils_internal\n",
    "    autograd (package)\n",
    "    backends (package)\n",
    "    contrib (package)\n",
    "    cuda (package)\n",
    "    distributed (package)\n",
    "    distributions (package)\n",
    "    for_onnx (package)\n",
    "    functional\n",
    "    futures (package)\n",
    "    hub\n",
    "    jit (package)\n",
    "    multiprocessing (package)\n",
    "    nn (package)\n",
    "    onnx (package)\n",
    "    optim (package)\n",
    "    quantization (package)\n",
    "    quasirandom\n",
    "    random\n",
    "    serialization\n",
    "    sparse (package)\n",
    "    storage\n",
    "    tensor\n",
    "    testing (package)\n",
    "    types\n",
    "    utils (package)\n",
    "    version\n",
    "\n",
    "**SUBMODULES**\n",
    "- classes\n",
    "- cpp\n",
    "- ops\n",
    "\n",
    "**CLASSES**\n",
    "- builtins.Exception(builtins.BaseException)\n",
    "    - FatalError\n",
    "    - torch.jit.Error\n",
    "- builtins.object\n",
    "    - BoolTensor\n",
    "    - ByteTensor\n",
    "    - CharTensor\n",
    "    - DoubleTensor\n",
    "    - FloatTensor\n",
    "    - IntTensor\n",
    "    - LongTensor\n",
    "    - ShortTensor\n",
    "    - device\n",
    "    - dtype\n",
    "    - finfo\n",
    "    - iinfo\n",
    "    - layout\n",
    "    - memory_format\n",
    "    - qscheme\n",
    "    - torch.\\_C.Generator\n",
    "    - torch.autograd.grad_mode.set_grad_enabled\n",
    "- builtins.tuple(builtins.object)\n",
    "    - Size\n",
    "- pybind11_builtins.pybind11_object(builtins.object)\n",
    "    - torch.\\_C.AggregationType\n",
    "    - torch.\\_C.Argument\n",
    "    - torch.\\_C.ArgumentSpec\n",
    "    - torch.\\_C.BenchmarkConfig\n",
    "    - torch.\\_C.BenchmarkExecutionStats\n",
    "    - torch.\\_C.Block\n",
    "    - torch.\\_C.BufferDict\n",
    "    - torch.\\_C.CallStack\n",
    "    - torch.\\_C.Capsule\n",
    "    - torch.\\_C.Code\n",
    "    - torch.\\_C.CompilationUnit\n",
    "    - torch.\\_C.CompleteArgumentSpec\n",
    "    - torch.\\_C.ConcreteModuleType\n",
    "    - torch.\\_C.ConcreteModuleTypeBuilder\n",
    "    - torch.\\_C.DeepCopyMemoTable\n",
    "    - torch.\\_C.ErrorReport\n",
    "    - torch.\\_C.ExecutionPlan\n",
    "    - torch.\\_C.ExtraFilesMap\n",
    "    - torch.\\_C.FileCheck\n",
    "    - torch.\\_C.FunctionSchema\n",
    "    - torch.\\_C.Future\n",
    "    - torch.\\_C.Gradient\n",
    "    - torch.\\_C.Graph\n",
    "    - torch.\\_C.GraphExecutorState\n",
    "    - torch.\\_C.IODescriptor\n",
    "    - torch.\\_C.LiteScriptModule\n",
    "    - torch.\\_C.MobileOptimizerType\n",
    "    - torch.\\_C.ModuleDict\n",
    "    - torch.\\_C.Node\n",
    "    - torch.\\_C.ParameterDict\n",
    "    - torch.\\_C.PyTorchFileReader\n",
    "    - torch.\\_C.PyTorchFileWriter\n",
    "    - torch.\\_C.ScriptClass\n",
    "    - torch.\\_C.ScriptMethod\n",
    "    - torch.\\_C.ScriptObject\n",
    "        - torch.\\_C.ScriptModule\n",
    "    - torch.\\_C.ThroughputBenchmark\n",
    "    - torch.\\_C.TracingState\n",
    "    - torch.\\_C.Type\n",
    "        - torch.\\_C.AnyType\n",
    "        - torch.\\_C.BoolType\n",
    "        - torch.\\_C.ClassType\n",
    "        - torch.\\_C.DeviceObjType\n",
    "        - torch.\\_C.DictType\n",
    "        - torch.\\_C.FloatType\n",
    "        - torch.\\_C.FutureType\n",
    "        - torch.\\_C.IntType\n",
    "        - torch.\\_C.InterfaceType\n",
    "        - torch.\\_C.ListType\n",
    "        - torch.\\_C.NoneType\n",
    "        - torch.\\_C.NumberType\n",
    "        - torch.\\_C.OptionalType\n",
    "        - torch.\\_C.PyObjectType\n",
    "        - torch.\\_C.RRefType\n",
    "        - torch.\\_C.StringType\n",
    "        - torch.\\_C.TensorType\n",
    "        - torch.\\_C.TupleType\n",
    "    - torch.\\_C.Use\n",
    "    - torch.\\_C.Value\n",
    "    - torch.jit.ScriptFunction\n",
    "- torch.\\_C.BoolStorageBase(builtins.object)\n",
    "    - BoolStorage(torch.\\_C.BoolStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.ByteStorageBase(builtins.object)\n",
    "    - ByteStorage(torch.\\_C.ByteStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.CharStorageBase(builtins.object)\n",
    "    - CharStorage(torch.\\_C.CharStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.DoubleStorageBase(builtins.object)\n",
    "    - DoubleStorage(torch.\\_C.DoubleStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.FloatStorageBase(builtins.object)\n",
    "    - FloatStorage(torch.\\_C.FloatStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.IntStorageBase(builtins.object)\n",
    "    - IntStorage(torch.\\_C.IntStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.LoggerBase(pybind11_builtins.pybind11_object)\n",
    "    - torch.\\_C.LockingLogger\n",
    "    - torch.\\_C.NoopLogger\n",
    "- torch.\\_C.LongStorageBase(builtins.object)\n",
    "    - LongStorage(torch.\\_C.LongStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.ShortStorageBase(builtins.object)\n",
    "    - ShortStorage(torch.\\_C.ShortStorageBase, torch.storage.\\_StorageBase)\n",
    "- torch.\\_C.\\_TensorBase(builtins.object)\n",
    "    - Tensor\n",
    "- torch.autograd.grad_mode.\\_DecoratorContextManager(builtins.object)\n",
    "    - torch.autograd.grad_mode.enable_grad\n",
    "    - torch.autograd.grad_mode.no_grad\n",
    "- torch.storage.\\_StorageBase(builtins.object)\n",
    "    - BoolStorage(torch.\\_C.BoolStorageBase, torch.storage.\\_StorageBase)\n",
    "    - ByteStorage(torch.\\_C.ByteStorageBase, torch.storage.\\_StorageBase)\n",
    "    - CharStorage(torch.\\_C.CharStorageBase, torch.storage.\\_StorageBase)\n",
    "    - DoubleStorage(torch.\\_C.DoubleStorageBase, torch.storage.\\_StorageBase)\n",
    "    - FloatStorage(torch.\\_C.FloatStorageBase, torch.storage.\\_StorageBase)\n",
    "    - IntStorage(torch.\\_C.IntStorageBase, torch.storage.\\_StorageBase)\n",
    "    - LongStorage(torch.\\_C.LongStorageBase, torch.storage.\\_StorageBase)\n",
    "    - ShortStorage(torch.\\_C.ShortStorageBase, torch.storage.\\_StorageBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.bmm()\n",
    "`bmm(input, mat2, deterministic=False, out=None) -> Tensor`\n",
    "\n",
    "**Doctring**\n",
    "\n",
    "对 batch 层级的`input`和`mat2`张量做矩阵乘法，这要求`input`和`mat2`有相同的 batch 数；如果`input`是 $(b \\times n \\times m)$ 的张量，`mat2`是 $(b \\times m \\times p)$ 的张量，则`out`是 $(b \\times n \\times p)$ 的张量。此函数不支持`broadcast <broadcasting-semantics>`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- input, mat2, out: 略\n",
    "\n",
    "- deterministic: 使用更快的非确定性计算或较慢的确定性计算，该参数只适用于 sparse-dense CUDA bmm\n",
    "\n",
    "**Type**:      builtin_function_or_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cuda.set_device()\n",
    "`torch.cuda.set_device(device: Union[torch.device, str, int]) -> None`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "设定目前使用的设备；Usage of this function is discouraged in favor of `device`，大多情况下最好使用``CUDA_VISIBLE_DEVICES``环境变量\n",
    "\n",
    "**Args**\n",
    "\n",
    "- device: 可以是`torch.device`对象或整数，即设定的运行设备，若参数为负数，则此函数为无操作 (no-op) 函数\n",
    "\n",
    "**File**:   \\torch\\cuda\\\\\\_\\_init__.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.device()\n",
    "\n",
    "`torch.device`返回一个代表着运行设备的对象，`torch.Tensor`会被分配至该设备上运行；其接受的参数包括一个设备类型（`'cpu'`或`'cuda'`）及设备类型相应的设备序号，若没有指定设备序号，即使调用了`torch.cuda.set_device()`，此函数仍会返回当前的设备作为设备类型，例如通过设备`\"cuda\"`构造的张量等价于通过`'cuda:X'`构造的张量，其中 X 为`torch.cuda.current_device()`的返回值；\n",
    "\n",
    "一个`torch.Tensor`的设备可以通过`Tensor.device`属性获得，其设备可以通过一个字符串或字符串+设备序号来构造；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(torch.device('cuda:0'))\n",
    "print(torch.device(\"cuda\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.load()\n",
    "```python\n",
    "torch.load(\n",
    "    f,\n",
    "    map_location=None,\n",
    "    pickle_module=<module 'pickle' from '...\\\\lib\\\\pickle.py'>,\n",
    "    **pickle_load_args,\n",
    ")\n",
    "```\n",
    "\n",
    "``torch.load``从一个文件中加载由`torch.save`函数保存的对象，其使用 Python 的 unpickle 工具，但专门处理存储；这些存储首先在 CPU 上被反序列化，然后被移动到保存它们的设备，如果该过程失败则就会引发异常，然而可以利用``map_location``动态地将存储重新映射到另一组设备。\n",
    "\n",
    "若`map_location`为可调用函数，则对每个带有`storage`和`location`两个参数的序列化存储，该函数都会被调用一次，其中`storage`为存储在 CPU 上初始反序列化的结果；每个序列化存储都有一个与之关联的位置标签，其标识着保存存储的设备，这个标签是传递给`map_location`的第二个参数；对于 CPU 张量，内置的位置标签为``'cpu'``，GPU 张量的内置位置标签则是``'cuda:device_id'``。`map_location`应返回一个存储或是``None``，若其返回一个存储，则该存储将被视为已经移动到正确设备上的最终反序列化后的对象，返回``None``时`torch.load`将执行默认指令。\n",
    "\n",
    "若`map_location`是`torch.device`类对象或代表设备标签的字符串，此时其用于表示所有张量被加载到的位置；若`map_location`是一个字典，则其用于将所文件中位置标记（键）重新映射到指定着存储位置的位置标记（值）\n",
    "\n",
    "User extension 可以通过`torch.serialization.register_package`来注册他们自己的位置标签、标记及反序列化的方法；\n",
    "\n",
    "当使用`torch.load()`从含有 GPU 张量的文件加载张量时，这些张量会默认加载至 GPU；若想避免 GPU 内存内存占用增加，可以通过调用`torch.load(... , map_location='cpu')`函数，再调用`load_state_dict`函数加载模型检查点。默认情况下为避免在 Python 3 中加载由 Python 2 保存的文件时可能抛出的异常``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``，函数使用``utf-8``模式解码；若该默认解码方式不适用，可通过传递关键字参数来指明对象的加载方式；例如`encoding='latin1'`通过``latin1``编码将内容解码为字符串，`encoding='bytes'`将其转换为字节数组，随后可以通过``byte_array.decode(...)``来对字节数组进行解码；\n",
    "\n",
    "**Args**\n",
    "\n",
    "- f: 可以是一个类文件的对象，其必须含有`read`、`readline`、`tell`、`seek`方法的实现，或是一个包含文件名的字符串\n",
    "\n",
    "- map_location: 其可以是指明了怎样重新映射存储位置的一个函数、`torch.device`类、字符串、字典\n",
    "\n",
    "- pickle_module: 用于 unpickle 元数据及对象的模块，其必须与用于序列化文件的`pickle_module`属性相匹配\n",
    "\n",
    "- pickle_load_args: 传递给`pickle_module.load`和`pickle_module.Unpickler`的关键参数，例如`errors=...`\n",
    "\n",
    "**警告！**`torch.load()`隐式的使用``pickle``模块，这个过程可能是不安全的，期间可能会产生恶意数据，其可能在 unpickle 期间执行任意未知代码，故不要加载不信任的来源或可能被篡改的数据\n",
    "\n",
    "\n",
    "**Type**： function\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('tensors.pt')\n",
    "torch.load('tensors.pt', map_location=torch.device('cpu'))  # Load all tensors onto the CPU\n",
    "torch.load('tensors.pt', map_location=lambda storage, loc: storage)  # Load all tensors onto the CPU, using a function\n",
    "torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))  # Load all tensors onto GPU 1\n",
    "torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})  # Map tensors from GPU 1 to GPU 0\n",
    "with open('tensor.pt', 'rb') as f:\n",
    "    buffer = io.BytesIO(f.read())\n",
    "    torch.load(buffer)  # Load tensor from io.BytesIO object\n",
    "torch.load('module.pt', encoding='ascii')# Load a module with 'ascii' encoding for unpickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.no_grad()\n",
    "`torch.no_grad()`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "返回一个禁用梯度计算的上下文管理器，当不会调用`Tensor.backward()`时，禁用梯度计算对模型推断帮助，其可以减少计算的内存消耗；该模式下，即使输入张量指明了`requires_grad=True`，计算时也会是`requires_grad=False`。在使用`torch.enable_grad`上下文管理器时，此模式不起作用。该上下文管理器是 local thread 的，其不影响其他线程的计算\n",
    "\n",
    "Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
    "\n",
    "**File**:  \\torch\\autograd\\grad_mode.py\n",
    "\n",
    "**Type**:           type \n",
    "\n",
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "\n",
    "y = doubler(x)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.norm()\n",
    "`torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)`\n",
    "**Docstring**:\n",
    "\n",
    "根据给定的张量返回矩阵范数或向量范数，该函数可能会被弃用，请使用`torch.linalg.norm`\n",
    "\n",
    "**Args**:\n",
    "- input, out, dtype: pass\n",
    "- p: 可以是整型、浮点型、`inf`、`-inf`、`'fro'`、`'nuc'`等，指范数的阶；默认`'fro'`；核范数只能作用于 2 维矩阵\n",
    "oder| 矩阵范数 | 向量范数\n",
    ":---| :-------| :-------\n",
    "`None`|Frobenius 范数| 2 范数\n",
    "`'fro'`|Frobenius 范数|--\n",
    "`'nuc'`|核范数$tr\\left(\\sqrt{X^TX}\\right)$|--\n",
    "其他|`dim=None`时与向量范数相同|`sum(abs(x)**ord)**(1./ord)`\n",
    "- dim: 若为整型，则沿指定维度计算向量范数；若为 2 元元祖或 2 元列表，则沿指定维度计算矩阵范数；若为 None，当输入为 2 维张量时计算矩阵范数，当输入为 1 维张量时计算向量范数，当输入为高于 2 维的张量时沿最后一维计算向量范数\n",
    "- keepdim: 返回张量是否保持`dim`所指维度，默认 False；当`dim=None`且`out=None`时此参数无效\n",
    "\n",
    "**File**:     \\torch\\functional.py\n",
    "\n",
    "**Type**:      function\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n",
    ">>> torch.norm(d, dim=(1,2))\n",
    "tensor([ 3.7417, 11.2250])\n",
    ">>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n",
    "(tensor(3.7417), tensor(11.2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26.9815)\n",
      "tensor(168.)\n",
      "tensor(26.9815)\n",
      "tensor(26.9815)\n",
      "tensor(6.)\n",
      "tensor([[5.8310, 5.8310, 5.8310, 5.8310],\n",
      "        [7.6158, 7.6158, 7.6158, 7.6158],\n",
      "        [9.4868, 9.4868, 9.4868, 9.4868]])\n",
      "tensor([[[ 2.,  4.,  6.],\n",
      "         [ 8., 10., 12.]],\n",
      "\n",
      "        [[ 2.,  4.,  6.],\n",
      "         [ 8., 10., 12.]]])\n",
      "tensor([[ 7.4833, 17.5499],\n",
      "        [ 7.4833, 17.5499]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 7, dtype= torch.float).reshape(1, 2, 3, 1).expand(2, 2, 3, 4)\n",
    "print(torch.norm(x))\n",
    "print(torch.norm(x, p=1))\n",
    "print(torch.norm(x, p=2))\n",
    "print(torch.norm(x, p='fro'))\n",
    "print(torch.norm(x, p=float('inf')))\n",
    "print(torch.norm(x, p='nuc', dim=[0, 1]))\n",
    "print(torch.norm(x, p='fro', dim=-1))\n",
    "print(torch.norm(x, p='fro', dim=[-1, -2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.sort()\n",
    "\n",
    "`sort(input, dim=-1, descending=False, out=None) -> (Tensor, LongTensor)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "将`input`张量元素值沿给定`dim`升序/降序排列，并返回元组`(values, indices)`，其中\n",
    "`indices`为对应`values`在原张量中的索引\n",
    "\n",
    "**Args**\n",
    "\n",
    "- input, dim, out: 略\n",
    "\n",
    "- descending: 决定进行升序/降序排列\n",
    "\n",
    "**Type**\n",
    "\n",
    "builtin_function_or_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.arange(1, 25).reshape(1, 2, 3, 4)\n",
    "values, ids = torch.sort(x, descending=True, dim=-2)\n",
    "print(\"values\", values, \"indices\", ids, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.arange(1, 13).reshape(3, 4)\n",
    "ids = torch.sort(x, descending=True, dim=-1)[1]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.sum()\n",
    "`torch.sum(input, dim, keepdim=False, *, dtype=None) -> Tensor`\n",
    "\n",
    "Returns the sum of each row of the `input` tensor in the given dimension `dim`. If `dim` is a list of dimensions, reduce over all of them.\n",
    "\n",
    "**Args**\n",
    "\n",
    "- input: 略\n",
    "- dim: 可以是整数或整数列表，要进行加和的维度，即如果`dim`是一列表，则对所有指明的维度进行加和\n",
    "- keepdim: 输出张量维度的数量是否与输入相同\n",
    "- dtype: 默认为 None，指明时输入张量会先被转换成目标数据类型在进行函数操作\n",
    "\n",
    "**Type**:      builtin_function_or_method\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 7).reshape(2, 3, 1).expand(2, 3, 4)\n",
    "y = torch.sum(x, dim=[-1, -2])\n",
    "y = torch.sum(x).item()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.topk()\n",
    "`torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)\n",
    " <tensor>.topk(k, dim=None, largest=True, sorted=True, out=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "对于给定的张量，返回两个张量，第一个为给定维度上 k 个最大/小值，第二个为最大/小值对应在原张量`input`中`dim`维上的索引\n",
    "\n",
    "**Args**\n",
    "\n",
    "- input, k: 略\n",
    "\n",
    "- dim: 排序的维度，默认 -1；如$(a \\times b \\times c)$的 tensor，dim=0时返回两个 $(k \\times b \\times c)$ tensor\n",
    "\n",
    "- largest: 决定了返回最大值或最小值，`True`时返回 k 个最大值，`False`时返回 k 个最小值\n",
    "\n",
    "- sorted: 决定是否以排序的顺序返回各元素\n",
    "\n",
    "- out (tuple): the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(range(24)).reshape(2, 3, 4)\n",
    "x = a.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "\n",
    "print(torch.topk(a, 1, dim=0)[0].shape)\n",
    "print(torch.topk(a, 1, dim=1)[0].shape)\n",
    "print(torch.topk(a, 1, dim=2)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.tensor(range(24)).reshape(4, 6)\n",
    "_, pred = output.topk(k=1, dim=1, largest=True, sorted=True)\n",
    "pred = pred.t()\n",
    "target = torch.tensor([5, 32, 17, 11])\n",
    "correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "correct[:10].view(-1).float().sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20232375 0.40433996 0.36665815 0.37907609 0.35556185 0.34073845\n",
      " 0.39835009 0.26194353 0.2003465  0.38420623]\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    out = np.exp(X)\n",
    "    return out / np.sum(out)\n",
    "\n",
    "X = np.random.uniform(0.2, 0.42, [10])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08783213 0.10749491 0.10351968 0.1048132  0.10237734 0.10087096\n",
      " 0.10685296 0.09322791 0.08765863 0.10535228]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.unsqueeze()\n",
    "`unsqueeze(input, dim) -> Tensor`\n",
    "\n",
    "**Args**:\n",
    "- input: the input tensor.\n",
    "- dim (int): 要插入维数的索引值，取值范围在``[-input.dim() - 1, input.dim() + 1)``之间\n",
    "\n",
    "**Type**:      builtin_function_or_method\n",
    "\n",
    "### Example\n",
    "```python\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(torch.unsqueeze(x, dim=0).shape)  # => torch.Size([1, 4])\n",
    "print(torch.unsqueeze(x, dim=1).shape)  # => torch.Size([4, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch.Tensor\n",
    "## torch.Tensor.all()\n",
    "`<tensor>.all() -> bool`\n",
    "\n",
    "Returns True if all elements in the tensor are True, False otherwise.\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> a = torch.rand(1, 2).bool()\n",
    "    >>> a\n",
    "    tensor([[False, True]], dtype=torch.bool)\n",
    "    >>> a.all()\n",
    "    tensor(False, dtype=torch.bool)\n",
    "\n",
    ".. function:: all(dim, keepdim=False, out=None) -> Tensor\n",
    "\n",
    "Returns True if all elements in each row of the tensor in the given\n",
    "dimension :attr:`dim` are True, False otherwise.\n",
    "\n",
    "If :attr:`keepdim` is ``True``, the output tensor is of the same size as\n",
    ":attr:`input` except in the dimension :attr:`dim` where it is of size 1.\n",
    "Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting\n",
    "in the output tensor having 1 fewer dimension than :attr:`input`.\n",
    "\n",
    "Args:\n",
    "    dim (int): the dimension to reduce\n",
    "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not\n",
    "    out (Tensor, optional): the output tensor\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> a = torch.rand(4, 2).bool()\n",
    "    >>> a\n",
    "    tensor([[True, True],\n",
    "            [True, False],\n",
    "            [True, True],\n",
    "            [True, True]], dtype=torch.bool)\n",
    "    >>> a.all(dim=1)\n",
    "    tensor([ True, False,  True,  True], dtype=torch.bool)\n",
    "    >>> a.all(dim=0)\n",
    "    tensor([ True, False], dtype=torch.bool)\n",
    "Type:      method_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[True, False], [False, True]])\n",
    "y = torch.tensor([[False, False], [True, True]])\n",
    "print((x * y).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.item()\n",
    "`<tensor_name>.item()`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "以标准 Python 值的形式返回该张量的值，其只适用于只有一个元素的张量；其他情况见`torch.Tensor.tolist`，这个操作是不可导的；\n",
    "\n",
    "**Type**:      method_descriptor\n",
    "### Example\n",
    "```python\n",
    "x = torch.tensor([1])\n",
    "x.item()  # ==> 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.transpose()\n",
    "`<tensor_name>.transpose(dim0, dim1)`、`torch.transpose(input, dim0, dim1)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "将张量在维度`dim0`和`dim1`上进行转置，输出张量和输入张量共享底层存储空间，即改变一个张量的内容也会改变另一个张量的内容\n",
    "\n",
    "**Type**:      method_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1, 25).reshape(1, 2, 3, 4)\n",
    "print(x.transpose(1, 2))\n",
    "print(x.transpose(1, 3))\n",
    "print(x.transpose(0, 1))\n",
    "print(x.transpose(0, 2))\n",
    "print(x.transpose(0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.cat()\n",
    "`torch.cat(tensors, dim=0, out=None) -> Tensor`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "将一个张量序列`tensors`在给定维度上进行拼接，其中`tensors`可以是元祖或列表；进行拼接的各张量的形状除进行拼接的维度之外，其余维度应当相同，或为空张量；`torch.cat`可以看作`torch.split`和`torch.chunk`的逆运算；\n",
    "\n",
    "**Type**:      builtin_function_or_method\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.]])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [15., 16., 17., 18.]])\n",
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [15., 16., 17., 18.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([])\n",
    "x = torch.arange(1, 13).reshape(3, 4)\n",
    "y = torch.arange(15, 19).unsqueeze(0)\n",
    "print(torch.cat([x, z], 0))\n",
    "y = torch.cat([x, y], 0, out=z)\n",
    "print(z, y, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.cat([x, x, x], 0)\n",
    "y = torch.cat((x, x, x), 1)\n",
    "y = torch.cat((x, x, x), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
