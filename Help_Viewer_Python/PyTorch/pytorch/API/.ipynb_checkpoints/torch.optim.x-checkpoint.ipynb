{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim.SGD()\n",
    "```python\n",
    "optim.SGD(\n",
    "    params,\n",
    "    lr=<required parameter>,\n",
    "    momentum=0,\n",
    "    dampening=0,\n",
    "    weight_decay=0,\n",
    "    nesterov=False,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "\n",
    "实现随机梯度下降。注意，使用 Momentum/Nesterov 实现 SGD 与 Sutskever 等人以及其他一些框架中的对 SGD 的实现有些不同，考虑到动量的具体情况，更新可以写成：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
    "    p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $p, g, v, `\\mu$ 分别代表参数、梯度、速度、动量，这不同于 Sutskever 等人及其他框架中的更新形式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
    "    p_{t+1} & = p_{t} - v_{t+1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "原论文：[Nesterov momentum](http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf)\n",
    "\n",
    "**Args**\n",
    "\n",
    "- params: 要优化的可迭代的参数，或定义了参数组的字典\n",
    "\n",
    "- lr: pass\n",
    "\n",
    "- momentum: 动量因子，默认 0\n",
    "\n",
    "- weight_decay: 权值衰减系数，即 L2 正则化的正则化系数，默认 0\n",
    "\n",
    "- dampening: dampening for momentum, default: 0\n",
    "\n",
    "- nesterov: 是否使用 Nesterov 动量模型，默认 Flase\n",
    " \n",
    "\n",
    "**File**: \\torch\\optim\\sgd.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim.SGD.step()\n",
    "`<optim_name>.step(closure=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "执行一个更新参数的过程\n",
    "\n",
    "**Args**\n",
    "\n",
    "- closure (callable, optional): 重新评估模型并返回损失的闭包(closure)\n",
    "\n",
    "**File**: torch\\optim\\sgd.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim.SGD.zero_grad()\n",
    "`<optim_name>.zero_grad()`\n",
    "\n",
    "清除所有需要优化的`torch.Tensor`的梯度，常用于训练神经网络时避免上一轮梯度更新影响下一轮梯度更新处\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SDG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim.optimizer.state_dict\n",
    "\n",
    "`optimizer.state_dict()`\n",
    "\n",
    "**Docstring:**\n",
    "\n",
    "返回优化器(optimizer)的状态字典，该字典包括两个条目:`state`——包含目前优化器的状态的字典，`param_groups`——包含所有参数组的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.optim.lr_scheduler in torch.optim:\n",
      "\n",
      "NAME\n",
      "    torch.optim.lr_scheduler\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        ReduceLROnPlateau\n",
      "    _LRScheduler(builtins.object)\n",
      "        CosineAnnealingLR\n",
      "        CosineAnnealingWarmRestarts\n",
      "        CyclicLR\n",
      "        ExponentialLR\n",
      "        LambdaLR\n",
      "        MultiStepLR\n",
      "        MultiplicativeLR\n",
      "        OneCycleLR\n",
      "        StepLR\n",
      "    \n",
      "    class CosineAnnealingLR(_LRScheduler)\n",
      "     |  CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Set the learning rate of each parameter group using a cosine annealing\n",
      "     |  schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
      "     |  :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\begin{aligned}\n",
      "     |          \\eta_t & = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n",
      "     |          + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n",
      "     |          & T_{cur} \\neq (2k+1)T_{max}; \\\\\n",
      "     |          \\eta_{t+1} & = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n",
      "     |          \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n",
      "     |          & T_{cur} = (2k+1)T_{max}.\n",
      "     |      \\end{aligned}\n",
      "     |  \n",
      "     |  When last_epoch=-1, sets initial lr as lr. Notice that because the schedule\n",
      "     |  is defined recursively, the learning rate can be simultaneously modified\n",
      "     |  outside this scheduler by other operators. If the learning rate is set\n",
      "     |  solely by this scheduler, the learning rate at each step becomes:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
      "     |      \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\n",
      "     |  \n",
      "     |  It has been proposed in\n",
      "     |  `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
      "     |  implements the cosine annealing part of SGDR, and not the restarts.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      T_max (int): Maximum number of iterations.\n",
      "     |      eta_min (float): Minimum learning rate. Default: 0.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
      "     |      https://arxiv.org/abs/1608.03983\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineAnnealingLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CosineAnnealingWarmRestarts(_LRScheduler)\n",
      "     |  CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Set the learning rate of each parameter group using a cosine annealing\n",
      "     |  schedule, where :math:`\\eta_{max}` is set to the initial lr, :math:`T_{cur}`\n",
      "     |  is the number of epochs since the last restart and :math:`T_{i}` is the number\n",
      "     |  of epochs between two warm restarts in SGDR:\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |      \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n",
      "     |      \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n",
      "     |  \n",
      "     |  When :math:`T_{cur}=T_{i}`, set :math:`\\eta_t = \\eta_{min}`.\n",
      "     |  When :math:`T_{cur}=0` after restart, set :math:`\\eta_t=\\eta_{max}`.\n",
      "     |  \n",
      "     |  It has been proposed in\n",
      "     |  `SGDR: Stochastic Gradient Descent with Warm Restarts`_.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      T_0 (int): Number of iterations for the first restart.\n",
      "     |      T_mult (int, optional): A factor increases :math:`T_{i}` after a restart. Default: 1.\n",
      "     |      eta_min (float, optional): Minimum learning rate. Default: 0.\n",
      "     |      last_epoch (int, optional): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
      "     |      https://arxiv.org/abs/1608.03983\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineAnnealingWarmRestarts\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |      Step could be called after every batch update\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
      "     |          >>> iters = len(dataloader)\n",
      "     |          >>> for epoch in range(20):\n",
      "     |          >>>     for i, sample in enumerate(dataloader):\n",
      "     |          >>>         inputs, labels = sample['inputs'], sample['labels']\n",
      "     |          >>>         optimizer.zero_grad()\n",
      "     |          >>>         outputs = net(inputs)\n",
      "     |          >>>         loss = criterion(outputs, labels)\n",
      "     |          >>>         loss.backward()\n",
      "     |          >>>         optimizer.step()\n",
      "     |          >>>         scheduler.step(epoch + i / iters)\n",
      "     |      \n",
      "     |      This function can be called in an interleaved way.\n",
      "     |      \n",
      "     |      Example:\n",
      "     |          >>> scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
      "     |          >>> for epoch in range(20):\n",
      "     |          >>>     scheduler.step()\n",
      "     |          >>> scheduler.step(26)\n",
      "     |          >>> scheduler.step() # scheduler.step(27), instead of scheduler(20)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CyclicLR(_LRScheduler)\n",
      "     |  CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group according to\n",
      "     |  cyclical learning rate policy (CLR). The policy cycles the learning\n",
      "     |  rate between two boundaries with a constant frequency, as detailed in\n",
      "     |  the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
      "     |  The distance between the two boundaries can be scaled on a per-iteration\n",
      "     |  or per-cycle basis.\n",
      "     |  \n",
      "     |  Cyclical learning rate policy changes the learning rate after every batch.\n",
      "     |  `step` should be called after a batch has been used for training.\n",
      "     |  \n",
      "     |  This class has three built-in policies, as put forth in the paper:\n",
      "     |  \n",
      "     |  * \"triangular\": A basic triangular cycle without amplitude scaling.\n",
      "     |  * \"triangular2\": A basic triangular cycle that scales initial amplitude by half each cycle.\n",
      "     |  * \"exp_range\": A cycle that scales initial amplitude by :math:`\\text{gamma}^{\\text{cycle iterations}}`\n",
      "     |    at each cycle iteration.\n",
      "     |  \n",
      "     |  This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      base_lr (float or list): Initial learning rate which is the\n",
      "     |          lower boundary in the cycle for each parameter group.\n",
      "     |      max_lr (float or list): Upper learning rate boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_lr - base_lr).\n",
      "     |          The lr at any cycle is the sum of base_lr\n",
      "     |          and some scaling of the amplitude; therefore\n",
      "     |          max_lr may not actually be reached depending on\n",
      "     |          scaling function.\n",
      "     |      step_size_up (int): Number of training iterations in the\n",
      "     |          increasing half of a cycle. Default: 2000\n",
      "     |      step_size_down (int): Number of training iterations in the\n",
      "     |          decreasing half of a cycle. If step_size_down is None,\n",
      "     |          it is set to step_size_up. Default: None\n",
      "     |      mode (str): One of {triangular, triangular2, exp_range}.\n",
      "     |          Values correspond to policies detailed above.\n",
      "     |          If scale_fn is not None, this argument is ignored.\n",
      "     |          Default: 'triangular'\n",
      "     |      gamma (float): Constant in 'exp_range' scaling function:\n",
      "     |          gamma**(cycle iterations)\n",
      "     |          Default: 1.0\n",
      "     |      scale_fn (function): Custom scaling policy defined by a single\n",
      "     |          argument lambda function, where\n",
      "     |          0 <= scale_fn(x) <= 1 for all x >= 0.\n",
      "     |          If specified, then 'mode' is ignored.\n",
      "     |          Default: None\n",
      "     |      scale_mode (str): {'cycle', 'iterations'}.\n",
      "     |          Defines whether scale_fn is evaluated on\n",
      "     |          cycle number or cycle iterations (training\n",
      "     |          iterations since start of cycle).\n",
      "     |          Default: 'cycle'\n",
      "     |      cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
      "     |          to learning rate between 'base_momentum' and 'max_momentum'.\n",
      "     |          Default: True\n",
      "     |      base_momentum (float or list): Lower momentum boundaries in the cycle\n",
      "     |          for each parameter group. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the peak of a cycle, momentum is\n",
      "     |          'base_momentum' and learning rate is 'max_lr'.\n",
      "     |          Default: 0.8\n",
      "     |      max_momentum (float or list): Upper momentum boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_momentum - base_momentum).\n",
      "     |          The momentum at any cycle is the difference of max_momentum\n",
      "     |          and some scaling of the amplitude; therefore\n",
      "     |          base_momentum may not actually be reached depending on\n",
      "     |          scaling function. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
      "     |          and learning rate is 'base_lr'\n",
      "     |          Default: 0.9\n",
      "     |      last_epoch (int): The index of the last batch. This parameter is used when\n",
      "     |          resuming a training job. Since `step()` should be invoked after each\n",
      "     |          batch instead of after each epoch, this number represents the total\n",
      "     |          number of *batches* computed, not the total number of epochs computed.\n",
      "     |          When last_epoch=-1, the schedule is started from the beginning.\n",
      "     |          Default: -1\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
      "     |      >>> data_loader = torch.utils.data.DataLoader(...)\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     for batch in data_loader:\n",
      "     |      >>>         train_batch(...)\n",
      "     |      >>>         scheduler.step()\n",
      "     |  \n",
      "     |  \n",
      "     |  .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
      "     |  .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CyclicLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |      Calculates the learning rate at batch index. This function treats\n",
      "     |      `self.last_epoch` as the last batch index.\n",
      "     |      \n",
      "     |      If `self.cycle_momentum` is ``True``, this function has a side effect of\n",
      "     |      updating the optimizer's momentum.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExponentialLR(_LRScheduler)\n",
      "     |  ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma every epoch.\n",
      "     |  When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExponentialLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, gamma, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LambdaLR(_LRScheduler)\n",
      "     |  LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group to the initial lr\n",
      "     |  times a given function. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      lr_lambda (function or list): A function which computes a multiplicative\n",
      "     |          factor given an integer parameter epoch, or a list of such\n",
      "     |          functions, one for each group in optimizer.param_groups.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # Assuming optimizer has two groups.\n",
      "     |      >>> lambda1 = lambda epoch: epoch // 30\n",
      "     |      >>> lambda2 = lambda epoch: 0.95 ** epoch\n",
      "     |      >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LambdaLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The learning rate lambda functions will only be saved if they are callable objects\n",
      "     |      and not if they are functions or lambdas.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultiStepLR(_LRScheduler)\n",
      "     |  MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma once the\n",
      "     |  number of epoch reaches one of the milestones. Notice that such decay can\n",
      "     |  happen simultaneously with other changes to the learning rate from outside\n",
      "     |  this scheduler. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      milestones (list): List of epoch indices. Must be increasing.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |          Default: 0.1.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.05     if epoch < 30\n",
      "     |      >>> # lr = 0.005    if 30 <= epoch < 80\n",
      "     |      >>> # lr = 0.0005   if epoch >= 80\n",
      "     |      >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiStepLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultiplicativeLR(_LRScheduler)\n",
      "     |  MultiplicativeLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Multiply the learning rate of each parameter group by the factor given\n",
      "     |  in the specified function. When last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      lr_lambda (function or list): A function which computes a multiplicative\n",
      "     |          factor given an integer parameter epoch, or a list of such\n",
      "     |          functions, one for each group in optimizer.param_groups.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> lmbda = lambda epoch: 0.95\n",
      "     |      >>> scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiplicativeLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, lr_lambda, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |      The learning rate lambda functions will only be saved if they are callable objects\n",
      "     |      and not if they are functions or lambdas.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OneCycleLR(_LRScheduler)\n",
      "     |  OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Sets the learning rate of each parameter group according to the\n",
      "     |  1cycle learning rate policy. The 1cycle policy anneals the learning\n",
      "     |  rate from an initial learning rate to some maximum learning rate and then\n",
      "     |  from that maximum learning rate to some minimum learning rate much lower\n",
      "     |  than the initial learning rate.\n",
      "     |  This policy was initially described in the paper `Super-Convergence:\n",
      "     |  Very Fast Training of Neural Networks Using Large Learning Rates`_.\n",
      "     |  \n",
      "     |  The 1cycle learning rate policy changes the learning rate after every batch.\n",
      "     |  `step` should be called after a batch has been used for training.\n",
      "     |  \n",
      "     |  This scheduler is not chainable.\n",
      "     |  \n",
      "     |  Note also that the total number of steps in the cycle can be determined in one\n",
      "     |  of two ways (listed in order of precedence):\n",
      "     |  \n",
      "     |  #. A value for total_steps is explicitly provided.\n",
      "     |  #. A number of epochs (epochs) and a number of steps per epoch\n",
      "     |     (steps_per_epoch) are provided.\n",
      "     |     In this case, the number of total steps is inferred by\n",
      "     |     total_steps = epochs * steps_per_epoch\n",
      "     |  \n",
      "     |  You must either provide a value for total_steps or provide a value for both\n",
      "     |  epochs and steps_per_epoch.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      max_lr (float or list): Upper learning rate boundaries in the cycle\n",
      "     |          for each parameter group.\n",
      "     |      total_steps (int): The total number of steps in the cycle. Note that\n",
      "     |          if a value is not provided here, then it must be inferred by providing\n",
      "     |          a value for epochs and steps_per_epoch.\n",
      "     |          Default: None\n",
      "     |      epochs (int): The number of epochs to train for. This is used along\n",
      "     |          with steps_per_epoch in order to infer the total number of steps in the cycle\n",
      "     |          if a value for total_steps is not provided.\n",
      "     |          Default: None\n",
      "     |      steps_per_epoch (int): The number of steps per epoch to train for. This is\n",
      "     |          used along with epochs in order to infer the total number of steps in the\n",
      "     |          cycle if a value for total_steps is not provided.\n",
      "     |          Default: None\n",
      "     |      pct_start (float): The percentage of the cycle (in number of steps) spent\n",
      "     |          increasing the learning rate.\n",
      "     |          Default: 0.3\n",
      "     |      anneal_strategy (str): {'cos', 'linear'}\n",
      "     |          Specifies the annealing strategy: \"cos\" for cosine annealing, \"linear\" for\n",
      "     |          linear annealing.\n",
      "     |          Default: 'cos'\n",
      "     |      cycle_momentum (bool): If ``True``, momentum is cycled inversely\n",
      "     |          to learning rate between 'base_momentum' and 'max_momentum'.\n",
      "     |          Default: True\n",
      "     |      base_momentum (float or list): Lower momentum boundaries in the cycle\n",
      "     |          for each parameter group. Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the peak of a cycle, momentum is\n",
      "     |          'base_momentum' and learning rate is 'max_lr'.\n",
      "     |          Default: 0.85\n",
      "     |      max_momentum (float or list): Upper momentum boundaries in the cycle\n",
      "     |          for each parameter group. Functionally,\n",
      "     |          it defines the cycle amplitude (max_momentum - base_momentum).\n",
      "     |          Note that momentum is cycled inversely\n",
      "     |          to learning rate; at the start of a cycle, momentum is 'max_momentum'\n",
      "     |          and learning rate is 'base_lr'\n",
      "     |          Default: 0.95\n",
      "     |      div_factor (float): Determines the initial learning rate via\n",
      "     |          initial_lr = max_lr/div_factor\n",
      "     |          Default: 25\n",
      "     |      final_div_factor (float): Determines the minimum learning rate via\n",
      "     |          min_lr = initial_lr/final_div_factor\n",
      "     |          Default: 1e4\n",
      "     |      last_epoch (int): The index of the last batch. This parameter is used when\n",
      "     |          resuming a training job. Since `step()` should be invoked after each\n",
      "     |          batch instead of after each epoch, this number represents the total\n",
      "     |          number of *batches* computed, not the total number of epochs computed.\n",
      "     |          When last_epoch=-1, the schedule is started from the beginning.\n",
      "     |          Default: -1\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> data_loader = torch.utils.data.DataLoader(...)\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     for batch in data_loader:\n",
      "     |      >>>         train_batch(...)\n",
      "     |      >>>         scheduler.step()\n",
      "     |  \n",
      "     |  \n",
      "     |  .. _Super-Convergence\\: Very Fast Training of Neural Networks Using Large Learning Rates:\n",
      "     |      https://arxiv.org/abs/1708.07120\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneCycleLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, max_lr, total_steps=None, epochs=None, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ReduceLROnPlateau(builtins.object)\n",
      "     |  ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
      "     |  \n",
      "     |  Reduce learning rate when a metric has stopped improving.\n",
      "     |  Models often benefit from reducing the learning rate by a factor\n",
      "     |  of 2-10 once learning stagnates. This scheduler reads a metrics\n",
      "     |  quantity and if no improvement is seen for a 'patience' number\n",
      "     |  of epochs, the learning rate is reduced.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      mode (str): One of `min`, `max`. In `min` mode, lr will\n",
      "     |          be reduced when the quantity monitored has stopped\n",
      "     |          decreasing; in `max` mode it will be reduced when the\n",
      "     |          quantity monitored has stopped increasing. Default: 'min'.\n",
      "     |      factor (float): Factor by which the learning rate will be\n",
      "     |          reduced. new_lr = lr * factor. Default: 0.1.\n",
      "     |      patience (int): Number of epochs with no improvement after\n",
      "     |          which learning rate will be reduced. For example, if\n",
      "     |          `patience = 2`, then we will ignore the first 2 epochs\n",
      "     |          with no improvement, and will only decrease the LR after the\n",
      "     |          3rd epoch if the loss still hasn't improved then.\n",
      "     |          Default: 10.\n",
      "     |      threshold (float): Threshold for measuring the new optimum,\n",
      "     |          to only focus on significant changes. Default: 1e-4.\n",
      "     |      threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
      "     |          dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
      "     |          mode or best * ( 1 - threshold ) in `min` mode.\n",
      "     |          In `abs` mode, dynamic_threshold = best + threshold in\n",
      "     |          `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
      "     |      cooldown (int): Number of epochs to wait before resuming\n",
      "     |          normal operation after lr has been reduced. Default: 0.\n",
      "     |      min_lr (float or list): A scalar or a list of scalars. A\n",
      "     |          lower bound on the learning rate of all param groups\n",
      "     |          or each group respectively. Default: 0.\n",
      "     |      eps (float): Minimal decay applied to lr. If the difference\n",
      "     |          between new and old lr is smaller than eps, the update is\n",
      "     |          ignored. Default: 1e-8.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      "     |      >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
      "     |      >>> for epoch in range(10):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     val_loss = validate(...)\n",
      "     |      >>>     # Note that step should be called after validate()\n",
      "     |      >>>     scheduler.step(val_loss)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  is_better(self, a, best)\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |  \n",
      "     |  step(self, metrics, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  in_cooldown\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StepLR(_LRScheduler)\n",
      "     |  StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |  \n",
      "     |  Decays the learning rate of each parameter group by gamma every\n",
      "     |  step_size epochs. Notice that such decay can happen simultaneously with\n",
      "     |  other changes to the learning rate from outside this scheduler. When\n",
      "     |  last_epoch=-1, sets initial lr as lr.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      optimizer (Optimizer): Wrapped optimizer.\n",
      "     |      step_size (int): Period of learning rate decay.\n",
      "     |      gamma (float): Multiplicative factor of learning rate decay.\n",
      "     |          Default: 0.1.\n",
      "     |      last_epoch (int): The index of last epoch. Default: -1.\n",
      "     |      verbose (bool): If ``True``, prints a message to stdout for\n",
      "     |          each update. Default: ``False``.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> # Assuming optimizer uses lr = 0.05 for all groups\n",
      "     |      >>> # lr = 0.05     if epoch < 30\n",
      "     |      >>> # lr = 0.005    if 30 <= epoch < 60\n",
      "     |      >>> # lr = 0.0005   if 60 <= epoch < 90\n",
      "     |      >>> # ...\n",
      "     |      >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
      "     |      >>> for epoch in range(100):\n",
      "     |      >>>     train(...)\n",
      "     |      >>>     validate(...)\n",
      "     |      >>>     scheduler.step()\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StepLR\n",
      "     |      _LRScheduler\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_lr(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  get_last_lr(self)\n",
      "     |      Return last computed learning rate by current scheduler.\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict)\n",
      "     |      Loads the schedulers state.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          state_dict (dict): scheduler state. Should be an object returned\n",
      "     |              from a call to :meth:`state_dict`.\n",
      "     |  \n",
      "     |  print_lr(self, is_verbose, group, lr, epoch=None)\n",
      "     |      Display the current learning rate.\n",
      "     |  \n",
      "     |  state_dict(self)\n",
      "     |      Returns the state of the scheduler as a :class:`dict`.\n",
      "     |      \n",
      "     |      It contains an entry for every variable in self.__dict__ which\n",
      "     |      is not the optimizer.\n",
      "     |  \n",
      "     |  step(self, epoch=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _LRScheduler:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    bisect_right(...)\n",
      "        bisect_right(a, x[, lo[, hi]]) -> index\n",
      "        \n",
      "        Return the index where to insert item x in list a, assuming a is sorted.\n",
      "        \n",
      "        The return value i is such that all e in a[:i] have e <= x, and all e in\n",
      "        a[i:] have e > x.  So if x already appears in the list, i points just\n",
      "        beyond the rightmost x already there\n",
      "        \n",
      "        Optional args lo (default 0) and hi (default len(a)) bound the\n",
      "        slice of a to be searched.\n",
      "\n",
      "DATA\n",
      "    EPOCH_DEPRECATION_WARNING = 'The epoch parameter in `scheduler.step()`...\n",
      "    SAVE_STATE_WARNING = 'Please also save or load the state of the optimi...\n",
      "    inf = inf\n",
      "\n",
      "FILE\n",
      "    d:\\programmefiles\\python\\miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim.lr_scheduler.MultiStepLR()\n",
    "```python\n",
    "optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones,\n",
    "    gamma=0.1,\n",
    "    last_epoch=-1,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:     \n",
    "当达到`miletones`指定 epoch 时，对每一个参数组的学习率以因子`gamma`进行衰减；需要注意的是，该 scheduler 设定的衰减可与外部的学习率衰减同时发生\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- optimizer: Wrapped optimizer\n",
    "- milestones: epoch 索引组成的列表，其元素必须为递增的\n",
    "- gamma: 学习率衰减的乘积因子，默认 0.1\n",
    "- last_epoch: 使用权重衰减的最后一个 epoch，默认为 -1，这种情况下设置初始学习率`lr`即为`lr`\n",
    "\n",
    "**File**:      \\torch\\optim\\lr_scheduler.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Example\n",
    "\n",
    "假设初始 $lr = 0.05$，则当 $epoch < 30$ 时 $lr = 0.05$，当 $30 \\le epoch \\le 80$ 时 $lr = 0.005$，当 $epoch \\ge 80$ 时 $lr = 0.0005$\n",
    "\n",
    "```python\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    # some operations ...\n",
    "    scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "print(type(pathlib))\n",
    "\n",
    "# for k, v in Path.__dict__.items():\n",
    "#     print(\"key\", k, sep=\"\")\n",
    "#     print(\"value\", v, sep=\"\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
