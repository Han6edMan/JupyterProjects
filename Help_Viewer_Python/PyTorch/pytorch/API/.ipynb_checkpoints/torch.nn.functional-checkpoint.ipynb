{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### CLASSES\n",
    "    Tensor\n",
    "\n",
    "##### FUNCTIONS\n",
    "    _adaptive_max_pool1d\n",
    "    _adaptive_max_pool2d\n",
    "    _adaptive_max_pool3d\n",
    "    _add_docstr\n",
    "    _fractional_max_pool2d\n",
    "    _fractional_max_pool3d\n",
    "    _get_softmax_dim\n",
    "    _in_projection\n",
    "    _in_projection_packed\n",
    "    _infer_size\n",
    "    _list_with_default\n",
    "    _max_pool1d\n",
    "    _max_pool2d\n",
    "    _max_pool3d\n",
    "    _no_grad_embedding_renorm_\n",
    "    _overload\n",
    "    _pad\n",
    "    _pad_circular\n",
    "    _pair\n",
    "    _scaled_dot_product_attention\n",
    "    _single\n",
    "    _threshold\n",
    "    _triple\n",
    "    _unpool_output_size\n",
    "    _verify_batch_size\n",
    "    _verify_spatial_size\n",
    "    adaptive_avg_pool1d\n",
    "    adaptive_avg_pool2d\n",
    "    adaptive_avg_pool3d\n",
    "    adaptive_max_pool1d\n",
    "    adaptive_max_pool1d_with_indices\n",
    "    adaptive_max_pool2d\n",
    "    adaptive_max_pool2d_with_indices\n",
    "    adaptive_max_pool3d\n",
    "    adaptive_max_pool3d_with_indices\n",
    "    affine_grid\n",
    "    alpha_dropout\n",
    "    assert_int_or_pair\n",
    "    avg_pool1d\n",
    "    avg_pool2d\n",
    "    avg_pool3d\n",
    "    batch_norm\n",
    "    bilinear\n",
    "    binary_cross_entropy\n",
    "    binary_cross_entropy_with_logits\n",
    "    boolean_dispatch\n",
    "    celu\n",
    "    celu_\n",
    "    channel_shuffle\n",
    "    conv1d\n",
    "    conv2d\n",
    "    conv3d\n",
    "    conv_tbc\n",
    "    conv_transpose1d\n",
    "    conv_transpose2d\n",
    "    conv_transpose3d\n",
    "    cosine_embedding_loss\n",
    "    cosine_similarity\n",
    "    cross_entropy\n",
    "    ctc_loss\n",
    "    dropout\n",
    "    dropout2d\n",
    "    dropout3d\n",
    "    elu\n",
    "    elu_\n",
    "    embedding\n",
    "    embedding_bag\n",
    "    feature_alpha_dropout\n",
    "    fold\n",
    "    fractional_max_pool2d\n",
    "    fractional_max_pool2d_with_indices\n",
    "    fractional_max_pool3d\n",
    "    fractional_max_pool3d_with_indices\n",
    "    gaussian_nll_loss\n",
    "    gelu\n",
    "    glu\n",
    "    grid_sample\n",
    "    group_norm\n",
    "    gumbel_softmax\n",
    "    handle_torch_function\n",
    "    hardshrink\n",
    "    hardsigmoid\n",
    "    hardswish\n",
    "    hardtanh\n",
    "    hardtanh_\n",
    "    has_torch_function\n",
    "    has_torch_function_unary\n",
    "    has_torch_function_variadic\n",
    "    hinge_embedding_loss\n",
    "    huber_loss\n",
    "    instance_norm\n",
    "    interpolate\n",
    "    kl_div\n",
    "    l1_loss\n",
    "    layer_norm\n",
    "    leaky_relu\n",
    "    leaky_relu_\n",
    "    linear\n",
    "    local_response_norm\n",
    "    log_softmax\n",
    "    logsigmoid\n",
    "    lp_pool1d\n",
    "    lp_pool2d\n",
    "    margin_ranking_loss\n",
    "    max_pool1d\n",
    "    max_pool1d_with_indices\n",
    "    max_pool2d\n",
    "    max_pool2d_with_indices\n",
    "    max_pool3d\n",
    "    max_pool3d_with_indices\n",
    "    max_unpool1d\n",
    "    max_unpool2d\n",
    "    max_unpool3d\n",
    "    mish\n",
    "    mse_loss\n",
    "    multi_head_attention_forward\n",
    "    multi_margin_loss\n",
    "    multilabel_margin_loss\n",
    "    multilabel_soft_margin_loss\n",
    "    nll_loss\n",
    "    normalize\n",
    "    one_hot\n",
    "    pad\n",
    "    pairwise_distance\n",
    "    pdist\n",
    "    pixel_shuffle\n",
    "    pixel_unshuffle\n",
    "    poisson_nll_loss\n",
    "    prelu\n",
    "    relu\n",
    "    relu6\n",
    "    relu_\n",
    "    rrelu\n",
    "    rrelu_\n",
    "    selu\n",
    "    selu_\n",
    "    sigmoid\n",
    "    silu\n",
    "    smooth_l1_loss\n",
    "    soft_margin_loss\n",
    "    softmax\n",
    "    softmin\n",
    "    softplus\n",
    "    softshrink\n",
    "    softsign\n",
    "    tanh\n",
    "    tanhshrink\n",
    "    threshold\n",
    "    threshold_\n",
    "    triplet_margin_loss\n",
    "    triplet_margin_with_distance_loss\n",
    "    unfold\n",
    "    upsample\n",
    "    upsample_bilinear\n",
    "    upsample_nearest\n",
    "\n",
    "##### DATA\n",
    "    BroadcastingList1 = <torch._jit_internal.BroadcastingListCls object>\n",
    "    BroadcastingList2 = <torch._jit_internal.BroadcastingListCls object>\n",
    "    BroadcastingList3 = <torch._jit_internal.BroadcastingListCls object>\n",
    "    Callable = typing.Callable\n",
    "    GRID_SAMPLE_INTERPOLATION_MODES = {'bicubic': 2, 'bilinear': 0, ...\n",
    "    GRID_SAMPLE_PADDING_MODES = {'border': 1, 'reflection': 2, 'zeros': 0}\n",
    "    List = typing.List\n",
    "    Optional = typing.Optional\n",
    "    Tuple = typing.Tuple\n",
    "    reproducibility_notes = {'backward_reproducibility_note': 'This...\n",
    "    tf32_notes = {'tf32_note': 'This operator supports :ref:..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# F.avg_pool2d()\n",
    "```python\n",
    "avg_pool2d(\n",
    "    input,\n",
    "    kernel_size,\n",
    "    stride=None,\n",
    "    padding=0,\n",
    "    ceil_mode=False,\n",
    "    count_include_pad=True,\n",
    "    divisor_override=None\n",
    ") -> Tensor\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "以步长为 $S_h \\times S_w$ 对一个 2 维特征图的每个 $k_h \\times k_w$ 区域进行平均池化，输出特征图数量与输入特征图数量相等，更多细节参见`torch.nn.AvgPool2d`\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- input: 形状为 $(N_i \\times C_i \\times H_i \\times W_i)$ 的张量\n",
    "\n",
    "- kernel_size: 可以为单一的整数，或为元祖`(k_h, k_w)`\n",
    "\n",
    "- stride: 可以为单一的整数，或为元祖`(s_h, s_w)`，默认与`kernel_size`相同\n",
    "\n",
    "- padding: 可以为单一的整数，或为元祖`(pad_h, pad_w)`，默认为 0\n",
    "\n",
    "- ceil_mode: 决定在步长不为一时输出特征图的形状，True 时进行向上取整，否则进行向下取整；默认为 False\n",
    "\n",
    "- count_include_pad: True 时平均池化的运算会将边界扩充的元素考虑在内，默认 True\n",
    "\n",
    "- divisor_override: 指明时将使用该参数作为除数（即分母），否则会使用池化区域的大小\n",
    "\n",
    "**Type**:      builtin_function_or_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## F.conv2d()\n",
    "```python\n",
    "F.conv2d(\n",
    "    input,\n",
    "    weight,\n",
    "    bias=None,\n",
    "    stride=1,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    groups=1\n",
    ")\n",
    "```\n",
    "\n",
    "对输入进行 2D 卷积，该算子支持`TensorFloat32<tf32_on_ampere>`；更多细节参见`torch.nn.Conv2d`的说明文档；\n",
    "\n",
    "需要说明的是，对于需要使用CUDA的CuDNN backend 的情况，该操作可能会选择一个具有不确定性的算法以提高性能；若需要算法保持稳定，可设置`torch.backends.cudnn.deterministic =True`，但这同时可能会损失一定的性能；更多背景知识可以参见`/notes/randomness`\n",
    "\n",
    "##### Args:\n",
    "- input: input tensor of shape $(\\text{minibatch} , \\text{in\\_channels} , iH , iW)$\n",
    "- weight: filters of shape :math:`(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)`\n",
    "- bias: optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: `None`\n",
    "- stride: the stride of the convolving kernel. Can be a single number or a tuple `(sH, sW)`. Default: 1\n",
    "- padding: implicit paddings on both sides of the input. Can be a string {'valid', 'same'}, single number or a tuple `(padH, padW)`. Default: 0\n",
    "    - `padding='valid'` is the same as no padding.\n",
    "    - `padding='same'` pads the input so the output has the shape as the input. However, this mode doesn't support any stride values other than 1.\n",
    "    - .. warning::\n",
    "          For ``padding='same'``, if the ``weight`` is even-length and ``dilation`` is odd in any dimension, a full :func:`pad` operation may be needed internally. Lowering performance.\n",
    "- dilation: the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1\n",
    "- groups: split input into groups, :math:`\\text{in\\_channels}` should be divisible by the number of groups. Default: 1\n",
    "\n",
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    ">>> # With square kernels and equal stride\n",
    ">>> filters = torch.randn(8, 4, 3, 3)\n",
    ">>> inputs = torch.randn(1, 4, 5, 5)\n",
    ">>> F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F.pad()\n",
    "`F.pad(input, pad, mode='constant', value=0)`\n",
    "\n",
    "对一个张量进行扩充。\n",
    "\n",
    "需要说明的是，对于需要使用CUDA的CuDNN backend 的情况，该操作可能会选择一个具有不确定性的算法以提高性能；若需要算法保持稳定，可设置`torch.backends.cudnn.deterministic =True`，但这同时可能会损失一定的性能；更多背景知识可以参见`/notes/randomness`\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- input: 略\n",
    "- pad: 对张量各维度扩充的元素个数，应为含有 2m 个元素的元祖，此时`input`最后 m 维将会被填充；当 m 为奇数时向下取整；例如只填充`input`最后一维，则`pad`形状应为 $(N_{left}, N_{right})$，最后两维则 $(N_{left}, N_{right}, N_{top}, N_{bottom})$，最后三维则 $(N_{left}, N_{right}, N_{top}, N_{bottom}, N_{front}, N_{back})$\n",
    "- mode: 可以是`'constant'`, `'reflect'`, `'replicate'` or `'circular'`，默认`'constant'`；\n",
    "    - `constant`模式可以对任意维度的张量进行填充；\n",
    "    - `replicate`模式对 5D 张量的最后 3 个维度进行填充，对 4D 张量最后 2 个维度进行填充，对 3D 张量最后 1 个维度进行填充；\n",
    "    - `reflect`模式只能对 4D 张量最后 2 个维度进行填充，对 3D 张量最后 1 个维度进行填充；\n",
    "    - Tensor values at the beginning are used to pad the end, and values at the\n",
    "    end are used to pad the beginning. For example, consider a single dimension\n",
    "    with values [0, 1, 2, 3]. With circular padding of (1, 1) it would be\n",
    "    padded to [3, 0, 1, 2, 3, 0], and with padding (1, 2) it would be padded to\n",
    "    [3, 0, 1, 2, 3, 0, 1]. If negative padding is applied then the ends of the\n",
    "    tensor get removed. With circular padding of (-1, -1) the previous example\n",
    "    would become [1, 2]. Circular padding of (-1, 1) would produce\n",
    "    [1, 2, 3, 1]\n",
    "    - 关于各种扩充机制详见`torch.nn.ConstantPad2d`、`torch.nn.ReflectionPad2d`、`torch.nn.ReplicationPad2d`；\n",
    "\n",
    "- value: ``'constant'``模式下填充的值，默认 0\n",
    "\n",
    "##### Examples\n",
    "1. 在张量左侧扩充 0 列、右侧扩充 1 列、上部扩充 2 行、下部 1 行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [1, 2, 0],\n",
      "        [3, 4, 0],\n",
      "        [5, 6, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 2*3*3*2+1).reshape([2, 3, 3, 2])\n",
    "out = F.pad(x, (0, 1, 2, 1), \"constant\", 0)\n",
    "print(out[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 在张量左侧扩充 0 列、右侧扩充 1 列、上部扩充 2 行、下部 1 行、前后各扩充 1 块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 1,  2,  0],\n",
      "         [ 3,  4,  0],\n",
      "         [ 5,  6,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 7,  8,  0],\n",
      "         [ 9, 10,  0],\n",
      "         [11, 12,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [13, 14,  0],\n",
      "         [15, 16,  0],\n",
      "         [17, 18,  0],\n",
      "         [ 0,  0,  0]],\n",
      "\n",
      "        [[ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "out = F.pad(x, (0, 1, 2, 1, 1, 1), \"constant\", 0)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 对 ResNet 残差层的 pad 方案的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 4, 2])\n",
      "tensor([[[[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.],\n",
      "          [0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "channels = 8\n",
    "def lambdaLayer(lambda_, t):\n",
    "    return lambda_(t)\n",
    "\n",
    "out = lambdaLayer(\n",
    "    lambda y: F.pad(\n",
    "        y[:, :, ::2, ::2],\n",
    "        (0, 0, 0, 0, channels//4, channels//4),\n",
    "        \"constant\", 0),\n",
    "    torch.ones(1, 3, 8, 4)\n",
    ")\n",
    "\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[11.,  8.,  9., 10., 11.,  8.],\n",
       "          [ 3.,  0.,  1.,  2.,  3.,  0.],\n",
       "          [ 7.,  4.,  5.,  6.,  7.,  4.],\n",
       "          [11.,  8.,  9., 10., 11.,  8.],\n",
       "          [ 3.,  0.,  1.,  2.,  3.,  0.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canvas = torch.arange(3*4, dtype=float).reshape(1, 1, 3, 4)\n",
    "F.pad(canvas, (1,1,1,1), \"circular\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
