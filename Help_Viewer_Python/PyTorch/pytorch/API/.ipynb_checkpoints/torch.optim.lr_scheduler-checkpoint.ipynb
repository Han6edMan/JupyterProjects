{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lr_scheduler\n",
    "\n",
    "**CLASSES：**\n",
    "- builtins.object\n",
    "    - ReduceLROnPlateau\n",
    "    - \\_LRScheduler\n",
    "        - CosineAnnealingLR\n",
    "        - CosineAnnealingWarmRestarts\n",
    "        - CyclicLR\n",
    "        - ExponentialLR\n",
    "        - LambdaLR\n",
    "        - MultiStepLR\n",
    "        - MultiplicativeLR\n",
    "        - OneCycleLR\n",
    "        - StepLR\n",
    "\n",
    "**FUNCTIONS：**\n",
    "- bisect_right()\n",
    "\n",
    "**DATA**\n",
    "- EPOCH_DEPRECATION_WARNING\n",
    "- SAVE_STATE_WARNING\n",
    "- inf\n",
    "\n",
    "**FILE**:  \\torch\\optim\\lr_scheduler.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr_scheduler.ExponentialLR()\n",
    "```python\n",
    "lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "**Docstring**：每个 epoch 对所有参数组的学习率衰减 gamma\n",
    "\n",
    "**Args**:\n",
    "- optimizer：打包的优化器\n",
    "- gamma：应为浮点型，权重衰减所乘的因子；\n",
    "- last_epoch：使用学习率方案的最后一个 epoch 的索引，默认 -1\n",
    "- verbose：布尔值；默认 False；True 时将更新信息打印至 stdout；\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr_scheduler.LambdaLR()\n",
    "```python\n",
    "lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda,\n",
    "    last_epoch=-1,\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "**Docstring**：     将每个参数组的学习率设置为初始`lr`乘以给定函数，当`last_epoch=-1`时将初始`lr`设置为`lr`；\n",
    "\n",
    "**Args**:\n",
    "- optimizer：打包的优化器\n",
    "- gamma：应为浮点型，权重衰减所乘的因子；\n",
    "- lr_lambda：可以是一个函数或函数组成的列表；所有函数均以 epoch 作为输入并返回一个学习率所乘的因子；对于函数列表的情况，每个函数对应`optimizer.param_groups`中的一组参数；\n",
    "- verbose：布尔值；默认 False；True 时将更新信息打印至 stdout；\n",
    "\n",
    "\n",
    "**Type:**           type\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "lambda1 = lambda epoch: epoch // 30\n",
    "lambda2 = lambda epoch: 0.95 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr_scheduler.CosineAnnealingWarmRestarts()\n",
    "```python\n",
    "lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0,\n",
    "    T_mult=1,\n",
    "    eta_min=0,\n",
    "    last_epoch=-1,\n",
    "    verbose=False,\n",
    ")\n",
    "```\n",
    "**Docstring**：利用余弦退火方案对每个参数组的学习率进行配置；其中 $\\eta_{max}$ 被设置为初始学习率，$T_{i}$ 为SGDR两次 warm 重启之间的 epoch 个数，$T_{cur}$ 为重启后 epoch 的数量；\n",
    "\n",
    "$$\n",
    "\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)\n",
    "$$\n",
    "可以看出，$T_{cur}=0$ 时 $\\eta_t=\\eta_{max}$，$T_{cur}=T_{i}$ 时 $\\eta_t = \\eta_{min}$；更多细节参见文献 [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)；\n",
    "\n",
    "**Args**:\n",
    "- T_0：整型；第一次重启的迭代次数；\n",
    "- T_mult：整型；重启后与 $T_{i}$ 相乘以增大 $T_{i}$ 的因子，默认 1；\n",
    "- eta_min：浮点数；最小的学习率，默认 0.0；\n",
    "- last_epoch：整型，最后一个 epoch，默认 -1；\n",
    "- verbose：布尔值；默认 False；True 时将更新信息打印至 stdout；\n",
    "\n",
    "**Type**:           type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
