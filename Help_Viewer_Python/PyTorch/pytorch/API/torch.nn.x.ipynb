{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__call__\n",
      "__delattr__\n",
      "__dir__\n",
      "__getattr__\n",
      "__init__\n",
      "__repr__\n",
      "__setattr__\n",
      "__setstate__\n",
      "_apply\n",
      "_call_impl\n",
      "_get_name\n",
      "_load_from_state_dict\n",
      "_named_members\n",
      "_register_load_state_dict_pre_hook\n",
      "_register_state_dict_hook\n",
      "_replicate_for_data_parallel\n",
      "_save_to_state_dict\n",
      "_slow_forward\n",
      "add_module\n",
      "apply\n",
      "bfloat16\n",
      "buffers\n",
      "children\n",
      "cpu\n",
      "cuda\n",
      "double\n",
      "eval\n",
      "extra_repr\n",
      "float\n",
      "forward\n",
      "half\n",
      "load_state_dict\n",
      "modules\n",
      "named_buffers\n",
      "named_children\n",
      "named_modules\n",
      "named_parameters\n",
      "parameters\n",
      "register_backward_hook\n",
      "register_buffer\n",
      "register_forward_hook\n",
      "register_forward_pre_hook\n",
      "register_parameter\n",
      "requires_grad_\n",
      "share_memory\n",
      "state_dict\n",
      "to\n",
      "train\n",
      "type\n",
      "zero_grad\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(nn.Module.__dict__.items()):\n",
    "    if callable(v):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(nn.Module)\n",
    "\n",
    "Help on class Module in module torch.nn.modules.module:\n",
    "\n",
    "class Module(builtins.object)\n",
    "\n",
    "\n",
    "Methods defined here:\n",
    "\n",
    "add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> \n",
    "\n",
    "apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
    "\n",
    "bfloat16(self: ~T) -> ~T\n",
    "    Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
    "    \n",
    "    Returns:\n",
    "        Module: self\n",
    "\n",
    "buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
    "    \n",
    "\n",
    "children(self) -> Iterator[ForwardRef('Module')]\n",
    "    \n",
    "cpu(self: ~T) -> ~T\n",
    "    \n",
    "\n",
    "cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
    "    \n",
    "double(self: ~T) -> ~T\n",
    "    \n",
    "eval(self: ~T) -> ~T\n",
    "    \n",
    "extra_repr(self) -> str\n",
    "\n",
    "float(self: ~T) -> ~T\n",
    "    \n",
    "forward = _forward_unimplemented(self, *input: Any) -> None\n",
    "    \n",
    "\n",
    "half(self: ~T) -> ~T\n",
    "    \n",
    "load_state_dict(self, state_dict: Dict[str, torch.Tensor], strict: bool = True)\n",
    "    \n",
    "\n",
    "modules(self) -> Iterator[ForwardRef('Module')]\n",
    "    \n",
    "named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
    "   \n",
    "named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
    "    \n",
    "named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
    "    \n",
    "named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
    "    \n",
    "parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
    "    \n",
    "register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
    "    \n",
    "register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
    "    \n",
    "register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
    "    \n",
    "register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
    "    \n",
    "register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
    "    \n",
    "requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
    "    \n",
    "share_memory(self: ~T) -> ~T\n",
    "\n",
    "state_dict(self, destination=None, prefix='', keep_vars=False)\n",
    "    \n",
    "to(self, *args, **kwargs)\n",
    "    \n",
    "train(self: ~T, mode: bool = True) -> ~T\n",
    "    \n",
    "type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
    "    \n",
    "zero_grad(self, set_to_none: bool = False) -> None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module()\n",
    "`nn.Module()`\n",
    "\n",
    "所有神经网络模块的基类，所有搭建的神经网络模型应该继承这个类；该模块还包含很多其他子模块，进而从而嵌套，即可以将子模块作为类属性，如将`nn.Conv2d`作为类属性等；以这种方式声明的子模块将被注册，由`nn.Module`和`ScriptModule`共享，当调用`to`时也将转换它们的参数\n",
    "\n",
    "**File**: torch\\nn\\modules\\module.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**methods**\n",
    "\n",
    "    __call__\n",
    "    __dir__\n",
    "    __repr__\n",
    "    __setstate__\n",
    "    _apply\n",
    "    _call_impl\n",
    "    _get_name\n",
    "    _load_from_state_dict\n",
    "    _named_members\n",
    "    _register_load_state_dict_pre_hook\n",
    "    _register_state_dict_hook\n",
    "    _replicate_for_data_parallel\n",
    "    _save_to_state_dict\n",
    "    _slow_forward\n",
    "    add_module\n",
    "    apply\n",
    "    bfloat16\n",
    "    buffers\n",
    "    children\n",
    "    cpu\n",
    "    cuda\n",
    "    double\n",
    "    eval\n",
    "    extra_repr\n",
    "    float\n",
    "    forward\n",
    "    half\n",
    "    load_state_dict\n",
    "    modules\n",
    "    named_buffers\n",
    "    named_children\n",
    "    named_modules\n",
    "    named_parameters\n",
    "    parameters\n",
    "    register_backward_hook\n",
    "    register_buffer\n",
    "    register_forward_hook\n",
    "    register_forward_pre_hook\n",
    "    register_parameter\n",
    "    requires_grad_\n",
    "    share_memory\n",
    "    state_dict\n",
    "    to\n",
    "    train\n",
    "    type\n",
    "    zero_grad\n",
    "\n",
    "## nn.Module.train()\n",
    "\n",
    "`<mudule>.train(mode = True)`\n",
    "\n",
    "\n",
    "`mode`为 True 时设定模块为训练模式，False 时评估模式，这只对某些模块产生影响；具体模块在训练/评估模式下的行为请参阅相关文档\n",
    "\n",
    "**Type**:      function\n",
    "\n",
    "## nn.Module.eval()\n",
    "\n",
    "`<module_name>.eval()`\n",
    "\n",
    "设定模块为评估状态，这只对某些模块产生影响；具体模块在训练/评估模式下的行为请参阅相关文档，该方法等价于`<module_name>.train(False)`\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.state_dict()\n",
    "\n",
    "`<module>.state_dict(destination=None, prefix='', keep_vars=False)`\n",
    "\n",
    "**Docstring:**\n",
    "\n",
    "返回一个包含了模块的所有状态的字典，其中参数和持久缓冲区 (persistent buffers)(如 running averages)，键值为相关参数和缓冲区的名称\n",
    "\n",
    "**Type**: function\n",
    "\n",
    "### Example\n",
    "```python\n",
    "module.state_dict().keys()  # => ['bias', 'weight']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.load_state_dict\n",
    "```python\n",
    "<module>.load_state_dict(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    strict=True,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "将参数和缓冲区从`state_dict`复制给该模块及其实例化对象，并一个返回带有``missing_keys``和``unexpected_keys``字段的``NamedTuple``；其中``missing_keys``是一个包括了丢失的键的有字符串组成的列表，``unexpected_keys``是一个是包含意外键的字符串列表；若`strict`为`True`，则`state_dict`的键值必须与`torch.nn.Module.state_dict`函数返回的键值完全匹配。\n",
    "\n",
    "**Args**\n",
    "\n",
    "- state_dict: 包含参数和持久缓冲区的字典\n",
    "\n",
    "- strict:  True 时强制`state_dict`中的键与该类的`state_dict`函数返回的键匹配\n",
    "\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.named_children()\n",
    "`<module>.named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "返回一个迭代器，其可生成该模块的所有直接子模块的名称及模块本身`(string, Module)`\n",
    "\n",
    "**File**:   \\torch\\nn\\modules\\module.py\n",
    "\n",
    "**Type**:      function\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "model = TestModel()\n",
    "for named_children in model.named_children():\n",
    "    print(named_children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.children()\n",
    "`nn.Module.children(self) -> Iterator[ForwardRef('Module')]`\n",
    "\n",
    "返回包含直接子模块的迭代器\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1)) is not an Iterator\n",
      "ReLU() is not an Iterator\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1)) is not an Iterator\n",
      "Iterator `module.children()`: <generator object Module.children at 0x0000025673457648>\n"
     ]
    }
   ],
   "source": [
    "def has_children(module):\n",
    "    try:\n",
    "        next(module.children())\n",
    "        print(\"Iterator `module.children()`:\", module.children())\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        print(\"{} is not an Iterator\".format(module))\n",
    "        return False\n",
    "\n",
    "class TestModel_Parent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel_Parent, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.model_chilren = TestModel()\n",
    "\n",
    "model_parent = TestModel_Parent()\n",
    "for name, module in model_parent.named_children():\n",
    "    if has_children(module):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.apply()\n",
    "`<model>.apply(fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "递归地将`fn`应用于自身及子模块（即由``children()``返回的模块）并返回应用`fn`后的`self`；典型的用法包括初始化模型的参数，另见`nn-init-doc`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- fn: 要应作用每个模块的函数，其输入为一模块，输出为 None\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.fill_(1.0)\n",
    "        print(m.weight)\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.register_forward_hook()\n",
    "```python\n",
    "model.register_forward_hook(hook: Callable[..., NoneType]) \n",
    "-> torch.utils.hooks.RemovableHandle\n",
    "```\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "在模块上注册一个`forward`的 hook，每次`forward`计算得到出输后 hook 均会被调用一次。\n",
    "\n",
    "hook 的其定义形式为\n",
    "\n",
    "```python\n",
    "hook(module, input, output) -> None or modified output\n",
    "```\n",
    "\n",
    "其中`input`只包含传递给`module`的位置参数，关键字参数只会传递给``forward``而不会传递给 hook；hook 可能会对`output`进行调整，同时也会对`input`进行 inplace 地调整，但由于 hook 是在`forward`调用之后被调用的，故其对`input`调整并不产生影响；此函数返回属于`torch.utils.hooks.RemovableHandle`类的 handle，可通过调用``handle.remove()``来移除附加的 hook\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.modules()\n",
    "`nn.Module.modules(self) -> Iterator[ForwardRef('Module')]`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "返回网络中所有模块及其子模块的迭代器；需要注意的是，重复的模块只返回一次，可参见下面的例子；\n",
    "\n",
    "**File**:    torch\\nn\\modules\\module.py\n",
    "\n",
    "**Type**:      function\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Linear(in_features=2, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Linear(in_features=2, out_features=2, bias=True)\n",
      "\n",
      "ReLU(inplace=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(2, 2)\n",
    "net = nn.Sequential(\n",
    "    linear,\n",
    "    linear,\n",
    "    nn.ReLU(inplace=True),\n",
    "    linear\n",
    ")\n",
    "for module in net.modules():\n",
    "    print(module, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module.named_modules()\n",
    "```python\n",
    "model.named_modules(\n",
    "    memo: Union[Set[ForwardRef('Module')], NoneType] = None,\n",
    "    prefix: str = '',\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "返回网络中所有模块及其子模块的迭代器，该迭代器可以以`(name, Module)`的形式输出某一模块的名称以及模块自身；需要注意的是，重复的模块只返回一次，可参见下面的例子；\n",
    "\n",
    "**File**:  \\torch\\nn\\modules\\module.py\n",
    "\n",
    "**Type**:      function\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(2, 2)\n",
    "net = nn.Sequential(\n",
    "    linear,\n",
    "    linear,\n",
    "    nn.ReLU(inplace=True),\n",
    "    linear\n",
    ")\n",
    "print(dict(net.named_modules())[\"0\"])\n",
    "\n",
    "# for name, m in net.named_modules():\n",
    "#     print(name+\":\", m, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"yield\" statement\n",
      "*********************\n",
      "\n",
      "   yield_stmt ::= yield_expression\n",
      "\n",
      "A \"yield\" statement is semantically equivalent to a yield expression.\n",
      "The yield statement can be used to omit the parentheses that would\n",
      "otherwise be required in the equivalent yield expression statement.\n",
      "For example, the yield statements\n",
      "\n",
      "   yield <expr>\n",
      "   yield from <expr>\n",
      "\n",
      "are equivalent to the yield expression statements\n",
      "\n",
      "   (yield <expr>)\n",
      "   (yield from <expr>)\n",
      "\n",
      "Yield expressions and statements are only used when defining a\n",
      "*generator* function, and are only used in the body of the generator\n",
      "function.  Using yield in a function definition is sufficient to cause\n",
      "that definition to create a generator function instead of a normal\n",
      "function.\n",
      "\n",
      "For full details of \"yield\" semantics, refer to the Yield expressions\n",
      "section.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(\"yield\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.DataParallel()\n",
    "`nn.DataParallel(module, device_ids=None, output_device=None, dim=0)`\n",
    "Docstring:     \n",
    "Implements data parallelism at the module level.\n",
    "\n",
    "This container parallelizes the application of the given :attr:`module` by\n",
    "splitting the input across the specified devices by chunking in the batch\n",
    "dimension (other objects will be copied once per device). In the forward\n",
    "pass, the module is replicated on each device, and each replica handles a\n",
    "portion of the input. During the backwards pass, gradients from each replica\n",
    "are summed into the original module.\n",
    "\n",
    "The batch size should be larger than the number of GPUs used.\n",
    "\n",
    ".. warning::\n",
    "    It is recommended to use :class:`~torch.nn.parallel.DistributedDataParallel`,\n",
    "    instead of this class, to do multi-GPU training, even if there is only a single\n",
    "    node. See: :ref:`cuda-nn-ddp-instead` and :ref:`ddp`.\n",
    "\n",
    "Arbitrary positional and keyword inputs are allowed to be passed into\n",
    "DataParallel but some types are specially handled. tensors will be\n",
    "**scattered** on dim specified (default 0). tuple, list and dict types will\n",
    "be shallow copied. The other types will be shared among different threads\n",
    "and can be corrupted if written to in the model's forward pass.\n",
    "\n",
    "The parallelized :attr:`module` must have its parameters and buffers on\n",
    "``device_ids[0]`` before running this :class:`~torch.nn.DataParallel`\n",
    "module.\n",
    "\n",
    ".. warning::\n",
    "    In each forward, :attr:`module` is **replicated** on each device, so any\n",
    "    updates to the running module in ``forward`` will be lost. For example,\n",
    "    if :attr:`module` has a counter attribute that is incremented in each\n",
    "    ``forward``, it will always stay at the initial value because the update\n",
    "    is done on the replicas which are destroyed after ``forward``. However,\n",
    "    :class:`~torch.nn.DataParallel` guarantees that the replica on\n",
    "    ``device[0]`` will have its parameters and buffers sharing storage with\n",
    "    the base parallelized :attr:`module`. So **in-place** updates to the\n",
    "    parameters or buffers on ``device[0]`` will be recorded. E.g.,\n",
    "    :class:`~torch.nn.BatchNorm2d` and :func:`~torch.nn.utils.spectral_norm`\n",
    "    rely on this behavior to update the buffers.\n",
    "\n",
    ".. warning::\n",
    "    Forward and backward hooks defined on :attr:`module` and its submodules\n",
    "    will be invoked ``len(device_ids)`` times, each with inputs located on\n",
    "    a particular device. Particularly, the hooks are only guaranteed to be\n",
    "    executed in correct order with respect to operations on corresponding\n",
    "    devices. For example, it is not guaranteed that hooks set via\n",
    "    :meth:`~torch.nn.Module.register_forward_pre_hook` be executed before\n",
    "    `all` ``len(device_ids)`` :meth:`~torch.nn.Module.forward` calls, but\n",
    "    that each such hook be executed before the corresponding\n",
    "    :meth:`~torch.nn.Module.forward` call of that device.\n",
    "\n",
    ".. warning::\n",
    "    When :attr:`module` returns a scalar (i.e., 0-dimensional tensor) in\n",
    "    :func:`forward`, this wrapper will return a vector of length equal to\n",
    "    number of devices used in data parallelism, containing the result from\n",
    "    each device.\n",
    "\n",
    ".. note::\n",
    "    There is a subtlety in using the\n",
    "    ``pack sequence -> recurrent network -> unpack sequence`` pattern in a\n",
    "    :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.\n",
    "    See :ref:`pack-rnn-unpack-with-data-parallelism` section in FAQ for\n",
    "    details.\n",
    "\n",
    "\n",
    "Args:\n",
    "    module (Module): module to be parallelized\n",
    "    device_ids (list of int or torch.device): CUDA devices (default: all devices)\n",
    "    output_device (int or torch.device): device location of output (default: device_ids[0])\n",
    "\n",
    "Attributes:\n",
    "    module (Module): the module to be parallelized\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "    >>> output = net(input_var)  # input_var can be on any device, including CPU\n",
    "Init docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "File:           d:\\programmefiles\\python\\anaconda3\\envs\\tensorflow2.2\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\n",
    "Type:           type\n",
    "Subclasses:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.parallel.DistributedDataParallel()\n",
    "```python\n",
    "nn.parallel.DistributedDataParallel(\n",
    "    module,\n",
    "    device_ids=None,\n",
    "    output_device=None,\n",
    "    dim=0,\n",
    "    broadcast_buffers=True,\n",
    "    process_group=None,\n",
    "    bucket_cap_mb=25,\n",
    "    find_unused_parameters=False,\n",
    "    check_reduction=False,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "在模块层面实现基于``torch.distributed``包的分布式数据并行；\n",
    "\n",
    "\n",
    "This container parallelizes the application of the given module by\n",
    "splitting the input across the specified devices by chunking in the batch\n",
    "dimension. The module is replicated on each machine and each device, and\n",
    "each such replica handles a portion of the input. During the backwards\n",
    "pass, gradients from each node are averaged.\n",
    "\n",
    "The batch size should be larger than the number of GPUs used locally.\n",
    "\n",
    "See also: :ref:`distributed-basics` and :ref:`cuda-nn-ddp-instead`.\n",
    "The same constraints on input as in :class:`torch.nn.DataParallel` apply.\n",
    "\n",
    "Creation of this class requires that ``torch.distributed`` to be already\n",
    "initialized, by calling :func:`torch.distributed.init_process_group`.\n",
    "\n",
    "``DistributedDataParallel`` is proven to be significantly faster than\n",
    ":class:`torch.nn.DataParallel` for single-node multi-GPU data\n",
    "parallel training.\n",
    "\n",
    "Here is how to use it: on each host with N GPUs, you should spawn up N\n",
    "processes, while ensuring that each process individually works on a single GPU\n",
    "from 0 to N-1. Therefore, it is your job to ensure that your training script\n",
    "operates on a single given GPU by calling:\n",
    "\n",
    "    >>> torch.cuda.set_device(i)\n",
    "\n",
    "where i is from 0 to N-1. In each process, you should refer the following\n",
    "to construct this module:\n",
    "\n",
    "    >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n",
    "    >>> model = DistributedDataParallel(model, device_ids=[i], output_device=i)\n",
    "\n",
    "In order to spawn up multiple processes per node, you can use either\n",
    "``torch.distributed.launch`` or ``torch.multiprocessing.spawn``\n",
    "\n",
    ".. note ::\n",
    "    Please refer to `PyTorch Distributed Overview <https://pytorch.org/tutorials/beginner/dist_overview.html>`__\n",
    "    for a brief introduction to all features related to distributed training.\n",
    "\n",
    ".. note:: ``nccl`` backend is currently the fastest and\n",
    "    highly recommended backend to be used with Multi-Process Single-GPU\n",
    "    distributed training and this applies to both single-node and multi-node\n",
    "    distributed training\n",
    "\n",
    ".. note:: This module also supports mixed-precision distributed training.\n",
    "    This means that your model can have different types of parameters such\n",
    "    as mixed types of fp16 and fp32, the gradient reduction on these\n",
    "    mixed types of parameters will just work fine.\n",
    "    Also note that ``nccl`` backend is currently the fastest and highly\n",
    "    recommended backend for fp16/fp32 mixed-precision training.\n",
    "\n",
    ".. note:: If you use ``torch.save`` on one process to checkpoint the module,\n",
    "    and ``torch.load`` on some other processes to recover it, make sure that\n",
    "    ``map_location`` is configured properly for every process. Without\n",
    "    ``map_location``, ``torch.load`` would recover the module to devices\n",
    "    where the module was saved from.\n",
    "\n",
    ".. warning::\n",
    "    This module works only with the ``gloo`` and ``nccl`` backends.\n",
    "\n",
    ".. warning::\n",
    "    Constructor, forward method, and differentiation of the output (or a\n",
    "    function of the output of this module) is a distributed synchronization\n",
    "    point. Take that into account in case different processes might be\n",
    "    executing different code.\n",
    "\n",
    ".. warning::\n",
    "    This module assumes all parameters are registered in the model by the\n",
    "    time it is created. No parameters should be added nor removed later.\n",
    "    Same applies to buffers.\n",
    "\n",
    ".. warning::\n",
    "    This module assumes all parameters are registered in the model of each\n",
    "    distributed processes are in the same order. The module itself will\n",
    "    conduct gradient all-reduction following the reverse order of the\n",
    "    registered parameters of the model. In other words, it is users'\n",
    "    responsibility to ensure that each distributed process has the exact\n",
    "    same model and thus the exact same parameter registration order.\n",
    "\n",
    ".. warning::\n",
    "    This module allows parameters with non-rowmajor-contiguous strides.\n",
    "    For example, your model may contain some parameters whose\n",
    "    :class:`torch.memory_format` is ``torch.contiguous_format``\n",
    "    and others whose format is ``torch.channels_last``.  However,\n",
    "    corresponding parameters in different processes must have the\n",
    "    same strides.\n",
    "\n",
    ".. warning::\n",
    "    This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n",
    "    only work if gradients are to be accumulated in ``.grad`` attributes of\n",
    "    parameters).\n",
    "\n",
    ".. warning::\n",
    "\n",
    "    If you plan on using this module with a ``nccl`` backend or a ``gloo``\n",
    "    backend (that uses Infiniband), together with a DataLoader that uses\n",
    "    multiple workers, please change the multiprocessing start method to\n",
    "    ``forkserver`` (Python 3 only) or ``spawn``. Unfortunately\n",
    "    Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will\n",
    "    likely experience deadlocks if you don't change this setting.\n",
    "\n",
    ".. warning::\n",
    "    Forward and backward hooks defined on :attr:`module` and its submodules\n",
    "    won't be invoked anymore, unless the hooks are initialized in the\n",
    "    :meth:`forward` method.\n",
    "\n",
    ".. warning::\n",
    "    You should never try to change your model's parameters after wrapping\n",
    "    up your model with DistributedDataParallel. In other words, when\n",
    "    wrapping up your model with DistributedDataParallel, the constructor of\n",
    "    DistributedDataParallel will register the additional gradient\n",
    "    reduction functions on all the parameters of the model itself at the\n",
    "    time of construction. If you change the model's parameters after\n",
    "    the DistributedDataParallel construction, this is not supported and\n",
    "    unexpected behaviors can happen, since some parameters' gradient\n",
    "    reduction functions might not get called.\n",
    "\n",
    ".. note::\n",
    "    Parameters are never broadcast between processes. The module performs\n",
    "    an all-reduce step on gradients and assumes that they will be modified\n",
    "    by the optimizer in all processes in the same way. Buffers\n",
    "    (e.g. BatchNorm stats) are broadcast from the module in process of rank\n",
    "    0, to all other replicas in the system in every iteration.\n",
    "\n",
    ".. note::\n",
    "    If you are using DistributedDataParallel in conjunction with the\n",
    "    :ref:`distributed-rpc-framework`, you should always use\n",
    "    :meth:`torch.distributed.autograd.backward` to compute gradients and\n",
    "    :class:`torch.distributed.optim.DistributedOptimizer` for optimizing\n",
    "    parameters.\n",
    "\n",
    "Example::\n",
    "    >>> import torch.distributed.autograd as dist_autograd\n",
    "    >>> from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "    >>> from torch import optim\n",
    "    >>> from torch.distributed.optim import DistributedOptimizer\n",
    "    >>> from torch.distributed.rpc import RRef\n",
    "    >>>\n",
    "    >>> t1 = torch.rand((3, 3), requires_grad=True)\n",
    "    >>> t2 = torch.rand((3, 3), requires_grad=True)\n",
    "    >>> rref = rpc.remote(\"worker1\", torch.add, args=(t1, t2))\n",
    "    >>> ddp_model = DDP(my_model)\n",
    "    >>>\n",
    "    >>> # Setup optimizer\n",
    "    >>> optimizer_params = [rref]\n",
    "    >>> for param in ddp_model.parameters():\n",
    "    >>>     optimizer_params.append(RRef(param))\n",
    "    >>>\n",
    "    >>> dist_optim = DistributedOptimizer(\n",
    "    >>>     optim.SGD,\n",
    "    >>>     optimizer_params,\n",
    "    >>>     lr=0.05,\n",
    "    >>> )\n",
    "    >>>\n",
    "    >>> with dist_autograd.context() as context_id:\n",
    "    >>>     pred = ddp_model(rref.to_here())\n",
    "    >>>     loss = loss_func(pred, loss)\n",
    "    >>>     dist_autograd.backward(context_id, loss)\n",
    "    >>>     dist_optim.step()\n",
    "\n",
    ".. warning::\n",
    "    Using DistributedDataParallel in conjuction with the\n",
    "    :ref:`distributed-rpc-framework` is experimental and subject to change.\n",
    "\n",
    "Args:\n",
    "    module (Module): module to be parallelized\n",
    "    device_ids (list of int or torch.device): CUDA devices. This should\n",
    "               only be provided when the input module resides on a single\n",
    "               CUDA device. For single-device modules, the ``i``th\n",
    "               :attr:`module` replica is placed on ``device_ids[i]``. For\n",
    "               multi-device modules and CPU modules, device_ids must be None\n",
    "               or an empty list, and input data for the forward pass must be\n",
    "               placed on the correct device. (default: all devices for\n",
    "               single-device modules)\n",
    "    output_device (int or torch.device): device location of output for\n",
    "                  single-device CUDA modules. For multi-device modules and\n",
    "                  CPU modules, it must be None, and the module itself\n",
    "                  dictates the output location. (default: device_ids[0] for\n",
    "                  single-device modules)\n",
    "    broadcast_buffers (bool): flag that enables syncing (broadcasting) buffers of\n",
    "                      the module at beginning of the forward function.\n",
    "                      (default: ``True``)\n",
    "    process_group: the process group to be used for distributed data\n",
    "                   all-reduction. If ``None``, the default process group, which\n",
    "                   is created by ```torch.distributed.init_process_group```,\n",
    "                   will be used. (default: ``None``)\n",
    "    bucket_cap_mb: DistributedDataParallel will bucket parameters into\n",
    "                   multiple buckets so that gradient reduction of each\n",
    "                   bucket can potentially overlap with backward computation.\n",
    "                   :attr:`bucket_cap_mb` controls the bucket size in MegaBytes (MB)\n",
    "                   (default: 25)\n",
    "    find_unused_parameters (bool): Traverse the autograd graph of all tensors\n",
    "                                   contained in the return value of the wrapped\n",
    "                                   module's ``forward`` function.\n",
    "                                   Parameters that don't receive gradients as\n",
    "                                   part of this graph are preemptively marked\n",
    "                                   as being ready to be reduced. Note that all\n",
    "                                   ``forward`` outputs that are derived from\n",
    "                                   module parameters must participate in\n",
    "                                   calculating loss and later the gradient\n",
    "                                   computation. If they don't, this wrapper will\n",
    "                                   hang waiting for autograd to produce gradients\n",
    "                                   for those parameters. Any outputs derived from\n",
    "                                   module parameters that are otherwise unused can\n",
    "                                   be detached from the autograd graph using\n",
    "                                   ``torch.Tensor.detach``. (default: ``False``)\n",
    "    check_reduction: when setting to ``True``, it enables DistributedDataParallel\n",
    "                     to automatically check if the previous iteration's\n",
    "                     backward reductions were successfully issued at the\n",
    "                     beginning of every iteration's forward function.\n",
    "                     You normally don't need this option enabled unless you\n",
    "                     are observing weird behaviors such as different ranks\n",
    "                     are getting different gradients, which should not\n",
    "                     happen if DistributedDataParallel is correctly used.\n",
    "                     (default: ``False``)\n",
    "\n",
    "Attributes:\n",
    "    module (Module): the module to be parallelized\n",
    "\n",
    "Example::\n",
    "\n",
    "    >>> torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n",
    "    >>> net = torch.nn.DistributedDataParallel(model, pg)\n",
    "Init docstring: Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "File:           d:\\programmefiles\\python\\anaconda3\\envs\\tensorflow2.2\\lib\\site-packages\\torch\\nn\\parallel\\distributed.py\n",
    "Type:           type\n",
    "Subclasses:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.CrossEntropyLoss()\n",
    "```python\n",
    "nn.CrossEntropyLoss(\n",
    "    weight= None,\n",
    "    size_average=None,\n",
    "    ignore_index=-100,\n",
    "    reduce=None,\n",
    "    reduction='mean',\n",
    ") -> None\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "其结合了`nn.LogSoftmax`和`nn.NLLLoss`类，常用于具有 n 个类别的分类问题；`weight`代表为每个类分配权重，应为 1 维张量；`input`应该包含每个类的原始的、未标准化的 scores，其形状应为 $(m \\times n \\times d_1 \\times d_2 \\times \\cdots \\times d_K), K \\geq 1$，其中 m 为 batch 的大小，$d_1 \\cdots d_K$ 为\n",
    "\n",
    "该类要求对于长度为 m 的一维张量的每个值，其都有一个范围在 $[0, n - 1]$ 内的类索引作为 target；若指明了`ignore_index`，则该类也接收`ignore_index`这些值。记输入值为 $\\boldsymbol{x} = (x_1, x_2, \\cdots, x_n)$，则 loss 值为\n",
    "\n",
    "$$\\text{loss}(x_i) = -\\log\\left( \\frac{\\exp(x_i)}{\\sum_{j = 1}^n \\exp(x_j)} \\right) = -x_i + \\log\\left(\\sum_{j = 1}^n \\exp(x_j)\\right)$$\n",
    "\n",
    "`weight`被指明的情况下，记权重值为 $\\boldsymbol{w} = (w_1, w_2, \\cdots, w_n)$，则 loss 值为\n",
    "\n",
    "$$\\text{loss}(x_i) = w_i \\left(-x_i + \\log\\left(\\sum_{j = 1}^n \\exp(x_j)\\right)\\right)$$\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "- weight: 代表为每个类分配权重，应为长度为 C 的一维张量\n",
    "\n",
    "- size_average: 默认为``True``，该情况下 loss 值为 batch 内所有样本 loss 值的平均值，如果字段`size_average`被指明为``False``，则 loss 值将对每个 batch 进行求和，当`reduce`为``False``忽略此参数，此参数已被弃用，见`reduction`；Note that for some losses, there are multiple elements per sample.\n",
    "\n",
    "- ignore_index: 指定一个被忽略且对输入梯度没有贡献的目标值；当`size_average`为``True``时，loss 值为所有没被忽略目标对应的 loss 值的平均值\n",
    "\n",
    "- reduce: 已被弃用，见`reduction`；``False``时对 batch 的每一个元素返回一个 loss 值，同时忽略`size_average`参数；默认``True``\n",
    "\n",
    "- reduction: 可以是``'none'``、``'mean'``、``'sum'``，`size_average`和`reduce`将会被弃用，但目前指明这两者中的一个参数会重写`reduction`；`reduction`默认为``'mean'``\n",
    "    - ``'none'``: 不采取任何 reduction\n",
    "    - ``'mean'``: 输出的总和会除以输出元素的总数量\n",
    "    - ``'sum'``: 输出会被求和\n",
    "\n",
    "**Shape**\n",
    "\n",
    "- input: 形状为 $(m \\times n)$，其中 n 为类别的个数；或对于 K 维的损失值，形状为 $(m \\times n \\times d_1 \\times d_2 \\times \\cdots \\times d_K), K \\geq 1$ \n",
    "\n",
    "- target: 形状为 $(m)$，或对于 K 维的损失值，形状为 $(m \\times d_1 \\times d_2 \\times \\cdots \\times d_K)\\,, K \\geq 1$；其中每个元素的值满足 $0 \\leq \\text{target}[i] \\leq C-1 $；\n",
    "\n",
    "- output: 若`reduction`为``'none'``，则输出值与`target`形状相同，否则为标量\n",
    "\n",
    "**File**:   \\torch\\nn\\modules\\loss.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5514, 0.5514, 0.5514], grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "output = torch.tensor([[1.0, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32, requires_grad=True)\n",
    "# output = torch.ones([3, 3])\n",
    "target = torch.arange(3, dtype=torch.long)\n",
    "loss = loss_func(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Embedding()\n",
    "```python\n",
    "nn.Embedding(\n",
    "    num_embeddings,\n",
    "    embedding_dim,\n",
    "    padding_idx=None,\n",
    "    max_norm=None,\n",
    "    norm_type=2.0,\n",
    "    scale_grad_by_freq=False,\n",
    "    sparse=False,\n",
    "    _weight=None,\n",
    ") -> None\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "一个简单的查找表，用于存储一个固定的字典的嵌入及其形状大小；该模块常用于存储词嵌入，*其输入为一个索引构成的列表，输出为相应的词嵌入*\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- num_embeddings: 嵌入的字典的形状大小，即字典包含词向量的个数\n",
    "\n",
    "- embedding_dim: 每一个嵌入的向量的大小，即每个词向量维度\n",
    "\n",
    "- padding_idx: 即将查找表`padding_idx`位置的词向量置为全零向量，由于查找表中每个向量在后续训练中会经历被更新的环节，而对于该全零向量，其梯度永远是 0\n",
    "\n",
    "- max_norm: 若指明，则每个范数超过`max_norm`的嵌入向量都会 renormalize 至范数为`max_norm`的向量\n",
    "\n",
    "- norm_type: 指`max_norm`的范数形式，默认 2\n",
    "\n",
    "- scale_grad_by_freq: 指明时根据 minibatch 中单词的频率的倒数来缩放梯度，默认``False``.\n",
    "\n",
    "- sparse: ``True``时相应的权重矩阵的梯度为一稀疏矩阵，只有有限的 optimizer 支持稀疏梯度；目前支持的有`optim.SGD`(`CUDA`、`CPU`)，`optim.SparseAdam`(`CUDA`、`CPU`)，`optim.Adagrad`(`CPU`)\n",
    "\n",
    "**Attributes**\n",
    "\n",
    "- weight: 此模块的形状为 (num_embeddings, embedding_dim) 的可学习权重，其元素初始化时服从 $\\mathcal{N}(0, 1)$\n",
    "\n",
    "\n",
    "\n",
    "**File**:   \\torch\\nn\\modules\\sparse.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2980,  1.1256, -0.5481],\n",
      "         [ 0.3493,  0.9749, -0.9385]],\n",
      "\n",
      "        [[-1.5044, -0.6946,  0.2112],\n",
      "         [-0.8295,  0.4603, -0.3248]]], grad_fn=<EmbeddingBackward>)\n",
      "Parameter containing:\n",
      "tensor([[-0.2980,  1.1256, -0.5481],\n",
      "        [ 0.3493,  0.9749, -0.9385],\n",
      "        [-1.5044, -0.6946,  0.2112],\n",
      "        [-0.8295,  0.4603, -0.3248],\n",
      "        [-1.7520,  0.1621, -1.2363],\n",
      "        [ 0.6270, -1.0117,  0.4207],\n",
      "        [-1.1248,  0.2388,  1.0833],\n",
      "        [ 0.7649, -0.8190, -0.3316],\n",
      "        [ 1.1387, -0.5347, -0.0481],\n",
      "        [ 0.2216,  2.0920,  1.7258]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3)\n",
    "input = torch.LongTensor([[0, 1],[2, 3]])  # get the 1st, 2nd vector in the 1st batch, and the 3rd, 4th vector in the 2nd batch\n",
    "print(embedding(input))\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with padding_idx\n",
    "embedding = nn.Embedding(10, 3, padding_idx=3)\n",
    "input = torch.LongTensor([3, 2, 1, 0, 1, 2, 3])\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Linear()\n",
    "`nn.Linear(in_features, out_features, bias=True) -> None`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "初始化该模块内部状态，由`nn.Module`和`ScriptModule`共享，用于对输入做线性变换 $y = x A^T + b$\n",
    "\n",
    "**Args**\n",
    "\n",
    "- in_features: 每个输入样本的大小，即该层输入张量形状应满足`(N, *, in_features)`，其中`*`表示任意个附加维度\n",
    "\n",
    "- out_features: 类比上文\n",
    "\n",
    "- bias: 略\n",
    "\n",
    "**Attributes**\n",
    "\n",
    "- weight: 形状为`(out_featurs, in_features)`的可训练参数，权重值初始化时服从分布 $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$，其中 $k = \\frac{1}{\\text{in_features}}$\n",
    "\n",
    "- bias: True 时为形状`(out_featurs)`的可训练参数，初始化方法与权重相同\n",
    "            \n",
    "**File**:  \\torch\\nn\\modules\\linear.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     _LinearWithBias, Linear\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Conv2d()\n",
    "```python\n",
    "nn.Conv2d(\n",
    "    in_channels: int,\n",
    "    out_channels: int,\n",
    "    kernel_size: Union[int, Tuple[int, int]],\n",
    "    stride: Union[int, Tuple[int, int]] = 1,\n",
    "    padding: Union[int, Tuple[int, int]] = 0,\n",
    "    dilation: Union[int, Tuple[int, int]] = 1,\n",
    "    groups: int = 1,\n",
    "    bias: bool = True,\n",
    "    padding_mode: str = 'zeros',\n",
    ")\n",
    "```\n",
    "**Docstring**:\n",
    "\n",
    "对一个由若干输入平面组成的输入信号进行 2 维的卷积；对于输入形状为 $(N \\times C_{i} \\times H_{i} \\times W_{i})$ 的特征图 $\\boldsymbol{X}^{(in)}$，输出形状为 $(N \\times C_{o} \\times H_{o} \\times W_{o})$ 的特征图$\\boldsymbol{X}^{(out)}$，卷积过程可描述为\n",
    "$$\n",
    "\\boldsymbol{X}^{(out)}_{nc_o}= \\boldsymbol{b}_o + \\sum_{c_i = 1}^{C_{i}} \\boldsymbol{w}_{oi} \\star \\boldsymbol{X}^{(in)}_{nc_i} \\;,\\; c_o = 1, 2, \\cdots, C_{o} \\;,\\; n = 1, 2, \\cdots, N\n",
    "$$\n",
    "且满足\n",
    "$$\n",
    "H_{o} = \\left\\lfloor\\frac{H_{i}  + 2 P_h - D_h \\times (K_h - 1) - 1}{S_h} + 1\\right\\rfloor\\\\\n",
    "W_{o} = \\left\\lfloor\\frac{W_{i}  + 2 P_w - D_w \\times (K_w - 1) - 1}{S_w} + 1\\right\\rfloor\n",
    "$$\n",
    "其中 $\\star$ 表示[互关连算符](https://en.wikipedia.org/wiki/Cross-correlation)；卷积的动态演示参见 [here](https://github.com/vdumoulin/conv_arithmetic)\n",
    "\n",
    "需要注意的是，对于需要使用CUDA的CuDNN后端的情况，该操作可能会选择一个具有不确定性的算法以提高性能；若需要算法保持稳定，可设置``torch.backends.cudnn.deterministic =True``，但这同时可能会损失一定的性能；更多背景知识可以参见`/notes/randomness`\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- in_channels, out_channels: pass\n",
    "\n",
    "- kernel_size, stride, padding, dilation: 可以是整数或元祖；整数`n`时，其同时应用于纵向和横向两个维度；元祖`(h, w)`时，h 和 w 分别应用于纵向维度和横向维度，其中`dilation`指的对输入特征图采样的间隔；\n",
    "\n",
    "- padding_mode: 可以是``'zeros'``、``'reflect'``、``'replicate'``、``'circular'``，默认为``'zeros'``\n",
    "\n",
    "- groups: 整数且必须能够整除 $C_{in}, C_{out}$，其决定了卷积过程中独立进行卷积的个数，例如`groups=2`意味着输入特征图会被分为两部分，这两部分会分别各自进行卷积，随后再将卷积的结果进行拼接（如 AlexNet 中的机制）；当`groups == in_channels`且`out_channels == K * in_channels`该过程也称为 depthwise 卷积\n",
    "\n",
    "- bias: ``True``时附加偏置项，否则不附加，默认``True``\n",
    "\n",
    "\n",
    "\n",
    "**Attributes**:\n",
    "\n",
    "- weight: 形状为 $(C_{out} \\times \\frac{C_{in}}{groups} \\times K_h \\times K_w)$，其元素初始化默认服从 $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$，其中 $k = \\frac{groups}{C_{in} \\; K_h \\; K_w}$\n",
    "\n",
    "- bias: 形状为 $(C_{out})$，其元素初始化默认服从 $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$，其中 $k = \\frac{groups}{C_{in} \\; K_h \\; K_w}$\n",
    "\n",
    "\n",
    "**File**:          \\torch\\nn\\modules\\conv.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     Conv2d, ConvBn2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.ReLU()\n",
    "` nn.ReLU(inplace: bool = False)`\n",
    "\n",
    "`inplace`为 True 时直接对原变量进行操作，否则对输入进行复制，进过`ReLU`函数之后再返回；默认 False\n",
    "\n",
    "\n",
    "**File**:     torch\\nn\\modules\\activation.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     ReLU, ReLU6\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn.ReLU()\n",
    "m2 = nn.ReLU(inplace=True)\n",
    "inputs = torch.arange(1, 9) - 4\n",
    "outputs = m1(inputs)  # inputs == tensor([-3, -2, -1,  0,  1,  2,  3,  4]), outputs == tensor([0, 0, 0, 0, 1, 2, 3, 4])\n",
    "outputs = m2(inputs)  # outputs == inputs == tensor([0, 0, 0, 0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.AdaptiveAvgPool2d()\n",
    "`nn.AdaptiveAvgPool2d(output_size: Union[int, Tuple[int, ...]]) -> None`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "对一个由多 channel 组成的输入变量上应用二维的自适应平均池化；`output_size`可以是元祖`(H, W)`或单一整数`H`或为 None；第一种情况时对任意形状输入张量，输出张量形状为`(..., H, W)`，第二种情况则输出为`(..., H, H)`，第三种情况下 None 表示输出张量对应维度与输入张量相同；三种情况下输出张量的 channel 数和输入张量的 channel 数相同\n",
    "\n",
    "**File**:          \\torch\\nn\\modules\\pooling.py\n",
    "\n",
    "**Type**:           type \n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool1= nn.AdaptiveAvgPool2d((2, 1))\n",
    "avgpool2 = nn.AdaptiveAvgPool2d(1)\n",
    "avgpool3 = nn.AdaptiveMaxPool2d((None, 2))\n",
    "x = torch.arange(-9, 9, dtype=torch.float32).reshape(1, 2, 3, 3)\n",
    "y1 = avgpool1(x)  # y1.shape => (1, 2, 2, 1)\n",
    "y2 = avgpool2(x)  # y2.shape => (1, 2, 1, 1)\n",
    "y3 = avgpool3(x)  # y3.shape => (1, 2, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.BatchNorm1d()\n",
    "```python\n",
    "nn.BatchNorm1d(\n",
    "    num_features,\n",
    "    eps=1e-05,\n",
    "    momentum=0.1,\n",
    "    affine=True,\n",
    "    track_running_stats=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:<br>\n",
    "对输入的 2D 或 3D 张量进行 BN 操作，其中 2D 对应张量形状为`(batch_size, features)`，3D 对应张量形状为`(batch_size, features, length)`，其中特征有时也称作通道；BN 的计算过程如下：\n",
    "$$\n",
    "y = \\gamma\\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} + \\beta\n",
    "$$\n",
    "其中 $\\gamma$ 和 $\\beta$ 为长度为`features`的可学习参数，分别默认为 1 和 0；平均值和标准差是相对`batch_size`计算的，且每个`features`的计算相互独立；标准差是通过有偏估计函数计算的，相当于`torch.var(input, unbiased=False)`；训练时默认对均值和方差进行滑动估计，遍历所有 batch 最后得到的滑动统计数据则作为评估时的归一化参数；滑动估计的`momentum`默认为 0.1；滑动统计数据的更新公式为：\n",
    "$$\n",
    "\\hat{x}_\\text{new} = (1 - \\mathtt{momentum}) \\cdot \\hat{x} + \\mathtt{momentum} \\cdot x_t\n",
    "$$\n",
    "其中 $\\hat{x}$ 为滑动平均的估计值，$x_t$ 为新的观察值；<br>\n",
    "[References](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "\n",
    "\n",
    "**Args**:\n",
    "- num_features: 输入形状为`(batch_size, features)`或`(batch_size, features, length)`时对应的`features`大小；\n",
    "- eps: 默认 1e-5\n",
    "- momentum: 用于计算`running_mean`和`running_var`的数值，默认为 0.1；设置为 None 时表明不使用滑动平均；\n",
    "- affine: True 时该模块含有可学习的仿射参数 $\\gamma$ 和 $\\beta$，默认为 True；\n",
    "- track_running_stats: True 时该模块会追踪`running_mean`和`running_var`的值；否则不追踪，，并将统计值缓冲区`running_mean`和`running_var`初始化为 None，进而该模块在训练和评估时均使用 batch 级别的统计数据；默认 True\n",
    "\n",
    "\n",
    "**File**:    \\torch\\nn\\modules\\batchnorm.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples\n",
    "```python\n",
    "m = nn.BatchNorm1d(100, affine=False)\n",
    "inputs = torch.randn(20, 100, 32)\n",
    "outputs = m(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.BatchNorm2d()\n",
    "```python\n",
    "nn.BatchNorm2d(\n",
    "    num_features,\n",
    "    eps=1e-05,\n",
    "    momentum=0.1,\n",
    "    affine=True,\n",
    "    track_running_stats=True,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "对输入的 4D 张量进行 BN 操作，对应张量形状为`(batch_size, channel, height, width)`，其中通道有时也称作特征；BN 的计算过程如下：\n",
    "$$\n",
    "y = \\gamma\\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} + \\beta\n",
    "$$\n",
    "其中 $\\gamma$ 和 $\\beta$ 为长度为`channel`的可学习参数，分别默认为 1 和 0；平均值和标准差是相对`batch_size`计算的，且每个`channel`的计算相互独立；标准差是通过有偏估计函数计算的，相当于`torch.var(input, unbiased=False)`；训练时默认对均值和方差进行滑动估计，遍历所有 batch 最后得到的滑动统计数据则作为评估时的归一化参数；滑动估计的`momentum`默认为 0.1；滑动统计数据的更新公式为：\n",
    "$$\n",
    "\\hat{x}_\\text{new} = (1 - \\mathtt{momentum}) \\cdot \\hat{x} + \\mathtt{momentum} \\cdot x_t\n",
    "$$\n",
    "其中 $\\hat{x}$ 为滑动平均的估计值，$x_t$ 为新的观察值；\n",
    "\n",
    "[References](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "\n",
    "**Args**:\n",
    "- num_features: 输入形状为`(batch_size, features)`或`(batch_size, features, length)`时对应的`features`大小；\n",
    "- eps: 默认 1e-5\n",
    "- momentum: 用于计算`running_mean`和`running_var`的数值，默认为 0.1；设置为 None 时表明不使用滑动平均；\n",
    "- affine: True 时该模块含有可学习的仿射参数 $\\gamma$ 和 $\\beta$，默认为 True；\n",
    "- track_running_stats: True 时该模块会追踪`running_mean`和`running_var`的值；否则不追踪，，并将统计值缓冲区`running_mean`和`running_var`初始化为 None，进而该模块在训练和评估时均使用 batch 级别的统计数据；默认 True\n",
    "\n",
    "**File**:    \\torch\\nn\\modules\\batchnorm.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Parameter()\n",
    "`nn.Parameter(data=None, requires_grad=True)`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "`Parameter`是`torch.Tensor`的子类；当其被分配为`nn.Module`的属性时，`Parameter`会被自动添加到`nn.Module`参数列表中，并会出现在例如`Module.parameters`迭代器中；然而，给`nn.Module`分配`torch.Tensor`并不会产生自动添加这种效果，这是因为`torch.Tensor`可能是出于在模型中缓存一些临时层而添加的，例如 RNN 的最后一个隐藏层；若没有`Parameter`这种类，这些临时层也会被注册在模型中\n",
    "\n",
    "**Args**:\n",
    "- data: 应为`torch.Tensor`类型，即添加的参数张量\n",
    "- requires_grad: 默认 True，该参数是否需要记录梯度；更多细节参见`excluding-subgraphs`\n",
    "\n",
    "**File**:  \\torch\\nn\\parameter.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.UpsamplingBilinear2d()\n",
    "```python\n",
    "nn.UpsamplingBilinear2d(\n",
    "    size: Union[int, Tuple[int, int], NoneType] = None,\n",
    "    scale_factor: Union[float, Tuple[float, float], NoneType] = None,\n",
    ") -> None\n",
    "```\n",
    "对输入进行 2D 双线性上采样；可以通过`size`或`scale_factor`指定输出特征图的形状；\n",
    "\n",
    "请注意！这个类已经弃用，可改为使用`F.interpolate(..., mode='bilinear', align_corners=True)`；\n",
    "\n",
    "**Args**\n",
    "- size：输出的形状；可以是整型、整型组成的元祖；\n",
    "- scale_factor：浮点型或浮点型组成的元祖；放缩的比例，当算得的特征图不为整数时向下取整；\n",
    "\n",
    "**File**:     \\torch\\nn\\modules\\upsampling.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],\n",
      "          [1.6667, 2.0000, 2.3333, 2.6667],\n",
      "          [2.3333, 2.6667, 3.0000, 3.3333],\n",
      "          [3.0000, 3.3333, 3.6667, 4.0000]]]])\n",
      "tensor([[[[1.0000, 1.2500, 1.7500, 2.0000],\n",
      "          [1.5000, 1.7500, 2.2500, 2.5000],\n",
      "          [2.5000, 2.7500, 3.2500, 3.5000],\n",
      "          [3.0000, 3.2500, 3.7500, 4.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n",
    "y1 = nn.UpsamplingBilinear2d(scale_factor=2)(x)\n",
    "y2 = nn.functional.interpolate(x, scale_factor=2, mode='bilinear',\n",
    "    align_corners=False)\n",
    "print(y1)\n",
    "print(y2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
