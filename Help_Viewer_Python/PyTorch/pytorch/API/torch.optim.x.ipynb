{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim.SGD()\n",
    "```python\n",
    "optim.SGD(\n",
    "    params,\n",
    "    lr=<required parameter>,\n",
    "    momentum=0,\n",
    "    dampening=0,\n",
    "    weight_decay=0,\n",
    "    nesterov=False,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "\n",
    "实现随机梯度下降。注意，使用 Momentum/Nesterov 实现 SGD 与 Sutskever 等人以及其他一些框架中的对 SGD 的实现有些不同，考虑到动量的具体情况，更新可以写成：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
    "    p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中 $p, g, v, `\\mu$ 分别代表参数、梯度、速度、动量，这不同于 Sutskever 等人及其他框架中的更新形式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
    "    p_{t+1} & = p_{t} - v_{t+1}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "原论文：[Nesterov momentum](http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf)\n",
    "\n",
    "**Args**\n",
    "\n",
    "- params: 要优化的可迭代的参数，或定义了参数组的字典\n",
    "\n",
    "- lr: pass\n",
    "\n",
    "- momentum: 动量因子，默认 0\n",
    "\n",
    "- weight_decay: 权值衰减系数，即 L2 正则化的正则化系数，默认 0\n",
    "\n",
    "- dampening: dampening for momentum, default: 0\n",
    "\n",
    "- nesterov: 是否使用 Nesterov 动量模型，默认 Flase\n",
    " \n",
    "\n",
    "**File**: \\torch\\optim\\sgd.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Example**\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input), target).backward()\n",
    "optimizer.step() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim.SGD.step()\n",
    "`<optim_name>.step(closure=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "执行一个更新参数的过程\n",
    "\n",
    "**Args**\n",
    "\n",
    "- closure (callable, optional): 重新评估模型并返回损失的闭包(closure)\n",
    "\n",
    "**File**: torch\\optim\\sgd.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim.SGD.zero_grad()\n",
    "`<optim_name>.zero_grad()`\n",
    "\n",
    "清除所有需要优化的`torch.Tensor`的梯度，常用于训练神经网络时避免上一轮梯度更新影响下一轮梯度更新处\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SDG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim.optimizer.state_dict\n",
    "\n",
    "`optimizer.state_dict()`\n",
    "\n",
    "**Docstring:**\n",
    "\n",
    "返回优化器(optimizer)的状态字典，该字典包括两个条目:`state`——包含目前优化器的状态的字典，`param_groups`——包含所有参数组的字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optim.lr_scheduler.MultiStepLR()\n",
    "```python\n",
    "optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones,\n",
    "    gamma=0.1,\n",
    "    last_epoch=-1,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:     \n",
    "当达到`miletones`指定 epoch 时，对每一个参数组的学习率以因子`gamma`进行衰减；需要注意的是，该 scheduler 设定的衰减可与外部的学习率衰减同时发生\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- optimizer: Wrapped optimizer\n",
    "- milestones: epoch 索引组成的列表，其元素必须为递增的\n",
    "- gamma: 学习率衰减的乘积因子，默认 0.1\n",
    "- last_epoch: 使用权重衰减的最后一个 epoch，默认为 -1，这种情况下设置初始学习率`lr`即为`lr`\n",
    "\n",
    "**File**:      \\torch\\optim\\lr_scheduler.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Example\n",
    "\n",
    "假设初始 $lr = 0.05$，则当 $epoch < 30$ 时 $lr = 0.05$，当 $30 \\le epoch \\le 80$ 时 $lr = 0.005$，当 $epoch \\ge 80$ 时 $lr = 0.0005$\n",
    "\n",
    "```python\n",
    "scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    # some operations ...\n",
    "    scheduler.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pathlib\n",
    "\n",
    "print(type(pathlib))\n",
    "\n",
    "# for k, v in Path.__dict__.items():\n",
    "#     print(\"key\", k, sep=\"\")\n",
    "#     print(\"value\", v, sep=\"\")\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
