{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgrammeFiles\\python\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow as tf2\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.train.AdamOptimizer()\n",
    "```python\n",
    "tf.train.AdamOptimizer(\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam',\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "\n",
    "　　返回 Adam 优化器的实例，相关文献参见 [Adam - A Method for Stochastic Optimization Kingma et al., 2015](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "\n",
    "对于梯度为 g 的变量 W，其更新规则采用论文第 2 节末尾描述的优化方法：\n",
    "\n",
    "$$\n",
    "v_t = \\beta_1 v_{t-1} + (1 - \\beta_1) g_t  \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_1^t}\\\\\n",
    "n_t = \\beta_2 n_{t-1} + (1 - \\beta_2) g_t^2  \\quad \\hat{n}_t = \\frac{n_t}{1 - \\beta_2^t}\\\\\n",
    "W_{t+1} =W_{t} - \\eta \\frac{\\hat{v}_t}{\\sqrt{\\hat{n}_t}+ \\varepsilon}\n",
    "$$\n",
    "\n",
    "一般来说，$\\varepsilon$ 的默认取 1e-8 可能不是一个好的选择，例如在 ImageNet 上训练初始网络时，目前一个较好的选择是 1.0 或 0.1；注意，由于AdamOptimizer 使用的是论文 2.1 节之前的公式，而不是算法 1 中的公式，所以这里提到的 $\\varepsilon$ 在本文中是 $\\hat{\\varepsilon}$\n",
    "\n",
    "　　梯度通常由于`tf.gather`或一个嵌入查找而以`IndexedSlices`对象的形式呈现，此时算法的稀疏实现 (sparse implementation) 会对 variable slices 应用动量，尽管在向前传播时并没有应用，即对他们的梯度为 0；动量衰减 $\\beta_1$ 也会应用于整个动量累加器，即 sparse behavior 与 dense behavior 是等价的，这与一些涉及到动量的实现相反，后者如果没有使用 variable slice，则会忽略动量项\n",
    "\n",
    "　　即时执行的模式下，`learning_rate`、`beta1`、`beta2`、`epsilon`都可以是不接收参数并返回所需数值的可调用的函数，在调用不同的优化函数来更改这些值时非常有用\n",
    "\n",
    "**Args**\n",
    "\n",
    "- learning_rate: Tensor或浮点数\n",
    "\n",
    "- beta1: 浮点数或浮点张量，1 阶矩估计的指数衰减率\n",
    "\n",
    "- beta2: 浮点数或浮点张量，2 阶矩估计的指数衰减率\n",
    "\n",
    "- epsilon: A small constant for numerical stability. \n",
    "\n",
    "- use_locking: 如果为真，则对更新操作使用 locks\n",
    "\n",
    "- name: 梯度下降时执行运算的操作名称，默认为 \"Adam\"\n",
    "\n",
    "**Type**: type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train.AdamOptimizer().minimize()\n",
    "```python\n",
    "optm.minimize(\n",
    "    loss,\n",
    "    global_step=None,\n",
    "    var_list=None,\n",
    "    gate_gradients=1,\n",
    "    aggregation_method=None,\n",
    "    colocate_gradients_with_ops=False,\n",
    "    name=None,\n",
    "    grad_loss=None,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "通过更新`var_list`给 Optimizer 添加最小化`loss`的操作，若`global_step`不为 None，则此函数在迭代中也执行增加`global_step`的操作；该函数等价于调用了`compute_gradients()`和`apply_gradients()`，若需要在使用这些函数之前计算梯度，则应直接调用上述两个函数；若更新的变量不是`Variable`对象，则会报错\n",
    "\n",
    "**Args**\n",
    "\n",
    "- loss: 包含了要被最小化数值的`Tensor`\n",
    "\n",
    "- global_step: `Variable`类型，其在变量更新后递增 1\n",
    "\n",
    "- var_list: 在最小化`loss`时被更新的由`Variable`组成的列表或元组，默认为计算图由`GraphKeys.TRAINABLE_VARIABLES`收集到的变量列表\n",
    "\n",
    "- gate_gradients: 怎样 gate 梯度的计算，可以是`GATE_NONE`、`GATE_OP`、`GATE_GRAPH`.\n",
    "\n",
    "- aggregation_method: 指定合并梯度项的方法，`AggregationMethod`中定义了该参数的有效值\n",
    "\n",
    "- colocate_gradients_with_ops: True 时尝试将梯度与相应操作同步\n",
    "\n",
    "- name: 返回的操作名称\n",
    "\n",
    "- grad_loss: 一个`Tensor`，其保留着用于计算`loss`的梯度值\n",
    "\n",
    "当允许即时执行时，`loss`应是一个不接受任何参数并能够计算出要被最小化值的 Python 函数；若`var_list`不为 None，则最小化和梯度的计算都是针对`var_list`的元素进行的，否则是针对`loss`函数执行期间创建的任何可训练变量进行的；`gate_gradients`、`aggregation_method`、`colocate_gradients_with_ops`、`grad_loss`这几个参数在允许即时执行时都会被忽略\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.train.Saver()\n",
    "```python\n",
    "tf.train.Saver(\n",
    "    var_list=None,\n",
    "    reshape=False,\n",
    "    sharded=False,\n",
    "    max_to_keep=5,\n",
    "    keep_checkpoint_every_n_hours=10000.0,\n",
    "    name=None,\n",
    "    restore_sequentially=False,\n",
    "    saver_def=None,\n",
    "    builder=None,\n",
    "    defer_build=False,\n",
    "    allow_empty=False,\n",
    "    write_version=2,\n",
    "    pad_step_number=False,\n",
    "    save_relative_paths=False,\n",
    "    filename=None,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "有关变量、保存、恢复的概述参见[Variables](https://tensorflow.org/guide/variables) \n",
    "\n",
    "`Saver`类添加了用于从检查点保存和恢复变量的操作，以及便于运行这些操作的方法。检查点是一种专用格式的二进制文件，它将变量名映射到张量值，核查检查点内容的最好方法是使用`Saver`将内容加载出来。`Saver`可可以将训练的 step 数传递给`save()`方法的`global_step`参数，进而利用所提供计数器自动为检查点文件名编号，便于在训练模型时于不同的 step 中设置多个检查点；此外，`Saver`会自动管理检查点文件，以防止磁盘被填满，例如只能保存 N 个最新的文件，或者每训练 N 小时保存一个检查点，即\n",
    "\n",
    "除检查点文件之外，`Saver`会磁盘上保存一个协议缓冲区，其包含了最近创建的检查点列表，进而能够用于管理经过编号的检查点文件，利用`latest_checkpoint()`能够很方便的得到最新创建的检查点的路径，该协议缓冲区存储在检查点文件旁边一个名为'checkpoint'文件中；创建多个`Saver`时，可以在调用`save()`时为协议缓冲区文件指定不同的文件名。\n",
    "\n",
    "在启用即时执行时，`var_list`必须指明一个要保存的变量的列表或字典，否则会抛出`RuntimeError`异常；尽管`Saver`在即时执行模式下可以工作，但其容易出错，建议使用`tf.train.Checkpoint`或`tf.keras.Model.save_weights`，它们基于对象的保存功能更加健壮，并能够将加载由`Saver`编写的检查点；\n",
    "\n",
    "`Saver`不支持新版本的`AutoTrackable`API，在这种情况下应使用`tf.train.Checkpoint`类\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "- var_list: `Variable`或`SaveableObject`组成的列表，如`[v1, v2]`，此时变量会以其操作名称作为键而保存到检查点文件中，即`{v.op.name: v for v in [v1, v2]}`；或一个将名称映射到`SaveableObject`的字典，如`{\"v1\": v1, \"v2\": v2}`；`None`时默认为所有可保存的对象所组成的列表\n",
    "\n",
    "- reshape: `True`时允许从检查点中恢复一个包含了不同形状但元素数量和类型相同的变量，常用于从较旧的检查点重新加载它一个被 reshape 的变量，\n",
    "\n",
    "- sharded: `True`时，shard the checkpoints, one per device\n",
    "\n",
    "- max_to_keep: 磁盘上最多保留的最近检查点数，默认 5，若为 None 或 0，则不会从文件系统中删除检查点，但只有最后一个检查点保存在`checkpoint`文件中\n",
    "\n",
    "- keep_checkpoint_every_n_hours: 保留检查点的频率，常用于追踪模型在长时间的训练过程中的进展状况；默认为 10000 小时保存一次，这实际上禁用这个功能\n",
    "\n",
    "- name: 在添加操作时用作前缀的名称\n",
    "\n",
    "- restore_sequentially: True 时，代表在复原不同变量时，这些复原过程会在每个设备中有顺序的进行，在恢复非常大的模型时降低内存使用\n",
    "\n",
    "- saver_def: 可选的“SaverDef”原型来使用，而不是运行生成器。这只适用于需要为之前构建的具有“Saver”的图形重新创建“Saver”对象的特殊代码。“saver_def”原型应该是调用为该“图形”创建的“保护程序”的“as_saver_def()”返回的\n",
    "Optional `SaverDef` proto to use instead of running the builder. This is only useful for specialty code that wants to recreate a `Saver` object for a previously built `Graph` that had a `Saver`. The `saver_def` proto should be the one returned by the `as_saver_def()` call of the `Saver` that was created for that `Graph`.\n",
    "\n",
    "- builder: `saver_def`未指明时使用的`SaverBuilder`，默认`BulkSaverBuilder()`\n",
    "\n",
    "- defer_build: `True`时，defer adding the save and restore ops to the `build()` call，这种情况下，应该在结束计算图或使用`Saver`之前调用`build()`\n",
    "\n",
    "- allow_empty: False 时会在图中没有变量时抛出异常，否则无论哪种情况都会构造`Saver`，并将其设置为 no-op\n",
    "\n",
    "- write_version: 用于指明保存检查点时使用的格式，它还影响某些文件路径的匹配逻辑 (matching logic)；在复原变量恢复期间所需的内存和延迟方面，V2 格式比 V1 更好，不管这个标志是什么，保护程序都能够从 V2 和 V1 检查点进行恢复\n",
    "\n",
    "- pad_step_number: True 时将检查点文件路径中的 step 数扩充某个固定宽度，默认为8；默认为 False\n",
    "\n",
    "- save_relative_paths: True 时将写入检查点状态文件的相对路径；常用于希望复制检查点目录并从复制的目录重新加载的情况\n",
    "\n",
    "- filename: 如果在图形构造时已知，文件名将用于变量的加载/保存\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train.Saver.save()\n",
    "```python\n",
    "saver.save(\n",
    "    sess,\n",
    "    save_path,\n",
    "    global_step=None,\n",
    "    latest_filename=None,\n",
    "    meta_graph_suffix='meta',\n",
    "    write_meta_graph=True,\n",
    "    write_state=True,\n",
    "    strip_default_attrs=False,\n",
    "    save_debug_info=False,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "用于保存变量，它需要一个能启动计算图的会话，要保存的变量也必须已经初始化；该方法返回新创建的检查点文件的路径前缀，如果`Saver` is sharded，该字符串结尾会是`'-?????-of-nnnnn'`，其中`'nnnnn'`是创建的 shards 的数量，路径字符串可以直接作为参数传递给`restore()`；如果`Saver`是空的，则返回 None\n",
    "\n",
    "**Args**\n",
    "\n",
    "- sess: 保存变量所需要的会话\n",
    "\n",
    "- save_path: String，为检查点创建的文件名的前缀\n",
    "\n",
    "- global_step: 若指明了该参数，则该数字会附加在创建的检查点文件`save_path`上，此参数可以是`Tensor`、`Tensor`的名称或整数\n",
    "\n",
    "- latest_filename: 协议缓冲区文件的名称，该文件将包含最近的检查点列表。该文件保存在与检查点文件相同的目录中，由`Saver`自动管理，以跟踪最近的检查点\n",
    "\n",
    "- meta_graph_suffix: `MetaGraphDef`文件的后缀，默认为'meta'\n",
    "\n",
    "- write_meta_graph: 是否写入元计算图文件 (meta graph file)\n",
    "\n",
    "- write_state: 是否写入`CheckpointStateProto`\n",
    "\n",
    "- strip_default_attrs: `True`时默认值的属性将从`NodeDef`中删除，详见[Stripping Default-Valued Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes)\n",
    "\n",
    "- save_debug_info: `True`时将`GraphDebugInfo`保存到与`save_path`相同目录下的单独文件中，并在文件扩展名前添加`_debug`；这只在`write_meta_graph`为`True`时启用\n",
    "\n",
    "**File**:   tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train.Saver.restore()\n",
    "`saver.restore(sess, save_path)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "用于恢复先前保存的变量，它需要一个能够启动计算图的会话，要还原的变量不必已经初始化，因为还原本身就是一种初始化变量的方法。\n",
    "\n",
    "**Args**\n",
    "\n",
    "- sess: 用于恢复变量的`Session`，在即使执行模式中应为None\n",
    "\n",
    "- save_path: 之前保存参数的路径，该参数通常是调用`save()`或`latest_checkpoint()`的返回值，若为 None 或无效的检查点则会抛出`ValueError`异常\n",
    "\n",
    "**File**:  tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver.restore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
