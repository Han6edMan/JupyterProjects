{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(dict, only_callable_key=False):\n",
    "    for k, v in sorted(dict.items()):\n",
    "        if only_callable_key:\n",
    "            if callable(v):\n",
    "                print(k)\n",
    "        else:\n",
    "            print(k, v, sep=\"\\n\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers\n",
    "**MODULE**\n",
    "- experimental\n",
    "\n",
    "**CLASSES**\n",
    "\n",
    "    AbstractRNNCell\n",
    "    Activation\n",
    "    ActivityRegularization\n",
    "    Add\n",
    "    AdditiveAttention\n",
    "    AlphaDropout\n",
    "    Attention\n",
    "    Average\n",
    "    AveragePooling1D\n",
    "    AveragePooling2D\n",
    "    AveragePooling3D\n",
    "    AvgPool1D\n",
    "    AvgPool2D\n",
    "    AvgPool3D\n",
    "    BatchNormalization\n",
    "    Bidirectional\n",
    "    Concatenate\n",
    "    Conv1D\n",
    "    Conv2D\n",
    "    Conv2DTranspose\n",
    "    Conv3D\n",
    "    Conv3DTranspose\n",
    "    ConvLSTM2D\n",
    "    Convolution1D\n",
    "    Convolution2D\n",
    "    Convolution2DTranspose\n",
    "    Convolution3D\n",
    "    Convolution3DTranspose\n",
    "    Cropping1D\n",
    "    Cropping2D\n",
    "    Cropping3D\n",
    "    Dense\n",
    "    DenseFeatures\n",
    "    DepthwiseConv2D\n",
    "    Dot\n",
    "    Dropout\n",
    "    ELU\n",
    "    Embedding\n",
    "    Flatten\n",
    "    GRU\n",
    "    GRUCell\n",
    "    GaussianDropout\n",
    "    GaussianNoise\n",
    "    GlobalAveragePooling1D\n",
    "    GlobalAveragePooling2D\n",
    "    GlobalAveragePooling3D\n",
    "    GlobalAvgPool1D\n",
    "    GlobalAvgPool2D\n",
    "    GlobalAvgPool3D\n",
    "    GlobalMaxPool1D\n",
    "    GlobalMaxPool2D\n",
    "    GlobalMaxPool3D\n",
    "    GlobalMaxPooling1D\n",
    "    GlobalMaxPooling2D\n",
    "    GlobalMaxPooling3D\n",
    "    Input\n",
    "    InputLayer\n",
    "    InputSpec\n",
    "    LSTM\n",
    "    LSTMCell\n",
    "    Lambda\n",
    "    Layer\n",
    "    LayerNormalization\n",
    "    LeakyReLU\n",
    "    LocallyConnected1D\n",
    "    LocallyConnected2D\n",
    "    Masking\n",
    "    MaxPool1D\n",
    "    MaxPool2D\n",
    "    MaxPool3D\n",
    "    MaxPooling1D\n",
    "    MaxPooling2D\n",
    "    MaxPooling3D\n",
    "    Maximum\n",
    "    Minimum\n",
    "    Multiply\n",
    "    PReLU\n",
    "    Permute\n",
    "    RNN\n",
    "    ReLU\n",
    "    RepeatVector\n",
    "    Reshape\n",
    "    SeparableConv1D\n",
    "    SeparableConv2D\n",
    "    SeparableConvolution1D\n",
    "    SeparableConvolution2D\n",
    "    SimpleRNN\n",
    "    SimpleRNNCell\n",
    "    Softmax\n",
    "    SpatialDropout1D\n",
    "    SpatialDropout2D\n",
    "    SpatialDropout3D\n",
    "    StackedRNNCells\n",
    "    Subtract\n",
    "    ThresholdedReLU\n",
    "    TimeDistributed\n",
    "    UpSampling1D\n",
    "    UpSampling2D\n",
    "    UpSampling3D\n",
    "    Wrapper\n",
    "    ZeroPadding1D\n",
    "    ZeroPadding2D\n",
    "    ZeroPadding3D\n",
    "\n",
    "**FUNCTIONS**\n",
    "\n",
    "    add\n",
    "    average\n",
    "    concatenate\n",
    "    deserialize\n",
    "    dot\n",
    "    maximum\n",
    "    minimum\n",
    "    multiply\n",
    "    serialize\n",
    "    subtract\n",
    "\n",
    "**file**: \\tensorflow\\keras\\layers\\\\\\_\\_init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.AveragePooling1D()\n",
    "```python\n",
    "layers.AveragePooling1D(\n",
    "    pool_size=2,\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "对时序序列进行平均池化；输入仅支持 3 维张量；此 API 与`layers.AvgPool1D()`为同一 API；\n",
    "\n",
    "**Args**\n",
    "\n",
    "- pool_size: 整型，平均池化窗的大小\n",
    "- strides: 整型或 None，下采样因子；None 时默认与`pool_size`相同\n",
    "- padding: `\"valid\"`或`\"same\"`\n",
    "- data_format: `channels_last`时输入形状为`(batch, steps, features)`；`channels_first`时输入形状为`(batch, features, steps)`；池化发生在`step`维度\n",
    "\n",
    "**File**:   \\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.AveragePooling2D()\n",
    "```python\n",
    "layers.AveragePooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "对空间数据进行最大池化；输入仅支持 4 维张量；`layers.AvgPool2D()`与此 API 实际上为同一接口；\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- pool_size: 可以是整数或二元元组，下采样因子；\n",
    "- strides: 可以是整数、二元元组、None，None 时默认和`pool_size`相同\n",
    "- padding: 可以是`\"valid\"`或`\"same\"`，前者不填充，后者则进行零填充；然而若`strides`为 1，则两种情况输出图像形状相同（特征图均与输入特征图大小相同）\n",
    "- data_format: `channels_last`时输入形状应为`(batch, height, width, channels)`；`channels_first`时输入形状应为`(batch, channels, height, width)`；池化发生在`height`, `width`维度上；\n",
    "\n",
    "**File**:      \\tensorflow\\python\\keras\\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.AveragePooling3D()\n",
    "```python\n",
    "layers.AveragePooling3D(\n",
    "    pool_size=(2, 2, 2),\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "对空间 3 维数据或时空 3 维数据进行平均池化；输入仅支持 5 维张量；此 API 与`layers.AvgPool3D()`为同一 API\n",
    "\n",
    "**Args**\n",
    "- pool_size: 三元整数元祖，下采样因子\n",
    "- strides: 三元整数元祖或 None\n",
    "- padding: `\"valid\"`或`\"same\"`\n",
    "- data_format: `channels_last`时输入形状为`(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`；`channels_first`时输入形状为`(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`，池化发生在`spatial_dim1`, `spatial_dim2`, `spatial_dim3`维度上\n",
    "\n",
    "\n",
    "**File**:        \\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.BatchNormalization()\n",
    "```python\n",
    "layers.BatchNormalization(\n",
    "    axis=-1,\n",
    "    momentum=0.99,\n",
    "    epsilon=1e-3,\n",
    "    center=True,\n",
    "    scale=True,\n",
    "    beta_initializer='zeros',\n",
    "    gamma_initializer='ones',\n",
    "    moving_mean_initializer='zeros',\n",
    "    moving_variance_initializer='ones',\n",
    "    beta_regularizer=None,\n",
    "    gamma_regularizer=None,\n",
    "    beta_constraint=None,\n",
    "    gamma_constraint=None,\n",
    "    renorm=False,\n",
    "    renorm_clipping=None,\n",
    "    renorm_momentum=0.99,\n",
    "    fused=None,\n",
    "    trainable=True,\n",
    "    virtual_batch_size=None,\n",
    "    adjustment=None,\n",
    "    name=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "**Args**\n",
    "- axis: 进行归一化的维度，例如在`channel_first`格式下，应指定`axis=1`\n",
    "- momentum: 滑动平均的动量值\n",
    "- epsilon: 即 $\\varepsilon$\n",
    "- center: True 时对归一化的张量添加`beta`的偏移量；False 时忽略`beta`\n",
    "- scale: True 时对归一化的张量以`gamma`倍进行放缩，False 时则忽略；若下一层是线性模块，则可以不使用此参数，因为下一层会自动进行放缩；\n",
    "- beta_initializer, gamma_initializer：略\n",
    "- moving_mean_initializer, moving_variance_initializer: 略\n",
    "- beta_regularizer, gamma_regularizer：略\n",
    "- beta_constraint, gamma_constraint: 对`beta`、`gamma`添加的约束\n",
    "- renorm: 是否使用 [Batch Renormalization](https://arxiv.org/abs/1702.03275)；这会在训练中增加额外变量，The inference is the same for either value of this parameter.\n",
    "- renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar `Tensors` used to clip the renorm correction. The correction `(r, d)` is used as `corrected_value = normalized_value * r + d`, with `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.\n",
    "- renorm_momentum: Momentum used to update the moving means and standard deviations with renorm. Unlike `momentum`, this affects training and should be neither too small (which would add noise) nor too large (which would give stale estimates). Note that `momentum` is still applied to get the means and variances for inference.\n",
    "- fused: True 时使用更快的融合实现 (fused implementation)，该融合实现的方式无法使用时抛出`ValueError`异常；None 时若融合实现能够使用，则使用融合实现，否则不使用；False 时不使用融合实现\n",
    "- trainable: True 时参数为可训练的\n",
    "- virtual_batch_size: 整型；默认为 None，即 BN 是在整个 batch 上进行的；当不为 None 时，则执行\"Ghost BN\"，即创建每个单独归一化的虚拟子 batch，他们共享`gamma`、`beta`和滑动统计值；子 batch 的大小应能够整除实际 batch 大小；\n",
    "- adjustment: 以`Tensor`为输入并返回一对`(scale，bias)`元祖；其中`Tensor`包含了输入张量的动态的形状，该元组作用于归一化之后，作用于`gamma`和`beta`之前，该机制仅在训练期间使用；例如当`axis==-1`时\n",
    "```python\n",
    "adjustment = lambda shape: (\n",
    "    tf.random.uniform(shape[-1:], 0.93, 1.07),\n",
    "    tf.random.uniform(shape[-1:], -0.1, 0.1)\n",
    ")\n",
    "```\n",
    "  则会对归一化后的值最多放缩至 7%，再将结果最多偏移至 0.1%，最后应用`gamma`和`beta`；该操作对每个通道是独立的，对每个样本是共享的；若指定了`virtual_batch_size`则不可指定此参数；\n",
    "\n",
    "\n",
    "  training: Python boolean indicating whether the layer should behave in\n",
    "    training mode or in inference mode.\n",
    "    - `training=True`: The layer will normalize its inputs using the\n",
    "      mean and variance of the current batch of inputs.\n",
    "    - `training=False`: The layer will normalize its inputs using the\n",
    "      mean and variance of its moving statistics, learned during training.\n",
    "\n",
    "**File**:  \\layers\\normalization_v2.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.Conv2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Conv2D()\n",
    "```python\n",
    "layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "**Docstring**:\n",
    "\n",
    "在第一层使用时需将`input_shape`以关键字参数传递给构造函数\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- filters: filter 的个数\n",
    "- kernel_size: 可以是一元整数或二元元组\n",
    "- strides: 可以是一元整数或二元元组；`strides`与`dilation_rate`不能同时不为一\n",
    "- padding: 可以是`\"valid\"`或`\"same\"`；`\"casual\"`时会报错\n",
    "- data_format: 应为`'channels_last'`或`'channels_first'`，即用于说明数据输入的形状是`(batch_size, height, width, channels)`还是`(batch_size, channels, rows, cols)`；默认为 Keras 的配置文件`~/.keras/keras.json`中的`image_data_format`的值，该值在未更改情况下为`channels_last`；`channels_last`时输出特征图的形状为`(batch_size, new_rows, new_cols, filters)`\n",
    "- dilation_rate: 可以是一元整数或二元元组；`strides`与`dilation_rate`不能同时不为一\n",
    "- activation:略，更多函数详见`keras.activations`\n",
    "- use_bias: pass\n",
    "- kernel_initializer, kernel_regularizer, bias_initializer, bias_regularizer: 略，更多函数详见`keras.initializers`和`keras.regularizers`\n",
    "- activity_regularizer: 对输出层应用的正则化函数\n",
    "- kernel_constraint, bias_constraint: 对卷积核及偏置应用的约束函数\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     Conv2DTranspose, DepthwiseConv2D, Conv2D\n",
    "\n",
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26, 26, 2)\n",
      "(4, 24, 24, 2)\n",
      "(4, 28, 28, 2)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal((4, 28, 28, 3))\n",
    "y = layers.Conv2D(2, 3, activation='relu')(x)  # y.shape == (4, 26, 26, 2)\n",
    "y = layers.Conv2D(2, 3, activation='relu', dilation_rate=2)(x)  # y.shape == (4, 24, 24, 2)\n",
    "y = layers.Conv2D(2, 3, activation='relu', padding=\"same\")(x)  # y.shape == (4, 28, 28, 2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'trainable', 'dtype', 'filters', 'kernel_size', 'strides', 'padding', 'data_format', 'dilation_rate', 'activation', 'use_bias', 'kernel_initializer', 'bias_initializer', 'kernel_regularizer', 'bias_regularizer', 'activity_regularizer', 'kernel_constraint', 'bias_constraint']\n"
     ]
    }
   ],
   "source": [
    "print(layers.Conv2D(2, 3, activation='relu').get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Dense()\n",
    "```python\n",
    "layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "在`layers.Dense`的属性中除`trainable`之外，在该层被调用一次之后便不能被更改了\n",
    "\n",
    "**Args**:\n",
    "- units: 输出神经元个数\n",
    "- activation: 激活函数，默认不附加激活函数\n",
    "- use_bias: 是否使用偏置\n",
    "- kernel_initializer, kernel_regularizer: 权重矩阵的初始化函数及正则化函数\n",
    "- bias_initializer, bias_regularizer: 偏置的初始化函数及正则化函数\n",
    "- activity_regularizer: 作用于输出层神经元的正则化函数\n",
    "- kernel_constraint: 作用在权重矩阵的约束函数\n",
    "- bias_constraint: 作用在偏置的约束函数\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     Dense\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as first layer in a sequential model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Dense(32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Flatten()\n",
    "`layers.Flatten(*args, **kwargs)`\n",
    "Docstring:     \n",
    "Flattens the input. Does not affect the batch size.\n",
    "\n",
    "If inputs are shaped `(batch,)` without a channel dimension, then flattening\n",
    "adds an extra channel dimension and output shapes are `(batch, 1)`.\n",
    "\n",
    "Arguments:\n",
    "  data_format: A string,\n",
    "    one of `channels_last` (default) or `channels_first`.\n",
    "    The ordering of the dimensions in the inputs.\n",
    "    `channels_last` corresponds to inputs with shape\n",
    "    `(batch, ..., channels)` while `channels_first` corresponds to\n",
    "    inputs with shape `(batch, channels, ...)`.\n",
    "    It defaults to the `image_data_format` value found in your\n",
    "    Keras config file at `~/.keras/keras.json`.\n",
    "    If you never set it, then it will be \"channels_last\".\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(3, 32, 32)))\n",
    "# now: model.output_shape == (None, 64, 32, 32)\n",
    "model.add(Flatten())\n",
    "# now: model.output_shape == (None, 65536)\n",
    "```\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\n",
    "Type:           type\n",
    "Subclasses:     Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.GlobalAveragePooling1D()\n",
    "`layers.GlobalAveragePooling1D(data_format='channels_last', **kwargs)`\n",
    "\n",
    "对时序序列进行全局平均池化；输入仅支持 3 维张量；此 API 与`layers.GlobalAvgPool1D()`为同一 API；调用时还可传递一形状为`(batch_size, steps)`的`mask`张量，用于指明每个时间步的数据是否应被 mask\n",
    "\n",
    "**Args**\n",
    "- data_format: `channels_last`时输入形状为`(batch, steps, features)`；`channels_first`时输入形状为`(batch, features, steps)`；池化发生在`step`维度，返回张量形状为`(batch_size, features)`\n",
    "\n",
    "**File**:   \\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.GlobalAveragePooling2D()\n",
    "`layers.GlobalAveragePooling2D(data_format=None, **kwargs)`\n",
    "\n",
    "对空间数据进行全局最大池化；输入仅支持 4 维张量；`layers.GlobalAvgPool2D()`与此 API 实际上为同一接口；\n",
    "\n",
    "**Args**:\n",
    "- data_format: `channels_last`时输入形状应为`(batch, height, width, channels)`；`channels_first`时输入形状应为`(batch, channels, height, width)`；池化发生在`height`、`width`维度上；两种情况下池化后输出形状均为`(batch_size, channels)`\n",
    "\n",
    "**File**:      \\tensorflow\\python\\keras\\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Layer()\n",
    "Init signature: layers.Layer(*args, **kwargs)\n",
    "Docstring:     \n",
    "This is the class from which all layers inherit.\n",
    "\n",
    "A layer is a callable object that takes as input one or more tensors and\n",
    "that outputs one or more tensors. It involves *computation*, defined\n",
    "in the `call()` method, and a *state* (weight variables), defined\n",
    "either in the constructor `__init__()` or in the `build()` method.\n",
    "\n",
    "Users will just instantiate a layer and then treat it as a callable.\n",
    "\n",
    "We recommend that descendants of `Layer` implement the following methods:\n",
    "\n",
    "* `__init__()`: Defines custom layer attributes, and creates layer state\n",
    "  variables that do not depend on input shapes, using `add_weight()`.\n",
    "* `build(self, input_shape)`: This method can be used to create weights that\n",
    "  depend on the shape(s) of the input(s), using `add_weight()`. `__call__()`\n",
    "  will automatically build the layer (if it has not been built yet) by\n",
    "  calling `build()`.\n",
    "* `call(self, *args, **kwargs)`: Called in `__call__` after making sure\n",
    "  `build()` has been called. `call()` performs the logic of applying the\n",
    "  layer to the input tensors (which should be passed in as argument).\n",
    "  Two reserved keyword arguments you can optionally use in `call()` are:\n",
    "    - `training` (boolean, whether the call is in\n",
    "      inference mode or training mode)\n",
    "    - `mask` (boolean tensor encoding masked timesteps in the input, used\n",
    "      in RNN layers)\n",
    "* `get_config(self)`: Returns a dictionary containing the configuration used\n",
    "  to initialize this layer. If the keys differ from the arguments\n",
    "  in `__init__`, then override `from_config(self)` as well.\n",
    "  This method is used when saving\n",
    "  the layer or a model that contains this layer.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Here's a basic example: a layer with two variables, `w` and `b`,\n",
    "that returns `y = w . x + b`.\n",
    "It shows how to implement `build()` and `call()`.\n",
    "Variables set as attributes of a layer are tracked as weights\n",
    "of the layers (in `layer.weights`).\n",
    "\n",
    "```python\n",
    "class SimpleDense(Layer):\n",
    "\n",
    "  def __init__(self, units=32):\n",
    "      super(SimpleDense, self).__init__()\n",
    "      self.units = units\n",
    "\n",
    "  def build(self, input_shape):  # Create the state of the layer (weights)\n",
    "    w_init = tf.random_normal_initializer()\n",
    "    self.w = tf.Variable(\n",
    "        initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                             dtype='float32'),\n",
    "        trainable=True)\n",
    "    b_init = tf.zeros_initializer()\n",
    "    self.b = tf.Variable(\n",
    "        initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "        trainable=True)\n",
    "\n",
    "  def call(self, inputs):  # Defines the computation from inputs to outputs\n",
    "      return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "# Instantiates the layer.\n",
    "linear_layer = SimpleDense(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert len(linear_layer.weights) == 2\n",
    "\n",
    "# These weights are trainable, so they're listed in `trainable_weights`:\n",
    "assert len(linear_layer.trainable_weights) == 2\n",
    "```\n",
    "\n",
    "Note that the method `add_weight()` offers a shortcut to create weights:\n",
    "\n",
    "```python\n",
    "class SimpleDense(Layer):\n",
    "\n",
    "  def __init__(self, units=32):\n",
    "      super(SimpleDense, self).__init__()\n",
    "      self.units = units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "      self.b = self.add_weight(shape=(self.units,),\n",
    "                               initializer='random_normal',\n",
    "                               trainable=True)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      return tf.matmul(inputs, self.w) + self.b\n",
    "```\n",
    "\n",
    "Besides trainable weights, updated via backpropagation during training,\n",
    "layers can also have non-trainable weights. These weights are meant to\n",
    "be updated manually during `call()`. Here's a example layer that computes\n",
    "the running sum of its inputs:\n",
    "\n",
    "```python\n",
    "class ComputeSum(Layer):\n",
    "\n",
    "  def __init__(self, input_dim):\n",
    "      super(ComputeSum, self).__init__()\n",
    "      # Create a non-trainable weight.\n",
    "      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                               trainable=False)\n",
    "\n",
    "  def call(self, inputs):\n",
    "      self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "      return self.total\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []\n",
    "```\n",
    "\n",
    "For more information about creating layers, see the guide\n",
    "[Writing custom layers and models with Keras](\n",
    "  https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n",
    "\n",
    "Arguments:\n",
    "  trainable: Boolean, whether the layer's variables should be trainable.\n",
    "  name: String name of the layer.\n",
    "  dtype: The dtype of the layer's computations and weights (default of\n",
    "    `None` means use `tf.keras.backend.floatx` in TensorFlow 2, or the type\n",
    "    of the first input in TensorFlow 1).\n",
    "  dynamic: Set this to `True` if your layer should only be run eagerly, and\n",
    "    should not be used to generate a static computation graph.\n",
    "    This would be the case for a Tree-RNN or a recursive network,\n",
    "    for example, or generally for any layer that manipulates tensors\n",
    "    using Python control flow. If `False`, we assume that the layer can\n",
    "    safely be used to generate a static computation graph.\n",
    "\n",
    "Attributes:\n",
    "  name: The name of the layer (string).\n",
    "  dtype: The dtype of the layer's computations and weights. If mixed\n",
    "    precision is used with a `tf.keras.mixed_precision.experimental.Policy`,\n",
    "    this is instead just the dtype of the layer's weights, as the computations\n",
    "    are done in a different dtype.\n",
    "  updates: List of update ops of this layer.\n",
    "  losses: List of losses added by this layer.\n",
    "  trainable_weights: List of variables to be included in backprop.\n",
    "  non_trainable_weights: List of variables that should not be\n",
    "    included in backprop.\n",
    "  weights: The concatenation of the lists trainable_weights and\n",
    "    non_trainable_weights (in this order).\n",
    "  trainable: Whether the layer should be trained (boolean).\n",
    "  input_spec: Optional (list of) `InputSpec` object(s) specifying the\n",
    "    constraints on inputs that can be accepted by the layer.\n",
    "\n",
    "Each layer has a dtype, which is typically the dtype of the layer's\n",
    "computations and variables. A layer's dtype can be queried via the\n",
    "`Layer.dtype` property. The dtype is specified with the `dtype` constructor\n",
    "argument. In TensorFlow 2, the dtype defaults to `tf.keras.backend.floatx()`\n",
    "if no dtype is passed. `floatx()` itself defaults to \"float32\". Additionally,\n",
    "layers will cast their inputs to the layer's dtype in TensorFlow 2. When mixed\n",
    "precision is used, layers may have different computation and variable dtypes.\n",
    "See `tf.keras.mixed_precision.experimental.Policy` for details on layer\n",
    "dtypes.\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\n",
    "Type:           type\n",
    "Subclasses:     TensorFlowOpLayer, AddLoss, AddMetric, Metric, Metric, InputLayer, Network, PreprocessingLayer, Resizing, CenterCrop, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.LSTM()\n",
    "Init signature: layers.LSTM(*args, **kwargs)\n",
    "Docstring:     \n",
    "Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
    "for details about the usage of RNN API.\n",
    "\n",
    "Based on available runtime hardware and constraints, this layer\n",
    "will choose different implementations (cuDNN-based or pure-TensorFlow)\n",
    "to maximize the performance. If a GPU is available and all\n",
    "the arguments to the layer meet the requirement of the CuDNN kernel\n",
    "(see below for details), the layer will use a fast cuDNN implementation.\n",
    "\n",
    "The requirements to use the cuDNN implementation are:\n",
    "\n",
    "1. `activation` == `tanh`\n",
    "2. `recurrent_activation` == `sigmoid`\n",
    "3. `recurrent_dropout` == 0\n",
    "4. `unroll` is `False`\n",
    "5. `use_bias` is `True`\n",
    "6. Inputs are not masked or strictly right padded.\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> inputs = tf.random.normal([32, 10, 8])\n",
    ">>> lstm = tf.keras.layers.LSTM(4)\n",
    ">>> output = lstm(inputs)\n",
    ">>> print(output.shape)\n",
    "(32, 4)\n",
    ">>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    ">>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    ">>> print(whole_seq_output.shape)\n",
    "(32, 10, 4)\n",
    ">>> print(final_memory_state.shape)\n",
    "(32, 4)\n",
    ">>> print(final_carry_state.shape)\n",
    "(32, 4)\n",
    "\n",
    "Arguments:\n",
    "  units: Positive integer, dimensionality of the output space.\n",
    "  activation: Activation function to use.\n",
    "    Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation\n",
    "    is applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "  recurrent_activation: Activation function to use for the recurrent step.\n",
    "    Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n",
    "    applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "  use_bias: Boolean (default `True`), whether the layer uses a bias vector.\n",
    "  kernel_initializer: Initializer for the `kernel` weights matrix, used for\n",
    "    the linear transformation of the inputs. Default: `glorot_uniform`.\n",
    "  recurrent_initializer: Initializer for the `recurrent_kernel` weights\n",
    "    matrix, used for the linear transformation of the recurrent state.\n",
    "    Default: `orthogonal`.\n",
    "  bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
    "  unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n",
    "    the forget gate at initialization. Setting it to true will also force\n",
    "    `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n",
    "        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
    "  kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
    "    matrix. Default: `None`.\n",
    "  recurrent_regularizer: Regularizer function applied to the\n",
    "    `recurrent_kernel` weights matrix. Default: `None`.\n",
    "  bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
    "    `None`.\n",
    "  activity_regularizer: Regularizer function applied to the output of the\n",
    "    layer (its \"activation\"). Default: `None`.\n",
    "  kernel_constraint: Constraint function applied to the `kernel` weights\n",
    "    matrix. Default: `None`.\n",
    "  recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
    "    weights matrix. Default: `None`.\n",
    "  bias_constraint: Constraint function applied to the bias vector. Default:\n",
    "    `None`.\n",
    "  dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
    "    transformation of the inputs. Default: 0.\n",
    "  recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
    "    the linear transformation of the recurrent state. Default: 0.\n",
    "  implementation: Implementation mode, either 1 or 2. Mode 1 will structure\n",
    "    its operations as a larger number of smaller dot products and additions,\n",
    "    whereas mode 2 will batch them into fewer, larger operations. These modes\n",
    "    will have different performance profiles on different hardware and for\n",
    "    different applications. Default: 2.\n",
    "  return_sequences: Boolean. Whether to return the last output. in the output\n",
    "    sequence, or the full sequence. Default: `False`.\n",
    "  return_state: Boolean. Whether to return the last state in addition to the\n",
    "    output. Default: `False`.\n",
    "  go_backwards: Boolean (default `False`). If True, process the input sequence\n",
    "    backwards and return the reversed sequence.\n",
    "  stateful: Boolean (default `False`). If True, the last state for each sample\n",
    "    at index i in a batch will be used as initial state for the sample of\n",
    "    index i in the following batch.\n",
    "  time_major: The shape format of the `inputs` and `outputs` tensors.\n",
    "    If True, the inputs and outputs will be in shape\n",
    "    `[timesteps, batch, feature]`, whereas in the False case, it will be\n",
    "    `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n",
    "    efficient because it avoids transposes at the beginning and end of the\n",
    "    RNN calculation. However, most TensorFlow data is batch-major, so by\n",
    "    default this function accepts input and emits output in batch-major\n",
    "    form.\n",
    "  unroll: Boolean (default `False`). If True, the network will be unrolled,\n",
    "    else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n",
    "    it tends to be more memory-intensive. Unrolling is only suitable for short\n",
    "    sequences.\n",
    "\n",
    "Call arguments:\n",
    "  inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n",
    "  mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n",
    "    a given timestep should be masked (optional, defaults to `None`).\n",
    "  training: Python boolean indicating whether the layer should behave in\n",
    "    training mode or in inference mode. This argument is passed to the cell\n",
    "    when calling it. This is only relevant if `dropout` or\n",
    "    `recurrent_dropout` is used (optional, defaults to `None`).\n",
    "  initial_state: List of initial state tensors to be passed to the first\n",
    "    call of the cell (optional, defaults to `None` which causes creation\n",
    "    of zero-filled initial state tensors).\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\n",
    "Type:           type\n",
    "Subclasses:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.MaxPooling1D()\n",
    "```python\n",
    "layers.MaxPooling1D(\n",
    "    pool_size=2,\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "对时序序列进行最大池化，即\n",
    "\n",
    "**Args**\n",
    "- pool_size: Integer, size of the max pooling windows.\n",
    "  strides: Integer, or None. Factor by which to downscale.\n",
    "    E.g. 2 will halve the input.\n",
    "    If None, it will default to `pool_size`.\n",
    "  padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n",
    "  data_format: A string,\n",
    "    one of `channels_last` (default) or `channels_first`.\n",
    "    The ordering of the dimensions in the inputs.\n",
    "    `channels_last` corresponds to inputs with shape\n",
    "    `(batch, steps, features)` while `channels_first`\n",
    "    corresponds to inputs with shape\n",
    "    `(batch, features, steps)`.\n",
    "\n",
    "Input shape:\n",
    "  - If `data_format='channels_last'`:\n",
    "    3D tensor with shape `(batch_size, steps, features)`.\n",
    "  - If `data_format='channels_first'`:\n",
    "    3D tensor with shape `(batch_size, features, steps)`.\n",
    "\n",
    "Output shape:\n",
    "  - If `data_format='channels_last'`:\n",
    "    3D tensor with shape `(batch_size, downsampled_steps, features)`.\n",
    "  - If `data_format='channels_first'`:\n",
    "    3D tensor with shape `(batch_size, features, downsampled_steps)`.\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow1.14\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\pooling.py\n",
    "Type:           type\n",
    "Subclasses:     MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.MaxPooling2D()\n",
    "```python\n",
    "layers.MaxPooling2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=None,\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "对空间数据进行最大池化，即在特征图的`height`和`width`组成的 2 维平面上执行；`layers.MaxPool2D()`与此 API 实际上为同一接口；\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- pool_size: 可以是整数或二元元组，下采样因子；\n",
    "- strides: 可以是整数或二元元组，默认和`pool_size`相同\n",
    "- padding: 可以是`\"valid\"`或`\"same\"`，前者不填充，后者则进行零填充；然而若`strides`为 1，则两种情况输出图像形状相同（特征图均与输入特征图大小相同）\n",
    "- data_format: 略\n",
    "\n",
    "**File**:      \\tensorflow\\python\\keras\\layers\\pooling.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.ReLU()\n",
    "`layers.ReLU(max_value=None, negative_slope=0, threshold=0, **kwargs)`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "\n",
    "默认返回 \\\\( \\max(x, 0) \\\\)，当指定 $\\texttt{max_value}=M, \\texttt{negative_slope}=\\alpha, \\texttt{threshold}=t$ 时，则返回\n",
    "\n",
    "$$f(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    M & x \\ge M \\\\\n",
    "    x & t \\le x < M \\\\\n",
    "    \\alpha (x-t) & x < t\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "**File**:   \\tensorflow\\python\\keras\\layers\\advanced_activations.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.0, -0.0, 0.0, 1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.ReLU(negative_slope=2.0, threshold=-1)\n",
    "output1 = layer(tf.constant([-3.0, -1.0, 0.0, 1.0, 2.0]))\n",
    "print(list(output1.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.UpSampling2D()\n",
    "`layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest', **kwargs)`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- size: 可以是整数或二元元组，二元元组时分别以`size[0]`和`size[1]`的倍数对输入张量的行和列进行上采样\n",
    "\n",
    "- data_format: 略\n",
    "\n",
    "- interpolation: 可以是`nearest`或`bilinear`\n",
    "\n",
    "**File**:       \\tensorflow\\python\\keras\\layers\\convolutional.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(range(1, 13), shape=[2, 2, 1, 3])\n",
    "y = tf.keras.layers.UpSampling2D(size=(2, 3), data_format=\"channels_first\")(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.ZeroPadding2D()\n",
    "`layers.ZeroPadding2D(padding=(1, 1), data_format=None, **kwargs)`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- padding: 可以是单个整数、由两个整数组成的元祖、由两个二元元祖组成的元祖，分别对应对称填充、高度方向和宽度方向的对称填充、`((top_pad, bottom_pad), (left_pad, right_pad))`填充\n",
    "\n",
    "- data_format: 可以是`'channels_last'`或`'channels_first'`，默认是`'channels_last'`，即输入张量的形状排列方式\n",
    "\n",
    "**File**:   \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable([[[[1, 2], [3, 4]]]])\n",
    "y = tf.keras.layers.ZeroPadding2D(padding=1, data_format=\"channels_first\")(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.ZeroPadding3D()\n",
    "`layers.ZeroPadding3D(*args, **kwargs)`\n",
    "\n",
    "\n",
    "**Args**\n",
    "\n",
    "- padding: 可以是单一整数、三元元祖、由三个二元元组组成的元祖；其中第三种对应`((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))`的情况\n",
    "\n",
    "- data_format: 略\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 5, 6, 8)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (1, 1, 3, 2, 2)\n",
    "x = np.arange(np.prod(input_shape)).reshape(input_shape)\n",
    "y = tf.keras.layers.ZeroPadding3D(padding=(1, 2, 3), data_format=\"channels_first\")(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
