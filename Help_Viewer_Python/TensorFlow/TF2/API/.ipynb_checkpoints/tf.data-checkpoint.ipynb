{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.data as data\n",
    "\n",
    "def print_dict(dict_, only_callable_key=False):\n",
    "    if only_callable_key:\n",
    "        for k, v in dict_.items():\n",
    "             if callable(v):\n",
    "                print(k)\n",
    "    else:\n",
    "        for k, v in dict_.items():\n",
    "            print(k, v, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data\n",
    "更多相关指导参见[这里](https://tensorflow.org/guide/data)\n",
    "\n",
    "**PACKAGE CONTENTS**\n",
    "- experimental\n",
    "\n",
    "**CLASSES**\n",
    "\n",
    "    DatasetSpec\n",
    "    Dataset\n",
    "    Options\n",
    "    Iterator\n",
    "    IteratorSpec\n",
    "    FixedLengthRecordDataset\n",
    "    TFRecordDataset\n",
    "    TextLineDataset\n",
    "\n",
    "**DATA**\n",
    "\n",
    "    AUTOTUNE = -1\n",
    "    INFINITE_CARDINALITY = -1\n",
    "    UNKNOWN_CARDINALITY = -2\n",
    "\n",
    "**FILE**: \\tensorflow\\_api\\v2\\data\\\\\\_\\_init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.Dataset()\n",
    "`tf.data.Dataset(variant_tensor)`\n",
    "\n",
    "创建一个`DatasetV2`对象，与`DatasetV1`不将数据传入构造函数不同，`DatasetV2`及其子类均会将所接收的数据或直接或通过`super`方法传递给`DatasetV2`的构造函数；对`Dataset`的使用主要包括：\n",
    "1. 创建数据集；最简单的创建数据集的方式便是将列表传递给`.from_tensor_slices()`方法；更多的创建数据集的方式参见`.list_files()`、`.from_generator()`方法和`TextLineDataset`、`TFRecordDataset`、`FixedLengthRecordDataset`类；\n",
    "\n",
    "2. 对数据进行预处理；例如使用`dataset.map(lambda x: x*2)`方法将数据集每个元素乘二；\n",
    "\n",
    "3. 遍历并生成数据；由于对数据的遍历方式是流模式，进而不需要将整个数据集加载至内存中；\n",
    "\n",
    "参数`variant_tensor`指能够表示数据集的一个`DT_VARIANT`张量；\n",
    "\n",
    "这里需要辨识一下官方英文文档及教程中的 element 和 component 的关系；element 指对`Dataset`对象使用`next`函数时产生的单个输出，其可以是包含了众多 component 的嵌套对象，嵌套结构可以是元祖、namedtuple、字典；需要注意的是，列表这里并不被视为嵌套结构；相对的，component 则是该嵌套中的每个叶结点，其类型是能够用`tf.TypeSpec`表示的任何类型，包括`tf.Tensor`、`tf.data.Dataset`、`tf.SparseTensor`、`tf.RaggedTensor`、`tf.TensorArray`等；例如 element `(1, (3, \"apple\"), [1, 2])`所含的 component 为`1`、`3`、`\"apple\"`、`[1, 2]`；\n",
    "\n",
    "\n",
    "\n",
    "**File**:   \\tensorflow\\python\\data\\ops\\dataset_ops.py\n",
    "\n",
    "**Type**:           ABCMeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices(tensors)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "`tensors`为张量构成的数据集，所有张量第一维形状必须相同；该函数沿着它们的第一维度将`tensors`进行切片；这个操作保留了各张量的结构，移除了每个张量的第一个维数，并使用它作为数据集维数；\n",
    "\n",
    "注意：若`tensors`包含 Numpy 数组并且预期的操作无法执行，这些值会作为一个或多个`tf.constant`嵌入至计算图中；对于大型数据集(> 1 GB)，这可能会浪费内存并遇到图形序列化的字节限制；若`tensors`包含一个或多个大型 Numpy 数组，须考虑另外的可行方案[this guide](\n",
    "https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a 1D tensor produces scalar tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for k, v in dataset.__dict__.items():\n",
    "    print(k, v, sep=\"\\n\")\n",
    "    print()\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a 2D tensor produces 1D tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [3, 4, 5], [4, 5, 6]])\n",
    "print(dataset._tensors)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a tuple of 1D tensors produces tuple elements containing\n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [3, 4, 5], [5, 6, 7]))\n",
    "print(dataset._tensors, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary structure is also preserved.\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2, 3], \"b\": [3, 4, 5]})\n",
    "print(dataset._tensors, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data, end=\"\\n\\n\")\n",
    "for k, v in dataset.__dict__.items():\n",
    "    print(k, v, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3])>, <tf.Tensor: shape=(), dtype=string, numpy=b'A'>)\n",
      "\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1])>, <tf.Tensor: shape=(), dtype=string, numpy=b'B'>)\n",
      "\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3])>, <tf.Tensor: shape=(), dtype=string, numpy=b'A'>)\n",
      "\n",
      "(array([1, 3]), b'A')\n",
      "\n",
      "(array([2, 1]), b'B')\n",
      "\n",
      "(array([3, 3]), b'A')\n",
      "\n",
      "[<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[1, 3],\n",
      "       [2, 1],\n",
      "       [3, 3]])>, <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'A', b'B', b'A'], dtype=object)>]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1, 3], [2, 1], [3, 3]])\n",
    "labels = tf.constant(['A', 'B', 'A'])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "for data in dataset:\n",
    "    print(data, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element, end=\"\\n\\n\")\n",
    "print(dataset._tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1, 3]), b'A'), (array([2, 1]), b'B'), (array([3, 3]), b'A')]\n",
      "\n",
      "(array([1, 3]), b'A')\n",
      "(array([2, 1]), b'B')\n",
      "(array([3, 3]), b'A')\n"
     ]
    }
   ],
   "source": [
    "# Both the features and the labels tensors can be converted to a Dataset object separately and combined after.\n",
    "features_dataset = Dataset.from_tensor_slices(features)\n",
    "labels_dataset = Dataset.from_tensor_slices(labels)\n",
    "dataset = Dataset.zip((features_dataset, labels_dataset))\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 3],\n",
      "       [2, 3]]), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]]), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]]), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "# A batched feature and label set can be converted to a Dataset in similar fashion.\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2, 1))\n",
    "dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "help(data.Dataset)\n",
    "\n",
    "Help on class DatasetV2 in module tensorflow.python.data.ops.dataset_ops:\n",
    "\n",
    "\n",
    "\n",
    "**Methods**\n",
    "\n",
    "apply(self, transformation_func)\n",
    "\n",
    "\n",
    "as_numpy_iterator(self)\n",
    "\n",
    "\n",
    "batch(self, batch_size, drop_remainder=False)\n",
    "\n",
    "cache(self, filename='')\n",
    "\n",
    "\n",
    "cardinality(self)\n",
    "\n",
    "\n",
    "concatenate(self, dataset)\n",
    "\n",
    "\n",
    "enumerate(self, start=0)\n",
    "\n",
    "\n",
    "filter(self, predicate)\n",
    "\n",
    "\n",
    "flat_map(self, map_func)\n",
    "\n",
    "\n",
    "interleave(self, map_func, cycle_length=None, block_length=None, num_parallel_calls=None, deterministic=None)\n",
    "\n",
    "\n",
    "map(self, map_func, num_parallel_calls=None, deterministic=None)\n",
    "\n",
    "\n",
    "options(self)\n",
    "\n",
    "padded_batch(self, batch_size, padded_shapes=None, padding_values=None, drop_remainder=False)\n",
    "\n",
    "prefetch(self, buffer_size)\n",
    "\n",
    "\n",
    "reduce(self, initial_state, reduce_func)\n",
    "\n",
    "repeat(self, count=None)\n",
    "\n",
    "\n",
    "shard(self, num_shards, index)\n",
    "\n",
    "\n",
    "shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)\n",
    "\n",
    "\n",
    "skip(self, count)\n",
    "\n",
    "\n",
    "take(self, count)\n",
    "\n",
    "\n",
    "unbatch(self)\n",
    "\n",
    "\n",
    "window(self, size, shift=None, stride=1, drop_remainder=False)\n",
    "\n",
    "\n",
    "with_options(self, options)\n",
    "\n",
    "\n",
    "\n",
    "**Static methods**\n",
    "\n",
    "from_generator(generator, output_types=None, output_shapes=None, args=None, output_signature=None)\n",
    "\n",
    "\n",
    "from_tensor_slices(tensors)\n",
    "\n",
    "\n",
    "from_tensors(tensors)\n",
    "\n",
    "\n",
    "list_files(file_pattern, shuffle=None, seed=None)\n",
    "\n",
    "\n",
    "range(*args, **kwargs)\n",
    "\n",
    "\n",
    "zip(datasets)\n",
    "\n",
    "\n",
    "**Readonly properties**\n",
    "\n",
    "element_spec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
