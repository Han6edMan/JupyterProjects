{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to graphs and tf.functions\n",
    "\n",
    "下面将介绍 TF 将代码转换为计算图的方法、计算图的保存和表示的方法，以及如何利用计算图来进行加速并导出模型；这涉及到 TF 内部机制，若需要快速上手，请参见 [Keras 相关指南](http://localhost:8888/tree/Help_Viewer_Python/TensorFlow/TF2/Guide/Keras)；这仅仅是对计算图与 TF 函数的一个简短的介绍，有关这些概念更完整的解释请参见[`tf.function`指南](https://www.tensorflow.org/guide/function)；\n",
    "\n",
    "需要注意的是，这里的计算图与 TF1.x 并不相同；\n",
    "\n",
    "## 计算图\n",
    "\n",
    "TF 的即时执行模式下，TF 操作由 Python 逐个执行，操作的返回结果也会返回给 Python；TF 的即时执行模式利用了 GPU 的优势，进而可以在 GPU 和 TPU 上部署`Variable`、`Tensor`、操作等，然而逐个执行 TF 操作的机制将无法使用很多加速机制；\n",
    "\n",
    "计算图是由张量计算抽象化得到的图，其由一系列`tf.Operation`对象和`tf.Tensor`对象组成的数据结构，其中每个`tf.Operation`对象代表一个计算单元，每个`tf.Tensor`对象代表在运算之间流动的数据单元；这两种对象是在`tf.Graph`上下文管理器中定义的；由于计算图本就是数据结构，进而其可以脱离 Python 代码而独立进行保存、运行、恢复；\n",
    "\n",
    "\n",
    "### 计算图的优点\n",
    "计算图给予使用者很大的灵活性；用户可以在没有 Python 解释器的环境下(例如移动设备、嵌入式设备、后端服务器等)使用 TF 计算图；TF 从 Python 导出模型时会使用计算图作为保存模型的格式；\n",
    "\n",
    "计算图也很容易进行优化，其允许编译器进行以下的转换：\n",
    "- 通过折叠计算图中的常数节点来对张量的值进行静态推断\n",
    "- 将一个计算的各个互相独立的部分分开，并在不同的设备和线程之间进行分配\n",
    "- 通过消除公共的子表达式，进而简化算术操作\n",
    "\n",
    "此外，还有一个完整的优化系统——[`Grappler`](https://www.tensorflow.org/guide/graph_optimization)——来执行这些操作以及其他加速；简而言之，计算图可以使 TF 更快更高效地、在多种设备上并行运行；\n",
    "\n",
    "## Tracing graphs\n",
    "通过调用[`tf.function`](../../API/tf.x.ipynb#tf.function())或将其作为修饰符，进而可以在 TF 中创建计算图；被修饰为`tf.function`的函数执行起来与原函数相同，不过他们含有一个特殊的类`python.eager.def_function.Function`\n",
    "\n",
    "```python\n",
    "def func_wo_graph(x, y, b):\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "    return x\n",
    "\n",
    "func_using_graph = tf.function(func_wo_graph)\n",
    "x0 = tf.constant([[1.0, 2.0]])\n",
    "y0 = tf.constant([[2.0], [3.0]])\n",
    "b0 = tf.constant(4.0)\n",
    "func_using_graph(x1, y1, b1).numpy()\n",
    "\n",
    "```\n",
    "\n",
    "tf.function-ized functions are Python callables that work the same as their Python equivalents. They have a particular class (python.eager.def_function.Function), but to you they act just as the non-traced version.\n",
    "\n",
    "tf.function recursively traces any Python function it calls.\n",
    "```\n",
    "def inner_function(x, y, b):\n",
    "  x = tf.matmul(x, y)\n",
    "  x = x + b\n",
    "  return x\n",
    "\n",
    "# Use the decorator\n",
    "@tf.function\n",
    "def outer_function(x):\n",
    "  y = tf.constant([[2.0], [3.0]])\n",
    "  b = tf.constant(4.0)\n",
    "\n",
    "  return inner_function(x, y, b)\n",
    "\n",
    "# Note that the callable will create a graph that\n",
    "# includes inner_function() as well as outer_function()\n",
    "outer_function(tf.constant([[1.0, 2.0]])).numpy()\n",
    "array([[12.]], dtype=float32)\n",
    "```\n",
    "\n",
    "\n",
    "### Flow control and side effects\n",
    "Flow control and loops are converted to TensorFlow via tf.autograph by default. Autograph uses a combination of methods, including standardizing loop constructs, unrolling, and AST manipulation.\n",
    "\n",
    "def my_function(x):\n",
    "  if tf.reduce_sum(x) <= 1:\n",
    "    return x * x\n",
    "  else:\n",
    "    return x-1\n",
    "\n",
    "a_function = tf.function(my_function)\n",
    "\n",
    "print(\"First branch, with graph:\", a_function(tf.constant(1.0)).numpy())\n",
    "print(\"Second branch, with graph:\", a_function(tf.constant([5.0, 5.0])).numpy())\n",
    "First branch, with graph: 1.0\n",
    "Second branch, with graph: [4. 4.]\n",
    "\n",
    "You can directly call the Autograph conversion to see how Python is converted into TensorFlow ops. This is, mostly, unreadable, but you can see the transformation.\n",
    "```\n",
    "# Don't read the output too carefully.\n",
    "print(tf.autograph.to_code(my_function))\n",
    "def tf__my_function(x):\n",
    "    with ag__.FunctionScope('my_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
    "        do_return = False\n",
    "        retval_ = ag__.UndefinedReturnValue()\n",
    "\n",
    "        def get_state():\n",
    "            return (do_return, retval_)\n",
    "\n",
    "        def set_state(vars_):\n",
    "            nonlocal do_return, retval_\n",
    "            (do_return, retval_) = vars_\n",
    "\n",
    "        def if_body():\n",
    "            nonlocal do_return, retval_\n",
    "            try:\n",
    "                do_return = True\n",
    "                retval_ = (ag__.ld(x) * ag__.ld(x))\n",
    "            except:\n",
    "                do_return = False\n",
    "                raise\n",
    "\n",
    "        def else_body():\n",
    "            nonlocal do_return, retval_\n",
    "            try:\n",
    "                do_return = True\n",
    "                retval_ = (ag__.ld(x) - 1)\n",
    "            except:\n",
    "                do_return = False\n",
    "                raise\n",
    "        ag__.if_stmt((ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) <= 1), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
    "        return fscope.ret(retval_, do_return)\n",
    "```\n",
    "\n",
    "Autograph automatically converts if-then clauses, loops, break, return, continue, and more.\n",
    "\n",
    "Most of the time, Autograph will work without special considerations. However, there are some caveats, and the tf.function guide can help here, as well as the complete autograph reference\n",
    "\n",
    "### Seeing the speed up\n",
    "Just wrapping a tensor-using function in tf.function does not automatically speed up your code. For small functions called a few times on a single machine, the overhead of calling a graph or graph fragment may dominate runtime. Also, if most of the computation was already happening on an accelerator, such as stacks of GPU-heavy convolutions, the graph speedup won't be large.\n",
    "\n",
    "For complicated computations, graphs can provide a significant speedup. This is because graphs reduce the Python-to-device communication and perform some speedups.\n",
    "\n",
    "The speedup is most obvious when running many small layers, as in the example below:\n",
    "```\n",
    "# Create an oveerride model to classify pictures\n",
    "class SequentialModel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(SequentialModel, self).__init__(**kwargs)\n",
    "    self.flatten = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "    # Add a lot of small layers\n",
    "    num_layers = 100\n",
    "    self.my_layers = [tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "                      for n in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "    self.dense_2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.flatten(x)\n",
    "    for layer in self.my_layers:\n",
    "      x = layer(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense_2(x)\n",
    "    return x\n",
    "input_data = tf.random.uniform([20, 28, 28])\n",
    "eager_model = SequentialModel()\n",
    "\n",
    "# Don't count the time for the initial build.\n",
    "eager_model(input_data)\n",
    "print(\"Eager time:\", timeit.timeit(lambda: eager_model(input_data), number=100))\n",
    "Eager time: 2.185799148000001\n",
    "\n",
    "# Wrap the call method in a `tf.function`\n",
    "graph_model = SequentialModel()\n",
    "graph_model.call = tf.function(graph_model.call)\n",
    "\n",
    "# Don't count the time for the initial build and trace.\n",
    "graph_model(input_data)\n",
    "print(\"Graph time:\", timeit.timeit(lambda: graph_model(input_data), number=100))\n",
    "Graph time: 0.30396231500003523\n",
    "```\n",
    "\n",
    "### Polymorphic functions\n",
    "When you trace a function, you create a Function object that is polymorphic. A polymorphic function is a Python callable that encapsulates several concrete function graphs behind one API.\n",
    "\n",
    "You can use this Function on all different kinds of dtypes and shapes. Each time you invoke it with a new argument signature, the original function gets re-traced with the new arguments. The Function then stores the tf.Graph corresponding to that trace in a concrete_function. If the function has already been traced with that kind of argument, you just get your pre-traced graph.\n",
    "\n",
    "Conceptually, then:\n",
    "\n",
    "A tf.Graph is the raw, portable data structure describing a computation\n",
    "A Function is a caching, tracing, dispatcher over ConcreteFunctions\n",
    "A ConcreteFunction is an eager-compatible wrapper around a graph that lets you execute the graph from Python\n",
    "### Inspecting polymorphic functions\n",
    "You can inspect a_function, which is the result of calling tf.function on the Python function my_function. In this example, calling a_function with three kinds of arguments results in three different concrete functions.\n",
    "```\n",
    "print(a_function)\n",
    "\n",
    "print(\"Calling a `Function`:\")\n",
    "print(\"Int:\", a_function(tf.constant(2)))\n",
    "print(\"Float:\", a_function(tf.constant(2.0)))\n",
    "print(\"Rank-1 tensor of floats\", a_function(tf.constant([2.0, 2.0, 2.0])))\n",
    "<tensorflow.python.eager.def_function.Function object at 0x7f90cc28e4a8>\n",
    "Calling a `Function`:\n",
    "Int: tf.Tensor(1, shape=(), dtype=int32)\n",
    "Float: tf.Tensor(1.0, shape=(), dtype=float32)\n",
    "Rank-1 tensor of floats tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
    "\n",
    "# Get the concrete function that works on floats\n",
    "print(\"Inspecting concrete functions\")\n",
    "print(\"Concrete function for float:\")\n",
    "print(a_function.get_concrete_function(tf.TensorSpec(shape=[], dtype=tf.float32)))\n",
    "print(\"Concrete function for tensor of floats:\")\n",
    "print(a_function.get_concrete_function(tf.constant([2.0, 2.0, 2.0])))\n",
    "Inspecting concrete functions\n",
    "Concrete function for float:\n",
    "ConcreteFunction my_function(x)\n",
    "  Args:\n",
    "    x: float32 Tensor, shape=()\n",
    "  Returns:\n",
    "    float32 Tensor, shape=()\n",
    "Concrete function for tensor of floats:\n",
    "ConcreteFunction my_function(x)\n",
    "  Args:\n",
    "    x: float32 Tensor, shape=(3,)\n",
    "  Returns:\n",
    "    float32 Tensor, shape=(3,)\n",
    "\n",
    "# Concrete functions are callable\n",
    "# Note: You won't normally do this, but instead just call the containing `Function`\n",
    "cf = a_function.get_concrete_function(tf.constant(2))\n",
    "print(\"Directly calling a concrete function:\", cf(tf.constant(2)))\n",
    "Directly calling a concrete function: tf.Tensor(1, shape=(), dtype=int32)\n",
    "```\n",
    "In this example, you are seeing pretty far into the stack. Unless you are specifically managing tracing, you will not normally need to call concrete functions directly as shown here.\n",
    "\n",
    "## Reverting to eager execution\n",
    "You may find yourself looking at long stack traces, specially ones that refer to tf.Graph or with tf.Graph().as_default(). This means you are likely running in a graph context. Core functions in TensorFlow use graph contexts, such as Keras's model.fit().\n",
    "\n",
    "It is often much easier to debug eager execution. Stack traces should be relatively short and easy to comprehend.\n",
    "\n",
    "In situations where the graph makes debugging tricky, you can revert to using eager execution to debug.\n",
    "\n",
    "Here are ways you can make sure you are running eagerly:\n",
    "\n",
    "Call models and layers directly as callables\n",
    "\n",
    "When using Keras compile/fit, at compile time use model.compile(run_eagerly=True)\n",
    "\n",
    "Set global execution mode via tf.config.run_functions_eagerly(True)\n",
    "\n",
    "Using run_eagerly=True\n",
    "```\n",
    "# Define an identity layer with an eager side effect\n",
    "class EagerLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(EagerLayer, self).__init__(**kwargs)\n",
    "    # Do some kind of initialization here\n",
    "\n",
    "  def call(self, inputs):\n",
    "    print(\"\\nCurrently running eagerly\", str(datetime.now()))\n",
    "    return inputs\n",
    "# Create an override model to classify pictures, adding the custom layer\n",
    "class SequentialModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(SequentialModel, self).__init__()\n",
    "    self.flatten = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "    self.dense_1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
    "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "    self.dense_2 = tf.keras.layers.Dense(10)\n",
    "    self.eager = EagerLayer()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense_1(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense_2(x)\n",
    "    return self.eager(x)\n",
    "\n",
    "# Create an instance of this model\n",
    "model = SequentialModel()\n",
    "\n",
    "# Generate some nonsense pictures and labels\n",
    "input_data = tf.random.uniform([60, 28, 28])\n",
    "labels = tf.random.uniform([60])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "```\n",
    "First, compile the model without eager. Note that the model is not traced; despite its name, compile only sets up loss functions, optimization, and other training parameters.\n",
    "\n",
    "model.compile(run_eagerly=False, loss=loss_fn)\n",
    "Now, call fit and see that the function is traced (twice) and then the eager effect never runs again.\n",
    "```\n",
    "model.fit(input_data, labels, epochs=3)\n",
    "Epoch 1/3\n",
    "\n",
    "Currently running eagerly 2021-01-13 02:25:36.809205\n",
    "\n",
    "Currently running eagerly 2021-01-13 02:25:36.941800\n",
    "2/2 [==============================] - 0s 3ms/step - loss: 2.0352\n",
    "Epoch 2/3\n",
    "2/2 [==============================] - 0s 3ms/step - loss: 0.0045\n",
    "Epoch 3/3\n",
    "2/2 [==============================] - 0s 2ms/step - loss: 0.0026\n",
    "\n",
    "<tensorflow.python.keras.callbacks.History at 0x7f90102f2550>\n",
    "If you run even a single epoch in eager, however, you can see the eager side effect twice.\n",
    "\n",
    "print(\"Running eagerly\")\n",
    "# When compiling the model, set it to run eagerly\n",
    "model.compile(run_eagerly=True, loss=loss_fn)\n",
    "\n",
    "model.fit(input_data, labels, epochs=1)\n",
    "Running eagerly\n",
    "\n",
    "Currently running eagerly 2021-01-13 02:25:37.173159\n",
    "1/2 [==============>...............] - ETA: 0s - loss: 0.0023\n",
    "Currently running eagerly 2021-01-13 02:25:37.195392\n",
    "2/2 [==============================] - 0s 13ms/step - loss: 0.0016\n",
    "\n",
    "<tensorflow.python.keras.callbacks.History at 0x7f90101981d0>\n",
    "```\n",
    "Using run_functions_eagerly\n",
    "You can also globally set everything to run eagerly. This is a switch that bypasses the polymorphic function's traced functions and calls the original function directly. You can use this for debugging.\n",
    "```\n",
    "# Now, globally set everything to run eagerly\n",
    "tf.config.run_functions_eagerly(True)\n",
    "print(\"Run all functions eagerly.\")\n",
    "\n",
    "# Create a polymorphic function\n",
    "polymorphic_function = tf.function(model)\n",
    "\n",
    "print(\"Tracing\")\n",
    "# This does, in fact, trace the function\n",
    "print(polymorphic_function.get_concrete_function(input_data))\n",
    "\n",
    "print(\"\\nCalling twice eagerly\")\n",
    "# When you run the function again, you will see the side effect\n",
    "# twice, as the function is running eagerly.\n",
    "result = polymorphic_function(input_data)\n",
    "result = polymorphic_function(input_data)\n",
    "Run all functions eagerly.\n",
    "```\n",
    "Tracing\n",
    "```\n",
    "Currently running eagerly 2021-01-13 02:25:37.594444\n",
    "ConcreteFunction function(self)\n",
    "  Args:\n",
    "    self: float32 Tensor, shape=(60, 28, 28)\n",
    "  Returns:\n",
    "    float32 Tensor, shape=(60, 10)\n",
    "\n",
    "Calling twice eagerly\n",
    "\n",
    "Currently running eagerly 2021-01-13 02:25:37.600183\n",
    "\n",
    "Currently running eagerly 2021-01-13 02:25:37.602196\n",
    "\n",
    "# Don't forget to set it back when you are done\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "WARNING:tensorflow:From <ipython-input-1-782fe9ce7b18>:2: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
    "```\n",
    "Instructions for updating:\n",
    "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n",
    "\n",
    "Tracing and performance\n",
    "Tracing costs some overhead. Although tracing small functions is quick, large models can take noticeable wall-clock time to trace. This investment is usually quickly paid back with a performance boost, but it's important to be aware that the first few epochs of any large model training can be slower due to tracing.\n",
    "\n",
    "No matter how large your model, you want to avoid tracing frequently. This section of the tf.function guide discusses how to set input specifications and use tensor arguments to avoid retracing. If you find you are getting unusually poor performance, it's good to check to see if you are retracing accidentally.\n",
    "\n",
    "You can add an eager-only side effect (such as printing a Python argument) so you can see when the function is being traced. Here, you see extra retracing because new Python arguments always trigger retracing.\n",
    "```\n",
    "# Use @tf.function decorator\n",
    "@tf.function\n",
    "def a_function_with_python_side_effect(x):\n",
    "  print(\"Tracing!\")  # This eager\n",
    "  return x * x + tf.constant(2)\n",
    "\n",
    "# This is traced the first time\n",
    "print(a_function_with_python_side_effect(tf.constant(2)))\n",
    "# The second time through, you won't see the side effect\n",
    "print(a_function_with_python_side_effect(tf.constant(3)))\n",
    "\n",
    "# This retraces each time the Python argument changes,\n",
    "# as a Python argument could be an epoch count or other\n",
    "# hyperparameter\n",
    "print(a_function_with_python_side_effect(2))\n",
    "print(a_function_with_python_side_effect(3))\n",
    "Tracing!\n",
    "tf.Tensor(6, shape=(), dtype=int32)\n",
    "tf.Tensor(11, shape=(), dtype=int32)\n",
    "Tracing!\n",
    "tf.Tensor(6, shape=(), dtype=int32)\n",
    "Tracing!\n",
    "tf.Tensor(11, shape=(), dtype=int32)\n",
    "```\n",
    "Next steps\n",
    "You can read a more in-depth discussion at both the tf.function API reference page and at the guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
