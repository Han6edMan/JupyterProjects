{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import matplotlib.pyplot as plt\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**！！！<br>下面以“张量”代指泛指的张量，即可能为`Tensor`对象，也可能为`Variable`对象；如不做特殊说明，“变量”特指`Variable`对象**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Tensors\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Variables\n",
    "</center>\n",
    "\n",
    "## 1. `Variable`的创建\n",
    "\n",
    "一般情况下`tf.Variable(initial_value)`以`initial_value`的数据类型和形状创建`Variable`实例，`initial_value`也可以是不带参数的可调用对象，其在调用时返回用于初始化的对象，这种情况下必须指明构造函数的`dtype`参数；需要注意的是，若调用`init_ops.py`模块中的初始化函数，则该初始化函数必须首先绑定到一个 shape 上；\n",
    "\n",
    "```python\n",
    "def init_fc():\n",
    "    return tf.constant([1, 2, 3])\n",
    "var = tf.Variable(init_fc)  # Different from `tf.Variable(init_fc())`\n",
    "assert tf.reduce_all(var == init_fc())\n",
    "\n",
    "```\n",
    "对`Variable`构造函数的`shape`参数指明`tf.TensorShape(None)`时，表示该变量的形状待定，进而可以在之后的代码中为其赋值，新赋予的值不必与传递给构造函数`initial_value`形状相同；需要注意的是，无论赋值多少次，`Variable`实例化对象的`shape`属性均为`<unknown>`，这意味着该对象可以多次赋予不同形状的张量；示例如下\n",
    "```python\n",
    "var = tf.Variable([[1.], [2.], [3.]], shape=tf.TensorShape(None))\n",
    "var.assign([[1., 2.], [3., 4.]])\n",
    "var.assign([1., 2., 3., 4.])\n",
    "```\n",
    "从现有`Variable`创建新的`Variable`只会复制其张量值，两个`Variable`不共享内存，即使在创建这些变量时使用了相同的名称，示例如下\n",
    "```python\n",
    "var1 = tf.Variable([2.0, 3.0], name=\"var_unique\")\n",
    "var2 = tf.Variable(var1, name=\"var_unique\")\n",
    "id(var1) == id(var2)  # ==> False\n",
    "var1.name, var2.name  # ==> ('var_unique:0', 'var_unique:0')\n",
    "```\n",
    "## 2. `Variable`与`Tensor`\n",
    "`tf.Variable`是由`tf.Tensor`支持的数据结构，其与`Tensor`一样也具有`shape`、`dtype`、`numpy()`等属性，所有为`Tensor`类重载的操作符都适用于`Variable`；但需要说明的是，大多张量操作会返回`Tensor`对象而非`Variable`对象，两者内存地址自然也不相同；而调用`var.assign()`，`var.assign_add()`等函数通常使用原内存地址并返回`Variable`对象；示例如下\n",
    "```python\n",
    "var1 = tf.Variable([2.0, 3.0])\n",
    "orig_id = id(var1)\n",
    "var1.assign_add([1, 2])\n",
    "orig_id == id(var1)  # ==> True\n",
    "var2 = tf.add(var1, [1, 2])\n",
    "id(var2) == id(var1)  # ==> False\n",
    "isinstance(var2, tf.Tensor)  # ==> True\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 3. Lifecycles, naming, and watching\n",
    "在基于 Python 的 TF 中，`tf.Variable`实例化对象与其他 Python 对象具有相同的生命周期，当没有对该变量的引用时，它会自动释放；\n",
    "\n",
    "对变量命名有利于对其跟踪和调试；\n",
    "\n",
    "保存和加载模型时会对变量名进行保存，默认情况下，模型中的变量将自动获取独有的变量名；\n",
    "\n",
    "创建变量时指明`trainable=False`可以不对其进行求导，例如训练步长计数器\n",
    "\n",
    "Variables are often captured and manipulated by `tf.function`s. This works the\n",
    "same way the un-decorated function would have:\n",
    "```\n",
    ">>> v = tf.Variable(0.)\n",
    ">>> read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n",
    ">>> read_and_decrement()\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\n",
    ">>> read_and_decrement()\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n",
    "\n",
    "Variables created inside a `tf.function` must be owned outside the function\n",
    "and be created only once:\n",
    "\n",
    ">>> class M(tf.Module):\n",
    "...   @tf.function\n",
    "...   def __call__(self, x):\n",
    "...     if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n",
    "...       self.v = tf.Variable(x)\n",
    "...     return self.v * x\n",
    ">>> m = M()\n",
    ">>> m(2.)\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>\n",
    ">>> m(3.)\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n",
    ">>> m.v\n",
    "<tf.Variable ... shape=() dtype=float32, numpy=2.0>\n",
    "```\n",
    "\n",
    "See the `tf.function` documentation for details.\n",
    "## 4. 部署`Variable`和`Tensor`\n",
    "为了提高性能，TF 总会在与张量数据类型所兼容的可用的最快设备上部署`Variable`和`Tensor`，即默认`tf.config.set_soft_device_placement(True)`；这意味着在 GPU 可用时，大多数`Variable`会被部署至 GPU 上；尽管如此，用户也可以通过`tf.device()`上下文管理器指定所使用的设备；可以通过设定`tf.debugging.set_log_device_placement(True)`来查看变量部署情况，注意该代码段需要在程序启动时运行\n",
    "``` python\n",
    "with tf.device('CPU:0'):\n",
    "    a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "print(c)\n",
    "```\n",
    "尽管手动部署可以运行，但分布式训练可以以一种更方便更灵活的方式来优化计算；\n",
    "- 更多使用`Variable`的方式参看[自动求导相关指南](#Gradients-and-automatic-differentiation)\n",
    "- 更多有关分布式训练的方法参见[相关指南](https://www.tensorflow.org/guide/distributed_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "# Automatic differentiation\n",
    "</center>\n",
    "\n",
    "TF 的[自动求导](https://en.wikipedia.org/wiki/Automatic_differentiation)功能主要靠[`tf.GradientTape`](http://localhost:8888/notebooks/Help_Viewer_Python/TensorFlow/TF2/API/tf.x.ipynb#tf.GradientTape())实现；调用`tf.GradientTape`会返回一个上下文管理器对象，它会记录**于其内部**发生在**所追踪的**`Variable`上的每一个操作、操作之间顺序、以及操作在相关张量上的导数值；在反向传播时，“梯度磁带”会逆向追溯这些操作，将相应导数值根据求导法则进行运算，进而实现自动求导；由于相应的导数值在向前传播时便已经计算好，进而反向传播时仅 [!TODO ]\n",
    "\n",
    "\n",
    "\n",
    "## 1. 计算导数\n",
    "`tape.gradient(target, sources)`利用“梯度磁带”所记录的操作及相关导数值计算`target`对`sources`的梯度，并返回一个与`sources`嵌套结构完全相同的对象(关于嵌套可参见`tf.nest`)；\n",
    "- 当`target`为标量时，返回对象中每个元素为`target`对`sources`中的相应元素的导数；\n",
    "\n",
    "    ```python\n",
    "    w = tf.Variable(tf.ones((3, 2)), name='w')\n",
    "    b = tf.Variable(tf.range(1, 3, dtype=tf.float32), name='b')\n",
    "    x = [[1., 2., 3.]]\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x @ w + b\n",
    "        loss = tf.reduce_mean(y**2)\n",
    "    [dl_dw, dl_db] = tape.gradient(loss, [w, {\"b\": b}])\n",
    "    isinstance(dl_dw, list) and isinstance(dl_db, dict)  # ==> True\n",
    "    ```\n",
    "    这有利于对`tf.Module`对象中变量求导，只需通过调用`Module.trainable_variables`属性将一个模型所有的可训练变量传递给“梯度磁带”，进而能够得到对模型中所有可训练变量的导数；\n",
    "\n",
    "- 当`target`为多个标量或非标量时，返回对象中每个元素为所有`target`对`sources`中的相应元素的导数的和\n",
    "\n",
    "    ```python\n",
    "    x = tf.Variable(2.0)\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        y = x**2\n",
    "        z = 1 / x\n",
    "        w = x * [3., 4.]\n",
    "    assert tape.gradient(y, x).numpy() == 4.0\n",
    "    assert tape.gradient(z, x).numpy() == -0.25\n",
    "    assert tape.gradient([y, z], x) == 3.75\n",
    "    assert tape.gradient(w, x) == 7.0\n",
    "    del tape   # Drop the reference to the tape\n",
    "    ```\n",
    "    这种方法常用于模型经 element-wise 计算得到的损失，且在进行 SGD 时以一个变量的所有梯度值之和为基础作参数更新的情况；如果需要对每个元素求导数值，请参考[雅可比矩阵](https://www.tensorflow.org/guide/advanced_autodiff#jacobians)\n",
    "\n",
    "这里参数`persistent`用于指明是否创建一个存留的“梯度磁带”；默认情况下“梯度磁带”会在调用`.gradient()`方法后释放所有持有的资源；若需要多次调用`.gradient()`以对多个梯度进行计算，须指明`persistent=True`，此时只有在丢弃相关引用后“梯度磁带”才会释放其内部资源；示例如上；\n",
    "\n",
    "## 2. 设置`GradientTape`所追踪的对象\n",
    "`tape.watched_variables()`方法返回所有“梯度磁带”正在追踪的 **`Variable`对象**所组成的元祖；“梯度磁带”默认对所有可训练的`Variable`进行追踪，而不对`Tensor`进行追踪；需要说明的是，尽管`Variable`在进行运算后输出中间结果为`Tensor`实例，但由于“梯度磁带”会记录发生在训练变量上所有操作以及相应梯度值，进而`target`对该中间结果的导数依旧可以求得，例如下面示例中`loss`对`y`的导数；\n",
    "\n",
    "```python\n",
    "w = tf.Variable(tf.ones((3, 2)), name='w')\n",
    "b = tf.Variable(tf.range(1, 3, dtype=tf.float32), name='b')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x @ w + b\n",
    "    assert isinstance(y, tf.Tensor)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "dl_dw, dl_dy, dl_dx = tape.gradient(loss, [w, y, x])\n",
    "assert dl_dy is not None\n",
    "assert dl_dx is None\n",
    "```\n",
    "\n",
    "可以通过指定`watch_accessed_variables=False`来取消“梯度磁带”对所有变量自动追踪的功能，此时需通过`tape.watch(tensor)`来指定“梯度磁带”追踪哪一个变量；`.watch()`方法也可以作用于`Tensor`对象上，但通过`.watch()`被追踪的`Tensor`并不会被添加在`.watched_variables()`方法所返回的元素中，实例如下\n",
    "```python\n",
    "x0 = tf.constant(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "x2 = tf.Variable(-1.0)\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x0)\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = tf.reduce_sum(y0 + y1 + x2)\n",
    "dy_dx0, dy_dx1, dy_dx2 = tape.gradient(y, [x0, x1, x2])\n",
    "assert len(tape.watched_variables()) == 1\n",
    "assert dy_dx2 is None  # x2 is not watched\n",
    "```\n",
    "## 3. 控制流\n",
    "“梯度磁带”的上下文管理器中的 Python 控制流会按其正常执行方式执行，例如下面shiyong`if`语句的例子，只有和操作涉及到的变量才具有梯度值；需要注意的是，控制语句本身是不可导的，进而其对于“梯度磁带”是不可见的，即对于`x`的导数永远是 None；\n",
    "\n",
    "```python\n",
    "x = tf.constant(1.0)\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0.0:\n",
    "        result = v0\n",
    "    else:\n",
    "        result = v1**2\n",
    "g_v0, g_v1, g_x = tape.gradient(result, [v0, v1, x])\n",
    "assert g_v1 is None and g_x is None\n",
    "assert g_v0 is not None\n",
    "```\n",
    "## 4. 得到梯度值为 None 的情况\n",
    "\n",
    "当计算图中`target`与`source`是不连通的状态时，对`source`求导会得到 None；除很明显的在计算图中没有连接之外，还可能存在以下不太明显的情况会使`target`与`source`处于不连通的状态\n",
    "1. 在“梯度磁带”中使用了非 TF 函数，例如 NumPy 函数\n",
    "2. TF 默认整型变量是不可导的，若无意间声明了整型变量，会返回 None\n",
    "3. 无意间将`Variable`替换成了`Tensor`对象，例如\n",
    "    ```python\n",
    "    x = tf.Variable(2.0)\n",
    "    for epoch in range(2):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = x+1\n",
    "        print(isinstance(x, tf.Variable))\n",
    "        print(tape.gradient(y, x) is not None)\n",
    "        x = x + 1\n",
    "    \"\"\" ==>\n",
    "    True\n",
    "    True\n",
    "    False\n",
    "    False\n",
    "    \"\"\"\n",
    "    ```\n",
    "\n",
    "4. 对一个含有状态的对象求导\n",
    "    `Tensor`对象一旦创建便不会再改变，至少在其值改变后，内存地址也随之改变，进而应视为另一个`Tensor`对象；`Tensor`对象含有值却不含有状态；目前为止讨论的所有操作都是无状态的，例如`tf.matmul`输出仅取决于其输入；\n",
    "\n",
    "    然而`Variable`却拥有内部状态，即它的取值；当“梯度磁带”调用该变量时，首先会对其状态进行读取；然而“梯度磁带”只能读取当前状态，而不能读取导致当前状态的历史，即变量的当前状态阻碍了对其更早状态的梯度的计算，例如下面的例子：\n",
    "    ```python\n",
    "    x0 = tf.Variable(3.0)\n",
    "    x1 = tf.Variable(0.0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        x1.assign_add(x0)  # The tape starts recording from x1 but not x0\n",
    "        y = x1**2\n",
    "\n",
    "    # dy/dx0 should have been 2 * (x1 + x0), but it won't work.\n",
    "    print(tape.gradient(y, x0))  # ==> None\n",
    "    ```\n",
    "    类似地，`tf.data.Dataset`迭代器和`tf.queues`也是含有状态的，进而会阻碍通过其自身的梯度流；\n",
    "\n",
    "可以通过指明`gradient()`方法的`unconnected_gradients`参数，来控制在`target`和`source`不连通时的返回值——`tf.UnconnectedGradients.NONE`或`tf.UnconnectedGradients.ZERO`\n",
    "## 5. 没有注册梯度的函数\n",
    "一些`tf.Operations`会注册为不可微函数，进而会返回 None，其他一些并没有注册梯度；在[`tf.raw_ops`](https://www.tensorflow.org/api_docs/python/tf/raw_ops)处可以查看有哪些低层操作注册了梯度；\n",
    "\n",
    "若通过没有注册梯度的操作来获取梯度，“梯度磁带”则会抛出异常而非返回 None；例如`tf.image.adjust_contrast`封装了`raw_ops.AdjustContrastv2`，该操作可以求导但并未对梯度进行实现；若想要通过这个操作进行微分，则需要人工实现梯度并利用`tf.RegisterGradient`注册梯度，或者使用其他操作重新实现该函数；\n",
    "```python\n",
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "with tf.GradientTape() as tape:\n",
    "    new_image = tf.image.adjust_contrast(image, delta)\n",
    "try:\n",
    "    print(tape.gradient(new_image, [image, delta]))\n",
    "    assert False\n",
    "except LookupError as e:\n",
    "    print(f'A {type(e).__name__} is raised:\\n{e}')\n",
    "\"\"\" ==>\n",
    "A LookupError is raised:\n",
    "gradient registry has no entry for: AdjustContrastv2\n",
    "\"\"\"\n",
    "```\n",
    "## 6. `GradientTape`对运行时性能的影响\n",
    "在“梯度磁带”上下文管理器中执行操作会增加开销，这个开销通常很小，对大多即时执行而言并不会增加明显的运算成本；“梯度磁带”会使用内存存储中间结果，例如输入和输出等，以便反向传播时使用；为了提高效率，有些操作例如`ReLU`则不需要保留中间结果，向前传递时它们从计算图中去除，但如果指明了`persistent=True`，则在计算过程中“梯度磁带”不会丢弃任何内容，进而内存占用的峰值会更高；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
