{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager execution\n",
    "\n",
    "TF2 的即时执行模式是一种命令式编程(imperative programming)环境，它在执行操作时直接返回具体值，而非构建一个稍后运行的计算图；需要说明的是，某些模型在即时执行模式下的运行开销会增大；\n",
    "\n",
    "\n",
    "\n",
    "## 基本使用方法\n",
    "\n",
    "TF2.x 中默认开启即时执行模式；该模式下运行 TF 操作会直接得到结果\n",
    "```python\n",
    "tf.executing_eagerly()  # ==> True\n",
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))\n",
    "```\n",
    "\n",
    "Enabling eager execution changes how TensorFlow operations behave—now they immediately evaluate and return their values to Python. tf.Tensor objects reference concrete values instead of symbolic handles to nodes in a computational graph. Since there isn't a computational graph to build and run later in a session, it's easy to inspect results using print() or a debugger. Evaluating, printing, and checking tensor values does not break the flow for computing gradients.\n",
    "\n",
    "即时执行可以很好地与 np 结合使用；np 操作接受`tf.Tensor`参数；`tf.math`可以将 Python 对象和 NumPy 数组转换为`tf.Tensor`对象；`tf.Tensor.numpy`可以将 tf 的张量转换为 np 的 ndarray；\n",
    "```python\n",
    "x = tf.constant([[1, 2], [3, 4]])\n",
    "y = tf.add(a, 1)  # Broadcasting is supported\n",
    "z = x * y  # Operator overloading is supported\n",
    "u = np.multiply(x, y)  # Use NumPy values\n",
    "v = a.numpy()  # Obtain numpy value from a tensor\n",
    "```\n",
    "\n",
    "\n",
    "## Eager training\n",
    "\n",
    "### Computing gradients\n",
    "自动微分机制有利于实现反向传播算法，在即时执行模式下，可以使用`tf.GradientTape`对操作进行追踪，以便于后续对梯度的计算；一个`tf.GradientTape`实例化对象通常只能计算一个梯度，对其重复的调用会报错；\n",
    "\n",
    "```python\n",
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)\n",
    "```\n",
    "\n",
    "### Train a model\n",
    "The following example creates a multi-layer model that classifies the standard MNIST handwritten digits. It demonstrates the optimizer and layer APIs to build trainable graphs in an eager execution environment.\n",
    "\n",
    "```python\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels, tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu',\n",
    "                           input_shape=(None, None, 1)),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "Even without training, call the model and inspect the output in eager execution:\n",
    "\n",
    "for images,labels in dataset.take(1):\n",
    "    print(\"Logits: \", mnist_model(images[0:1]).numpy())\n",
    "\n",
    "```\n",
    "\n",
    "While keras models have a builtin training loop (using the fit method), sometimes you need more customization. Here's an example, of a training loop implemented with eager:\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_history = []\n",
    "Note: Use the assert functions in tf.debugging to check if a condition holds up. This works in eager and graph execution.\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      train_step(images, labels)\n",
    "    print ('Epoch {} finished'.format(epoch))\n",
    "train(epochs = 3)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss [entropy]')\n",
    "\n",
    "\n",
    "### Variables and optimizers\n",
    "tf.Variable objects store mutable tf.Tensor-like values accessed during training to make automatic differentiation easier.\n",
    "\n",
    "The collections of variables can be encapsulated into layers or models, along with methods that operate on them. See Custom Keras layers and models for details. The main difference between layers and models is that models add methods like Model.fit, Model.evaluate, and Model.save.\n",
    "\n",
    "For example, the automatic differentiation example above can be rewritten:\n",
    "```\n",
    "class Linear(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Linear, self).__init__()\n",
    "    self.W = tf.Variable(5., name='weight')\n",
    "    self.B = tf.Variable(10., name='bias')\n",
    "  def call(self, inputs):\n",
    "    return inputs * self.W + self.B\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random.normal([NUM_EXAMPLES])\n",
    "noise = tf.random.normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "```\n",
    "Next:\n",
    "\n",
    "Create the model.\n",
    "The Derivatives of a loss function with respect to model parameters.\n",
    "A strategy for updating the variables based on the derivatives.\n",
    "```\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "steps = 300\n",
    "for i in range(steps):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "Initial loss: 68.602\n",
    "Loss at step 000: 65.948\n",
    "Loss at step 020: 30.156\n",
    "Loss at step 040: 14.091\n",
    "Loss at step 060: 6.878\n",
    "Loss at step 080: 3.637\n",
    "Loss at step 100: 2.180\n",
    "Loss at step 120: 1.525\n",
    "Loss at step 140: 1.231\n",
    "Loss at step 160: 1.098\n",
    "Loss at step 180: 1.038\n",
    "Loss at step 200: 1.011\n",
    "Loss at step 220: 0.999\n",
    "Loss at step 240: 0.994\n",
    "Loss at step 260: 0.991\n",
    "Loss at step 280: 0.990\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "Final loss: 0.990\n",
    "\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))\n",
    "W = 3.001922369003296, B = 2.0047335624694824\n",
    "```\n",
    "Note: Variables persist until the last reference to the python object is removed, and is the variable is deleted.\n",
    "### Object-based saving\n",
    "A tf.keras.Model includes a convenient save_weights method allowing you to easily create a checkpoint:\n",
    "\n",
    "model.save_weights('weights')\n",
    "status = model.load_weights('weights')\n",
    "Using tf.train.Checkpoint you can take full control over this process.\n",
    "\n",
    "This section is an abbreviated version of the guide to training checkpoints.\n",
    "```\n",
    "x = tf.Variable(10.)\n",
    "checkpoint = tf.train.Checkpoint(x=x)\n",
    "x.assign(2.)   # Assign a new value to the variables and save.\n",
    "checkpoint_path = './ckpt/'\n",
    "checkpoint.save('./ckpt/')\n",
    "'./ckpt/-1'\n",
    "x.assign(11.)  # Change the variable after saving.\n",
    "\n",
    "# Restore values from the checkpoint\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "print(x)  # => 2.0\n",
    "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n",
    "```\n",
    "To save and load models, tf.train.Checkpoint stores the internal state of objects, without requiring hidden variables. To record the state of a model, an optimizer, and a global step, pass them to a tf.train.Checkpoint:\n",
    "```\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "checkpoint_dir = 'path/to/model_dir'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "  os.makedirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                           model=model)\n",
    "\n",
    "root.save(checkpoint_prefix)\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9abd45f1d0>\n",
    "```\n",
    "Note: In many training loops, variables are created after tf.train.Checkpoint.restore is called. These variables will be restored as soon as they are created, and assertions are available to ensure that a checkpoint has been fully loaded. See the guide to training checkpoints for details.\n",
    "Object-oriented metrics\n",
    "tf.keras.metrics are stored as objects. Update a metric by passing the new data to the callable, and retrieve the result using the tf.keras.metrics.result method, for example:\n",
    "```\n",
    "m = tf.keras.metrics.Mean(\"loss\")\n",
    "m(0)\n",
    "m(5)\n",
    "m.result()  # => 2.5\n",
    "m([8, 9])\n",
    "m.result()  # => 5.5\n",
    "<tf.Tensor: shape=(), dtype=float32, numpy=5.5>\n",
    "```\n",
    "### Summaries and TensorBoard\n",
    "TensorBoard is a visualization tool for understanding, debugging and optimizing the model training process. It uses summary events that are written while executing the program.\n",
    "\n",
    "You can use tf.summary to record summaries of variable in eager execution. For example, to record summaries of loss once every 100 training steps:\n",
    "```\n",
    "logdir = \"./tb/\"\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "steps = 1000\n",
    "with writer.as_default():  # or call writer.set_as_default() before the loop.\n",
    "  for i in range(steps):\n",
    "    step = i + 1\n",
    "    # Calculate loss with your real train function.\n",
    "    loss = 1 - 0.001 * step\n",
    "    if step % 100 == 0:\n",
    "      tf.summary.scalar('loss', loss, step=step)\n",
    "ls tb/\n",
    "events.out.tfevents.1602033841.kokoro-gcp-ubuntu-prod-892356553.1670.619697.v2\n",
    "```\n",
    "## Advanced automatic differentiation topics\n",
    "### Dynamic models\n",
    "tf.GradientTape can also be used in dynamic models. This example for a backtracking line search algorithm looks like normal NumPy code, except there are gradients and is differentiable, despite the complex control flow:\n",
    "```\n",
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Variables are automatically tracked.\n",
    "    # But to calculate a gradient from a tensor, you must `watch` it.\n",
    "    tape.watch(init_x)\n",
    "    value = fn(init_x)\n",
    "  grad = tape.gradient(value, init_x)\n",
    "  grad_norm = tf.reduce_sum(grad * grad)\n",
    "  init_value = value\n",
    "  while value > init_value - rate * grad_norm:\n",
    "    x = init_x - rate * grad\n",
    "    value = fn(x)\n",
    "    rate /= 2.0\n",
    "  return x, value\n",
    "```\n",
    "### Custom gradients\n",
    "Custom gradients are an easy way to override gradients. Within the forward function, define the gradient with respect to the inputs, outputs, or intermediate results. For example, here's an easy way to clip the norm of the gradients in the backward pass:\n",
    "```\n",
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "  y = tf.identity(x)\n",
    "  def grad_fn(dresult):\n",
    "    return [tf.clip_by_norm(dresult, norm), None]\n",
    "  return y, grad_fn\n",
    "Custom gradients are commonly used to provide a numerically stable gradient for a sequence of operations:\n",
    "\n",
    "def log1pexp(x):\n",
    "  return tf.math.log(1 + tf.exp(x))\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    value = log1pexp(x)\n",
    "  return tape.gradient(value, x)\n",
    "# The gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.)).numpy()\n",
    "0.5\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "grad_log1pexp(tf.constant(100.)).numpy()\n",
    "nan\n",
    "```\n",
    "Here, the log1pexp function can be analytically simplified with a custom gradient. The implementation below reuses the value for tf.exp(x) that is computed during the forward pass—making it more efficient by eliminating redundant calculations:\n",
    "```\n",
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.math.log(1 + e), grad\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    value = log1pexp(x)\n",
    "  return tape.gradient(value, x)\n",
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.)).numpy()\n",
    "0.5\n",
    "# And the gradient computation also works at x = 100.\n",
    "grad_log1pexp(tf.constant(100.)).numpy()\n",
    "1.0\n",
    "```\n",
    "Performance\n",
    "Computation is automatically offloaded to GPUs during eager execution. If you want control over where a computation runs you can enclose it in a tf.device('/gpu:0') block (or the CPU equivalent):\n",
    "```\n",
    "import time\n",
    "\n",
    "def measure(x, steps):\n",
    "  # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "  tf.matmul(x, x)\n",
    "  start = time.time()\n",
    "  for i in range(steps):\n",
    "    x = tf.matmul(x, x)\n",
    "  # tf.matmul can return before completing the matrix multiplication\n",
    "  # (e.g., can return after enqueing the operation on a CUDA stream).\n",
    "  # The x.numpy() call below will ensure that all enqueued operations\n",
    "  # have completed (and will also copy the result to host memory,\n",
    "  # so we're including a little more than just the matmul operation\n",
    "  # time).\n",
    "  _ = x.numpy()\n",
    "  end = time.time()\n",
    "  return end - start\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 200\n",
    "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: {} secs\".format(measure(tf.random.normal(shape), steps)))\n",
    "\n",
    "# Run on GPU, if available:\n",
    "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: {} secs\".format(measure(tf.random.normal(shape), steps)))\n",
    "else:\n",
    "  print(\"GPU: not found\")\n",
    "Time to multiply a (1000, 1000) matrix by itself 200 times:\n",
    "CPU: 0.9788374900817871 secs\n",
    "GPU: 0.04241943359375 secs\n",
    "\n",
    "A tf.Tensor object can be copied to a different device to execute its operations:\n",
    "\n",
    "if tf.config.experimental.list_physical_devices(\"GPU\"):\n",
    "  x = tf.random.normal([10, 10])\n",
    "\n",
    "  x_gpu0 = x.gpu()\n",
    "  x_cpu = x.cpu()\n",
    "\n",
    "  _ = tf.matmul(x_cpu, x_cpu)    # Runs on CPU\n",
    "  _ = tf.matmul(x_gpu0, x_gpu0)  # Runs on GPU:0\n",
    "WARNING:tensorflow:From <ipython-input-1-876293b5769c>:4: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.identity instead.\n",
    "WARNING:tensorflow:From <ipython-input-1-876293b5769c>:5: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    "```\n",
    "Instructions for updating:\n",
    "Use tf.identity instead.\n",
    "\n",
    "Benchmarks\n",
    "For compute-heavy models, such as ResNet50 training on a GPU, eager execution performance is comparable to tf.function execution. But this gap grows larger for models with less computation and there is work to be done for optimizing hot code paths for models with lots of small operations.\n",
    "\n",
    "Work with functions\n",
    "While eager execution makes development and debugging more interactive, TensorFlow 1.x style graph execution has advantages for distributed training, performance optimizations, and production deployment. To bridge this gap, TensorFlow 2.0 introduces functions via the tf.function API. For more information, see the tf.function guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
