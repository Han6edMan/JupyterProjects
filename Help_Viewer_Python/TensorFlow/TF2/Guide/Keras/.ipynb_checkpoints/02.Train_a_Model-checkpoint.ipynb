{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic training loops\n",
    "训练神经网络的一般步骤为 (1)获取数据集，(2)搭建模型、定义损失函数和度量指标，(3)将数据送入网络、计算损失和度量，(4)根据损失反向传播计算梯度、更新参数；下面展示如何利用利用基础类通过简单的线性回归得到直线的斜率和截距：\n",
    "\n",
    "```python\n",
    "TRUE_W = 3.0\n",
    "TRUE_B = 2.0\n",
    "NUM_EXAMPLES = 1000\n",
    "x = tf.random.normal([NUM_EXAMPLES,])\n",
    "y = x * TRUE_W + TRUE_B + tf.random.normal([NUM_EXAMPLES,])\n",
    "\n",
    "class LinearRegresModel(tf.Module):\n",
    "    def __init__(self, init_fn, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = tf.Variable(init_fn([1,], \"float32\"))\n",
    "        self.b = tf.Variable(init_fn([1,], \"float32\"))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.w * x + self.b\n",
    "\n",
    "def loss_fn(y, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y - y_pred))\n",
    "\n",
    "def train(model, x, y, lr, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = loss_fn(y, model(x))\n",
    "        grad_w, grad_b = tape.gradient(loss, [model.w, model.b])\n",
    "        model.w.assign_sub(lr * grad_w)\n",
    "        model.b.assign_sub(lr * grad_b)\n",
    "        print(\"Epoch %2d/%d: W=%1.2f b=%1.2f, loss=%2.5f\" %\n",
    "          (epoch, epochs, model.w, model.b, loss))\n",
    "\n",
    "init_fn = tf.random_normal_initializer()\n",
    "model = LinearRegresModel(init_fn)\n",
    "train(model, x, y, lr=0.1, epochs=16)\n",
    "```\n",
    "\n",
    "利用 keras 实现该模型，并利用内置的`compile()`设置参数，利用`fit()`方法对网络进行训练：\n",
    "```python\n",
    "class KerasModel(tf.keras.Model):\n",
    "    def __init__(self, init_fn, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.w = tf.Variable(init_fn([1,], \"float32\"))\n",
    "        self.b = tf.Variable(init_fn([1,], \"float32\"))\n",
    "\n",
    "    def __call__(self, x, **kwargs):\n",
    "        return self.w * x + self.b\n",
    "\n",
    "init_fn = tf.random_normal_initializer()\n",
    "keras_model = KerasModel()\n",
    "keras_model.compile(\n",
    "    run_eagerly=False,\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.1),\n",
    "    loss=keras.losses.mean_squared_error,\n",
    ")\n",
    "keras_model.fit(x, y, epochs=16, batch_size=1000)\n",
    "```\n",
    "以上仅是一个极其简单的问题，有关更实用的介绍，请参见[自定义训练教程](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough)；更多关于 keras 内置训练函数的信息参加下面第 2 小节；如何人工实现训练循环请参见[相关指导](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)；有关自定义分布式训练的方法，请参见[相关指导](https://www.tensorflow.org/guide/guide/distributed_training#using_tfdistributestrategy_with_basic_training_loops_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using built-in methods\n",
    "\n",
    "本节不包含分布式训练，如需要了解分布式训练相关内容，请参阅[相关指导](https://www.tensorflow.org/guide/guide/distributed_training)；\n",
    "\n",
    "本节基于 MNIST 数据集演示利用 keras 内置函数实现模型的训练、验证和评估的过程，数据准备和模型定义如下所示：\n",
    "```python\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255.\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255.\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "def mnist_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"dense3\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def mnist_model_compiled():\n",
    "    model = mnist_model()\n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = mnist_model()\n",
    "model_compiled = mnist_model_compiled()\n",
    "```\n",
    "\n",
    "给定一个`keras.Modle`模型实例，可以利用内置的`.compile()`方法定义优化算法、损失函数、度量值，利用`.fit()`方法对模型进行训练，利用`.evaluate()`方法对模型进行评估 (即测试)；简单的示例如上面代码`mnist_model_compiled`函数所示；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 The `compile()` method\n",
    "`compile()`方法通过接收`optimizer`、`loss`、`metrics`参数，以将模型与优化器 (即优化算法)、损失函数、度量值进行包装；其中`optimizer`可以是表示优化器的字符串，或`keras.optimizers`模块中的优化器实例；`loss`可以是表示目标函数的字符串，或自定义的目标函数，或`.keras.losses.Loss`实例；`metrics`应为一个列表，其每个元素可以是代表表示度量的字符串，或自定义函数，或`keras.metrics.Metric`实例；\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.1 Custom losses\n",
    "实现自定义损失的方法有两种——为`loss`参数传递一个自定义函数，或传递一个继承了`keras.losses.Loss`类的实例\n",
    "\n",
    "\n",
    "\n",
    "- 自定义的损失函数应满足范式`fn(y_true, y_pred)`，示例如下：\n",
    "\n",
    "    ```python\n",
    "    def custom_mse(y_true, y_pred):\n",
    "        return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mse)\n",
    "    ```\n",
    "\n",
    "\n",
    "- 若想要定义一个接收`y_true`和`y_pred`之外的参数的损失函数，则需要继承`keras.losses.Loss`类，同时对其`__init__(self, ...)`方法和`call(self, y_true, y_pred)`方法进行实现；下面假设我们希望在 MSE 损失中添加一个损失项，该损失项会惩罚远离 0.5 的预测值，由于使用 MSE 时分类目标总是 one-hot 编码，于是不难看出，这个附加项会使得模型对结果预判不过于绝对，进而有助于减少过拟合，其实现方法如下：\n",
    "\n",
    "    ```python\n",
    "    class CustomMSE(keras.losses.Loss):\n",
    "        def __init__(self, reg_factor=0.1, name=\"custom_mse\"):\n",
    "            super().__init__(name=name)\n",
    "            self.reg_factor = reg_factor\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "            mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "            reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "            return mse + reg * self.reg_factor\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.2 Custom metrics\n",
    "自定义度量标准则需要继承`keras.metrics.Metric`类，同时实现以下方法\n",
    "- `__init__(self, custom_args)`：用于定义所需参数\n",
    "- `update_state(self, y_true, y_pred, sample_weight=None)`：利用`y_true`、`y_pred`对状态变量进行更新；\n",
    "- `result(self)`：利用状态变量得到最终度量结果；\n",
    "- `reset_states(self)`：重新初始化\n",
    "\n",
    "考虑到有时对度量结果的计算需要很大的计算量，并且只能定期进行，进而通常将对度量状态的更新和得到最终度量结果的方法分开；下面以“被正确分类的样本个数”为度量，演示添加自定义度量示例：\n",
    "\n",
    "```python\n",
    "class CateTruePositive(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positive\", **kwargs):\n",
    "        super(CateTruePositive, self).__init__(name=name, **kwargs)\n",
    "        self.true_positive = self.add_weight(\n",
    "            name=\"ctp\", initializer=\"zeros\"\n",
    "        )\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[CateTruePositive()],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.3 Handling those don't fit the standard signature\n",
    "对于无法利用`y_true`和`y_pred`计算的损失和度量，其可以在`call()`方法内调用`add_loss()`和`add_metric()`方法进行添加，这种方式添加的损失会在`fit()`阶段自动被添加至总损失中，模型也会自动对度量进行追踪；下面是添加对激活正则项的损失，以及添加激活标准差的度量的示例：\n",
    "\n",
    "```python\n",
    "class ActRegulAndMetric(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        self.add_metric(\n",
    "            keras.backend.std(inputs),\n",
    "            name=\"std_of_activation\",\n",
    "            aggregation=\"mean\"\n",
    "        )\n",
    "        return inputs  # a `pass` layer.\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "x = ActRegulAndMetric()(x)  # Insert loss and metrics logging\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)\n",
    "```\n",
    "\n",
    "此外，也可以通过这两个方法对模型的总损失和度量进行定义，进而在模型编码阶段无需再指明`loss`和`metrics`参数；例如下面的示例，其在`LogisticEndpoint`层中定义了交叉熵损失和准确率度量：\n",
    "\n",
    "```python\n",
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        self.add_metric(acc)\n",
    "        # Return the prediction for `.predict()`\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # No loss argument!\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.4 Compile multi-input, multi-output models\n",
    "本小节以\n",
    "对于有多个输出的模型，可以通过为`losses`传递一个由损失函数组成的列表，以为每个输出单独指定损失，该列表中损失函数排列顺序应与模型`compile`时传递的输出排列顺序相同；同样地也可以给`metrics`传递一个由度量组成的列表，以为每个输出单独指定损失，对于一个输出使用多个度量的情况，则可以使用嵌套列表`[[metric11, metric12], [metric21]]`；示例如下：\n",
    "\n",
    "```python\n",
    "\n",
    "若对输出指定了名称，则可以以字典的形式传递损失函数和度量\n",
    "\n",
    "Consider the following model, which has an image input of shape (32, 32, 3) (that's (height, width, channels)) and a timeseries input of shape (None, 10) (that's (timesteps, features)). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape (1,)) and a probability distribution over five classes (of shape (5,)).\n",
    "\n",
    "At compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:\n",
    "\n",
    "If we only passed a single loss function to the model, the same loss function would be applied to every output (which is not appropriate here).\n",
    "\n",
    "Likewise for metrics:\n",
    "\n",
    "```python\n",
    "img_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
    "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(img_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
    "class_output = layers.Dense(5, name=\"class_output\")(x)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[image_input, timeseries_input],\n",
    "    outputs=[score_output, class_output]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[\n",
    "        keras.losses.MeanSquaredError(),\n",
    "        keras.losses.CategoricalCrossentropy()\n",
    "    ],\n",
    "    metrics=[\n",
    "        [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        [keras.metrics.CategoricalAccuracy()],\n",
    "    ],\n",
    ")\n",
    "\n",
    "Since we gave names to our output layers, we could also specify per-output losses and metrics via a dict:\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    ")\n",
    "```\n",
    "We recommend the use of explicit names and dicts if you have more than 2 outputs.\n",
    "\n",
    "It's possible to give different weights to different output-specific losses (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), using the loss_weights argument:\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"score_output\": keras.losses.MeanSquaredError(),\n",
    "        \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "    },\n",
    "    metrics={\n",
    "        \"score_output\": [\n",
    "            keras.metrics.MeanAbsolutePercentageError(),\n",
    "            keras.metrics.MeanAbsoluteError(),\n",
    "        ],\n",
    "        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "    },\n",
    "    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
    ")\n",
    "You could also chose not to compute a loss for certain outputs, if these outputs meant for prediction but not for training:\n",
    "\n",
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n",
    ")\n",
    "Passing data to a multi-input or multi-output model in fit works in a similar way as specifying a loss function in compile: you can pass lists of NumPy arrays (with 1:1 mapping to the outputs that received a loss function) or dicts mapping output names to NumPy arrays.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    ")\n",
    "\n",
    "# Generate dummy NumPy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit(\n",
    "    {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "    {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    ")\n",
    "```\n",
    "\n",
    "Here's the Dataset use case: similarly as what we did for NumPy arrays, the Dataset should return a tuple of dicts.\n",
    "```python\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        {\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "        {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "    )\n",
    ")\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 The `fit()` method\n",
    "`fit()`, `evaluate()`, `predict()`方法均接收参数`x`、`y`作为训练、验证和预测的数据，其支持的格式包括 Numpy 数组、TF张量、字典、`tf.data.Dataset`实例、生成器或`keras.utils.Sequence`实例；此外`fit()`方法还接收参数`validation_split`、`validation_data`分别用于指明从训练集分割出作为验证集的比例、以及用于指明验证数据；需要说明的是，`validation_split`仅支持输入数据可被切片的类型；下面展示利用`validation_split`指明验证集的模型训练过程：\n",
    "```python\n",
    "model = mnist_model_compiled()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.1 Using `tf.data Dataset` as an input\n",
    "`tf.data`模块包含了 TF2 中用于加载和预处理数据的接口；有关其更详细的指导，参见[这里](https://www.tensorflow.org/guide/data)；\n",
    "\n",
    "由于`Dataset`产生的是输入-输出对，以内部属性指定 batch 大小，且不支持切片操作，进而当以`Dataset`实例作为输入传递给`x`时，无需指定参数`y`、`batch_size`参数，且不支持参数`validation_split`；此时若需要指定验证集，则应通过参数`validation_data`传递，示例如下：\n",
    "\n",
    "```python\n",
    "trainset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "trainset = trainset.shuffle(buffer_size=1024).batch(64)\n",
    "valset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(64)\n",
    "testset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n",
    "\n",
    "model = mnist_model_compiled()\n",
    "model.fit(trainset, epochs=3, validation_data=valset)\n",
    "result = model.evaluate(testset)\n",
    "```\n",
    "\n",
    "需要说明的是，`fit()`还提供了`initial_epoch`参数，其默认值为 0，进而上面的训练共进行了 3 个 epoch；一般情况下，训练在`range(initial_epoch, epochs)`内进行；此外，默认每个 epoch 训练执行的步数为总样本数与 batch 大小的商，结束后训练集的`Dataset`会重置；然而若指明了`steps_per_epoch`参数，训练集的`Dataset`在 epoch 结束后则不重置，且在下一个 epoch 中会从上次结束的位置继续向后迭代，生成数据；类似地，`validation_steps`则用于指定每次验证执行的步数，不同的是，每次验证时都会先将验证集`Dataset`重置，以确保每次验证使用了相同的数据；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.2 Using `keras.utils.Sequece` as an input\n",
    "通过继承`keras.utils.Sequence`得到的生成器对象不仅支持多进程处理，还支持`fit()`方法的`shuffle`参数；继承`keras.utils.Sequence`生成器时必须实现其`__getitem__()`方法和`__len__()`方法，其中调用前者时应返回一个 batch 所含的数据，调用后者返回总 batch 个数；如果需要在 epoch 结束时对数据集进行调整，则应将`on_epoch_end`进行实现；\n",
    "\n",
    "```python\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "class CIFAR10Sequence(Sequence):\n",
    "    def __init__(self, files, labels, batch_size):\n",
    "        self.files, self.labels = files, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.files) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.files[\n",
    "            idx * self.batch_size: (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        batch_y = self.labels[\n",
    "            idx * self.batch_size: (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        return np.array([\n",
    "            resize(imread(file), (200, 200)) for file in batch_x\n",
    "        ]), np.array(batch_y)\n",
    "\n",
    "sequence = CIFAR10Sequence(files, labels, batch_size)\n",
    "model.fit(sequence, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.3 Other input formats supported\n",
    "Besides NumPy arrays, eager tensors, and TensorFlow Datasets, it's possible to train a Keras model using Pandas dataframes, or from Python generators that yield batches of data & labels.\n",
    "\n",
    "In particular, the keras.utils.Sequence class offers a simple interface to build Python data generators that are multiprocessing-aware and can be shuffled.\n",
    "\n",
    "In general, we recommend that you use:\n",
    "\n",
    "NumPy input data if your data is small and fits in memory\n",
    "Dataset objects if you have large datasets and you need to do distributed training\n",
    "Sequence objects if you have large datasets and you need to do a lot of custom Python-side processing that cannot be done in TensorFlow (e.g. if you rely on external libraries for data loading or preprocessing).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.4 Using sample weighting and class weighting\n",
    "默认情况下，样本权重由其在数据集中的出现频率决定；除此之外也可以通过`fit()`方法的`class_weight`参数和`sample_weight`方法指定样本权重；其中`class_weight`是将类别名称映射至相应权重的字典，进而可以弥补类别的不平衡问题；`sample_weight`适用于输入为 Numpy 数组的情况，其自身也是形状与输入数据相同的 Numpy 数组，其常用于处理类别不平衡问题，当`sample_weight`为一连串由 0、1 组成的数组时，可以视为损失的掩码使用；需要注意的是，对于输入为`tf.data.Dataset`，其不支持`sample_weight`，然而可以通过将`(input_batch, label_batch, sample_weight_batch)`元祖传递给`Dataset`来实现对样本的加权；\n",
    "\n",
    "下面是利用`class_weight`和`sample_weight`对 MNIST 数据集中类别属于 5 的数据进行加权的示例：\n",
    "\n",
    "```python\n",
    "model = mnist_model_compiled()\n",
    "\n",
    "# specify `class_weight`\n",
    "class_weight = {\n",
    "    0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0,\n",
    "    5: 2.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0,\n",
    "}\n",
    "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64)\n",
    "\n",
    "# specify `sample_weight` with Numpy array inputs\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64)\n",
    "\n",
    "# specify `sample_weight` with `Dataset` inputs\n",
    "trainset = Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
    "trainset = trainset.shuffle(buffer_size=1024).batch(64)\n",
    "model.fit(train_dataset, epochs=1)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.5 Using callbacks\n",
    "Keras 中的回调 (callback) 指在训练过程中不同时刻 (例如在 epoch 开始时时、一个 step 结束时、一个 epoch 结束时等) 调用的对象，它可以用于实现诸如\n",
    "- 在训练的不同时刻进行验证，这里的验证与内置的验证并非同一个验证；\n",
    "- 在模型执行一定步数后或当其精度超过一定阈值后对模型记录检查点；\n",
    "- 在训练趋近于饱和时改变学习率或对顶层网络进行微调；\n",
    "- 训练结束或模型超过某个性能阈值时发送电子邮件或消息通知\n",
    "- Etc.\n",
    "\n",
    "`keras.callbacks`模块提供了很多实现回调的 API，例如`CSVLogger`、`EarlyStopping`、`TensorBoard`、`ModelCheckpoint`等；实现时只需将需要进行的回调对象以列表的形式传递给`fit()`方法的`callbacks`参数即可；\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.5.1 Checkpointing models\n",
    "\n",
    "训练中对模型保存检查点的最简单方法是使用`ModelCheckpoint`回调；下面是利用`ModelCheckpoint`回调实现代码容错的示例，即在训练被随机中断后，可以从最近保存的模型状态继续训练；\n",
    "\n",
    "```python\n",
    "ckpt_dir = \"./ckpt\"  # dir to store all the checkpoints.\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "def make_or_restore_model():\n",
    "    ckpts = [ckpt_dir + \"/\" + name for name in os.listdir(ckpt_dir)]\n",
    "    if ckpts:\n",
    "        latest_ckpt = max(ckpts, key=os.path.getctime)\n",
    "        print(\"Restoring from\", latest_ckpt)\n",
    "        return keras.models.load_model(latest_ckpt)\n",
    "    print(\"Creating a new model\")\n",
    "    return mnist_model_compiled()\n",
    "\n",
    "model = make_or_restore_model()\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_dir + \"/ckpt-loss={loss:.2f}\", save_freq=100\n",
    ")]\n",
    "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)\n",
    "```\n",
    "当然也可以自定义回调函数来保存和恢复模型；关于模型序列化和保存的方法，请参阅[相关指导](https://www.tensorflow.org/guide/keras/save_and_serialize/)\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.5.2 Using learning rate schedules\n",
    "\n",
    "学习率静态衰减中，学习率通常是当前 epoch 数值或 batch 索引的一个函数；学习率动态衰减中，学习率是根据模型当前的行为来动态调整的，例如在验证损失不再提高时减少学习率；\n",
    "\n",
    "实现学习率静态衰减的方式之一便是利用`keras.optimizers.schedules`模块内置的静态学习率衰减 API，如`ExponentialDecay`、`PiecewiseConstantDecay`、`PolynomialDecay`、`InverseTimeDecay`；使用时只需将其实例化对象传递给优化器的`learning_rate`参数即可\n",
    "\n",
    "```python\n",
    "init_lr = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    init_lr, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "```\n",
    "\n",
    "然而由于优化器无法访问验证指标，进而`keras.optimizers.schedules`模块中的 API 无法实现学习率的动态衰减；不过`keras.callbacks`模块中的 API 均可以对验证指标进行访问，例如内置的`ReduceLROnPlateau`类便可实现；\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.5.3 Visualizing loss and metrics during training\n",
    "\n",
    "TensorBoard 可以实时地绘制训练和验证的损失和度量图、对层模型的激活进行可视化、对嵌入层学习到的嵌入空间进行 3 维可视化；使用`TensorBoard`回调可以对这些概念进行快速实现，示例如下；更多信息请参阅`TensorBoard`回调的[相关文档](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/tensorboard/)\n",
    "\n",
    "```python\n",
    "keras.callbacks.TensorBoard(\n",
    "    log_dir=\"/full_path_to_your_logs\",\n",
    "    histogram_freq=0,\n",
    "    embeddings_freq=0,\n",
    "    update_freq=\"epoch\",\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2.2.5.4 Writing your own callback\n",
    "\n",
    "可以通过继承`keras.callbacks.Callback`类来创建自定义回调，回调可以通过类属性`.model`访问其关联的模型；更多信息请参见[相关指导](https://www.tensorflow.org/guide/keras/custom_callback/)；下面是在训练期间将每个 batch 的损失以列表形式保存的简单示例：\n",
    "\n",
    "```python\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             multiple                  50240     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             multiple                  650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense3 = layers.Dense(10, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, x, **kwargs):\n",
    "        y = self.dense3(self.dense2(self.dense1(x)))\n",
    "        return y\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            (x, y), sample_weight = data, None\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.trainable_variables)\n",
    "        )\n",
    "        loss_tracker.update_state(loss)\n",
    "        mae_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker, mae_metric]\n",
    "\n",
    "\n",
    "model = CustomModel()\n",
    "model.compile(optimizer=\"adam\")\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Tensor(\"IteratorGetNext:2\", shape=(None, 1), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:2\", shape=(None, 1), dtype=float32)\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2469 - mae: 0.4152\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2467 - mae: 0.4150\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2467 - mae: 0.4150\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2467 - mae: 0.4150\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2467 - mae: 0.4150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23f6b8a3310>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1000, 784))\n",
    "y = np.random.random((1000, 1))\n",
    "weight = np.random.random((1000, 1))\n",
    "model.fit(x, y, sample_weight=weight, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Customize what happens in `Model.fit`\n",
    "\n",
    "本节介绍的方法对`Sequetial`模型、functional 模型、类继承模型均适用；以下的操作需要 TF2.2 及以上版本；\n",
    "\n",
    "## Customize training\n",
    "通常情况下，调用`compile()`和`fit()`方法便可满足大部分训练要求；若需要自定义训练过程，一种方式是利用`GradientTape`完全手动实现；然而若仍希望利用`fit()`函数的便捷性，可以通过重写`Model`类`train_step()`方法来实现；具体而言，首先创建一个继承`keras.Model`的类，同时对其`train_step(self, data)`方法进行重写，其中参数`data`指被传递至`fit()`方法的训练数据组成的二元元祖，对于指明了`sample_weight`参数的情况，`data`则为包含了相应的权重的三元元祖；需要说明的是，尽管`fit()`可以接收多种格式的训练数据，但当其内部调用`train_step()`方法时，总是将输入与输出以及样本权重组成一个元祖传递给`data`参数，进而这里只需将参数设置为`data`即可；\n",
    "\n",
    "`train_step()`方法内部则需要实现模型参数更新；这里使用`self.compiled_loss()`计算损失，使用`self.compiled_metrics.update_state()`计算度量，前提是损失函数和度量已提前通过调用`compile()`方法封装至模型中；当然也可以通过传递`keras.losses`模块和`keras.metrics`模块中的 API 来手动计算损失和度量，后者的实现方法参见第二个`CustomModel`示例\n",
    "\n",
    "最后通过访问`self.metrics`返回一个将度量名称映射至度量值的字典，这里度量包括了损失；\n",
    "\n",
    "```python\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            (x, y), sample_weight = data, None\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = self.compiled_loss(\n",
    "                y, y_pred,\n",
    "                sample_weight=sample_weight,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.trainable_variables)\n",
    "        )\n",
    "        self.compiled_metrics.update_state(\n",
    "            y, y_pred,\n",
    "            sample_weight=sample_weight\n",
    "        )\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\", name=\"dense3\")(x)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "x = np.random.random((1000, 784))\n",
    "y = np.random.random((1000, 10))\n",
    "weight = np.random.random((1000, 1))\n",
    "model.fit(x, y, sample_weight=weight, epochs=5)\n",
    "```\n",
    "\n",
    "对于自定义损失函数和度量的情况，便无需在`compile()`函数中再制定损失和度量，同时需要在一个训练步结束后调用`metric.update_state()`来更新度量值；需要注意的是，实际上所有度量在一个 epoch 训练结束后应该进行重置，一种方式是手动调用`metric.reset_states()`方法，另一种方式是将所有度量以列表的形式重写至`metrics`属性，进而`fit()`函数会在每个 epoch 一开始时对`metrics`属性中所有度量进行重置；示例如下，这里演示了使用类继承实现的方式：\n",
    "\n",
    "```python\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2 = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense3 = layers.Dense(10, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, x, **kwargs):\n",
    "        y = self.dense3(self.dense2(self.dense1(x)))\n",
    "        return y\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            x, y, sample_weight = data\n",
    "        else:\n",
    "            (x, y), sample_weight = data, None\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)\n",
    "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.trainable_variables)\n",
    "        )\n",
    "        loss_tracker.update_state(loss)\n",
    "        mae_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker, mae_metric]\n",
    "\n",
    "model = CustomModel()\n",
    "model.compile(optimizer=\"adam\")\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "weight = np.random.random((1000, 1))\n",
    "model.fit(x, y, sample_weight=sw, epochs=5)\n",
    "```\n",
    "\n",
    "\n",
    "## Customize evaluation\n",
    "如果需要对`model.evaluate()`进行自定义，可以通过重写`test_step()`方法实现：\n",
    "\n",
    "```python\n",
    "class CustomModel(keras.Model):\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "inputs = keras.Input(shape=(32,))\n",
    "outputs = keras.layers.Dense(1)(inputs)\n",
    "model = CustomModel(inputs, outputs)\n",
    "model.compile(loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "x = np.random.random((1000, 32))\n",
    "y = np.random.random((1000, 1))\n",
    "model.evaluate(x, y)\n",
    "```\n",
    "\n",
    "## Wrapping up: an end-to-end GAN example\n",
    "\n",
    "下面以一个在 MNIST 数据集上训练的 GAN 为例，演示自定义`fit()`函数的训练环节的过程；训练循环包括 2 个分支：\n",
    "\n",
    "- 对判别器的训练：在隐空间中随机抽取一个 batch 的点 $\\rightarrow$ 利用生成器将这些点变成假图像 $\\rightarrow$ 获取一批真实图像，并与生成的图像进行组合 $\\rightarrow$ 训练判别器以使其能够正确的对生成的图像和真实图像进行分类；\n",
    "\n",
    "- 对生成器的训练：在隐空间中随机抽取一个 batch 的点 $\\rightarrow$ 利用生成器将这些点变成假图像 $\\rightarrow$ 获取一批真实图像，并与生成的图像进行组合 $\\rightarrow$ 训练生成器，以使其生成的图片被判别器误以为是真的图片\n",
    "\n",
    "实现如下：\n",
    "\n",
    "```python\n",
    "def get_discriminator_generator(latent_dim=128):\n",
    "    discriminator = keras.Sequential([\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "        ], name=\"discriminator\",\n",
    "    )\n",
    "    generator = keras.Sequential([\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(7 * 7 * 128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "        ], name=\"generator\",\n",
    "    )\n",
    "    return discriminator, generator\n",
    "\n",
    "\n",
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim=128):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_imgs):\n",
    "        if isinstance(real_imgs, tuple):\n",
    "            real_imgs = real_imgs[0]\n",
    "        batch_size = tf.shape(real_imgs)[0]\n",
    "        # ==> Train the discriminator\n",
    "        latent_vecs = tf.random.normal([batch_size, self.latent_dim])\n",
    "        fake_imgs = self.generator(latent_vecs)\n",
    "        combined_imgs = tf.concat([fake_imgs, real_imgs], axis=0)\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n",
    "            axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.discriminator(combined_imgs)\n",
    "            d_loss = self.loss_fn(labels, preds)\n",
    "        grads = tape.gradient(\n",
    "            d_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "        # ==> Train the generator\n",
    "        latent_vecs = tf.random.normal([batch_size, self.latent_dim])\n",
    "        fake_labels = tf.zeros((batch_size, 1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.discriminator(self.generator(latent_vecs))\n",
    "            g_loss = self.loss_fn(fake_labels, preds)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(grads, self.generator.trainable_weights)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "\n",
    "def load_data(batch_size=64):\n",
    "    (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "    digits = np.concatenate([x_train, x_test])\n",
    "    digits = digits.astype(\"float32\") / 255.0\n",
    "    digits = np.reshape(digits, (-1, 28, 28, 1))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(digits)\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data(batch_size=64)\n",
    "discriminator, generator = get_discriminator_generator(latent_dim=128)\n",
    "gan = GAN(discriminator, generator, latent_dim=128)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "gan.fit(dataset.take(100), epochs=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing a training loop from scratch\n",
    "\n",
    "利用 keras 的内置函数`fit()`和`evaluate()`可以实现快速的训练和评估，相关指导可参见第 2 节；通过继承`Model`类并实现`train_step()`方法，可以在自定义模型的学习算法同时，依旧利用`fit()`函数进行训练，相关指导可参见第 3 节；下面仍以基于 MNIST 数据集的一个简单的模型为示例，介绍如何对训练和评估进行底层实现，即从头编写训练的循环代码；\n",
    "\n",
    "下面是一个简单的示例：\n",
    "\n",
    "```python\n",
    "def mnist_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"dense3\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def load_dataset(batch_size):\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "    x_train = np.reshape(x_train, (-1, 784))\n",
    "    x_test = np.reshape(x_test, (-1, 784))\n",
    "    x_val = x_train[-10000:]\n",
    "    y_val = y_train[-10000:]\n",
    "    x_train = x_train[:-10000]\n",
    "    y_train = y_train[:-10000]\n",
    "    trainset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    trainset = trainset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    valset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    valset = valset.batch(batch_size)\n",
    "    return trainset, valset\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\n==> Epoch %d:\" % (epoch,))\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(trainset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch_train, training=True)\n",
    "                loss = loss_fn(y_batch_train, y_pred)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(\n",
    "                grads, model.trainable_weights\n",
    "            ))\n",
    "            if step % 200 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss))\n",
    "                )\n",
    "\n",
    "trainset, valset = load_dataset(64)\n",
    "model = mnist_model()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train(model, trainset, epochs=2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4.1 Low-level handling of metrics & losses\n",
    "\n",
    "若需要在循环中对度量进行追踪，考虑到度量值是对整个数据集而言的，进而需要**在每个 batch 训练完成后调用`metric.update_state()`以更新度量值**，以及在每个 epoch 训练结束后，调用`metric.reset_states()`对其清零；此外可以利用`metric.result()`获取训练期间度量值以对模型性能进行监测；\n",
    "\n",
    "在前向传播的过程中，层和模型对象会递归地调用网络层的`self.add_loss(value)`方法，来对期间产生的所有损失进行追踪，所有通过`.add_loss()`方法添加损失所组成的列表可以通过`.losses`属性获得；进而在人工实现训练循环时，只需在得到损失函数返回的损失值后，将其与模型向前传播期间得到的损失值相加即可；\n",
    "\n",
    "对上述模型记录`SparseCategoricalAccuracy`度量，并添加激活正则项的方式如下：\n",
    "\n",
    "```python\n",
    "def mnist_model_act_reg():\n",
    "    class ActivityRegularizationLayer(layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "            return inputs\n",
    "\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    x = ActivityRegularizationLayer()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\n==> Epoch %d:\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        for step, (x_train, y_train) in enumerate(trainset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_train, training=True)\n",
    "                loss = loss_fn(y_train, y_pred)\n",
    "                loss += sum(model.losses)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(\n",
    "                grads, model.trainable_weights\n",
    "            ))\n",
    "            train_acc_metric.update_state(y_train, y_pred)\n",
    "            if step % 200 == 0:\n",
    "                print(\"step: %3d, loss: %.4f\" % (step, loss))\n",
    "        train_acc = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        for x_val, y_val in valset:\n",
    "            y_pred = model(x_val, training=False)\n",
    "            val_acc_metric.update_state(y_val, y_pred)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "trainset, valset = load_dataset(batch_size=64)\n",
    "model = mnist_model_act_reg()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train(epochs=2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4.2 Speeding-up training with `tf.function`\n",
    "TF2 默认的运行时 (runtime) 是即时执行模型，尽管这对调试很友好，然而利用计算图编译具有明显的性能优势 —— 如果将整个模型计算描述为静态图，则可以使计算框架对全局性能进行优化；而如果计算框架不得不贪婪地执行操作，而无法获知后面的计算，显然无法对整体进行很好的加速；\n",
    "\n",
    "TF2 中，通过添加`@tf.function`修饰符，可以将任何一个以张量为输入的函数编译至静态图形中，进而实现加速，以上面的添加了激活正则项的 MNIST 模型为例，演示如下：\n",
    "\n",
    "```python\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, y_pred)\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"==> Epoch: %d\" % epoch)\n",
    "        start_time = time.time()\n",
    "        for step, (x, y) in enumerate(trainset):\n",
    "            loss = train_step(x, y)\n",
    "            loss += sum(model.losses)\n",
    "            if step % 200 == 0:\n",
    "                print(\"step: %3d, loss: %.4f\" % (step, loss))\n",
    "        train_acc = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        print(\"Training acc over the epoch: %.4f\" % train_acc)\n",
    "        \n",
    "        for x, y in valset:\n",
    "            test_step(x, y)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % val_acc)\n",
    "        print(\"Time taken: %.2f\" % (time.time() - start_time))\n",
    "\n",
    "model = mnist_model_act_reg()\n",
    "optimizer = keras.optimizers.SGD(1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "train(epochs=2)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4.3 An end-to-end Example\n",
    "\n",
    "下面以一个在 MNIST 数据集上训练的 GAN 为例，演示人工从头实现训练循环的过程；训练循环包括 2 个分支：\n",
    "\n",
    "- 对判别器的训练：在隐空间中随机抽取一个 batch 的点 $\\rightarrow$ 利用生成器将这些点变成假图像 $\\rightarrow$ 获取一批真实图像，并与生成的图像进行组合 $\\rightarrow$ 训练判别器以使其能够正确的对生成的图像和真实图像进行分类；\n",
    "\n",
    "- 对生成器的训练：在隐空间中随机抽取一个 batch 的点 $\\rightarrow$ 利用生成器将这些点变成假图像 $\\rightarrow$ 获取一批真实图像，并与生成的图像进行组合 $\\rightarrow$ 训练生成器，以使其生成的图片被判别器误以为是真的图片\n",
    "\n",
    "实现如下：\n",
    "\n",
    "```python\n",
    "def load_dataset(batch_size=64):\n",
    "    (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "    digits = np.concatenate([x_train, x_test]).astype(\"float32\") / 255.0\n",
    "    digits = np.reshape(digits, (-1, 28, 28, 1))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(digits)\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def GAN_model(latent_dim=128):\n",
    "    discriminator = keras.Sequential([\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "        ], name=\"discriminator\",\n",
    "    )\n",
    "    generator = keras.Sequential([\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(7 * 7 * 128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "        ], name=\"generator\",\n",
    "    )\n",
    "    return discriminator, generator\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_imgs, latent_dim=128, batch_size=64):\n",
    "    # ==> Train the discriminator\n",
    "    latent_vecs = tf.random.normal([batch_size, latent_dim])\n",
    "    fake_imgs = generator(latent_vecs)\n",
    "    combined_imgs = tf.concat([fake_imgs, real_imgs], axis=0)\n",
    "    labels = tf.concat(\n",
    "        [tf.ones((batch_size, 1)), tf.zeros((real_imgs.shape[0], 1))],\n",
    "        axis=0\n",
    "    )\n",
    "    # Add random noise to labels -- important trick!\n",
    "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = discriminator(combined_imgs)\n",
    "        d_loss = loss_fn(labels, preds)\n",
    "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "    d_optimizer.apply_gradients(\n",
    "        zip(grads, discriminator.trainable_weights)\n",
    "    )\n",
    "    # ==> Train the generator\n",
    "    latent_vecs = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "    fake_labels = tf.zeros((batch_size, 1))\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = discriminator(generator(latent_vecs))\n",
    "        g_loss = loss_fn(fake_labels, preds)\n",
    "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "    return d_loss, g_loss, fake_imgs\n",
    "\n",
    "def train(epochs=16, save_dir=None):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"==> Epoch:\", epoch)\n",
    "        for step, real_imgs in enumerate(dataset):\n",
    "            d_loss, g_loss, fake_imgs = train_step(real_imgs)\n",
    "            if step % 200 == 0:\n",
    "                print(\n",
    "                    \"step: {},\"\n",
    "                    \"discriminator loss: {},\"\n",
    "                    \"generator loss: {}\".format(step, d_loss, g_loss)\n",
    "                )\n",
    "                if save_dir:\n",
    "                    img = tf.keras.preprocessing.image.array_to_img(\n",
    "                        fake_imgs[0] * 255.0, scale=False\n",
    "                    )\n",
    "                    img.save(os.path.join(\n",
    "                        save_dir, \"generated_img\" + str(step) + \".png\"\n",
    "                    ))\n",
    "\n",
    "\n",
    "dataset = load_dataset()\n",
    "discriminator, generator = GAN_model()\n",
    "d_optimizer = keras.optimizers.Adam(learning_rate=0.0003)\n",
    "g_optimizer = keras.optimizers.Adam(learning_rate=0.0004)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "train(epochs=2, \"../../test/Guide/Keras/02.Train_a_Model/GAN_example\")\n",
    "```\n",
    "注意，由于涉及到卷积运算，建议在 GPU 上运行；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
