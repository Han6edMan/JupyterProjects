{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `Sequential` Model\n",
    "\n",
    "## 1.1 Create a `Sequential` model\n",
    "通过将一个网络层组成的列表传递给`Sequential`构造函数中的`layers`参数，可以创造一个`Sequential`模型；或通过`add()`方法将各层添加至`Sequential`对象的`.layers`属性中，进而建立模型；此外，通过`pop()`方法可以将模型最后一层删去\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "])\n",
    "# is equivalent to\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "len(model.layers)  # ==> 3\n",
    "\n",
    "model.pop()\n",
    "len(model.layers)  # ==> 2\n",
    "```\n",
    "可以看出，`Sequential`模型适用于模块的简单叠加，其中每个模块只有一个输入和一个输出；而不适用于有多个输入或输出、需要模块间共享权重、或搭建非线性结构模型的情况；\n",
    "\n",
    "此外，`Sequential`构造函数同样地接受`name`参数，对于在 TBoard 中标注语义信息很有帮助\n",
    "\n",
    "\n",
    "## 1.2 Specifying the input shape\n",
    "作为`Sequential`模型的基本单元，Keras 网络层都需要知道它们输入的形状，以便创建相应的权重；然而大部分 Keras 构造函数并未将输入形状作为未知参数，进而当以`l1 = layers.Dense(3)`的方式创建网络层后，其权重并未被初始化，即`.weights`属性返回的是空列表；例如上面的示例中\n",
    "```python\n",
    "for l in seq_model.layers:\n",
    "    assert len(l.weights) == 0\n",
    "```\n",
    "\n",
    "注意，此时调用模型的`.weights`属性会报错；网络层模型获取其权重的常见方式有 5 种：\n",
    "\n",
    "1. 当一个网络层被添加至另一个网络层后面时，其会自动根据上一层网络的输出形状自动推断出本层的输入形状 (注意网络的输出形状总是位置函数)\n",
    "\n",
    "2. 上述情况中，可能网络最前端的层模型仍无法获得其输入形状，此时可以利用`Input`对象指明，例如\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(64,)))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "len(model.weights)  # ==> 2\n",
    "```\n",
    "\n",
    "3. 或在整个模型搭建完成后，通过将一个张量作为输入送入网络，或调用`fit`等训练或验证的类方法，可以等效地指明了网络前端的输入形状；\n",
    "\n",
    "4. 也可以在构建第一层时将`input_shape`或`input_dim`或`batch_input_shape`传递给`**kwargs`参数；需要注意的是，若在中间层指明输入形状时，该形状应与上一层输出形状匹配；示例如下\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(16, input_shape=(4,)))\n",
    "model.add(layers.Dense(32, input_dim=(16,)))\n",
    "model.add(layers.Dense(10, batch_input_shape=(None, 32)))\n",
    "len(model.weights)  # ==> 6\n",
    "```\n",
    "\n",
    "5. 调用`.build`方法初始化权重\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(16))\n",
    "model.build((None, 16))\n",
    "len(model.weights)  # ==> 4\n",
    "```\n",
    "需要注意的是，在网络权重初始化之前，调用`.summary()`方法会报错\n",
    "\n",
    "## 1.3 Transfer Learning with a Sequential model\n",
    "迁移学习其中一步便是冻结底层模型而只训练顶层模型，更多有关迁移学习的内容参见[相关指导](https://www.tensorflow.org/guide/keras/transfer_learning/)；下面介绍 2 种利用模型进行迁移学习的方式；\n",
    "\n",
    "1. 逐层指定`Sequential`模型中的网络层是否可训练，例如下面的例子中仅指定最后 2 层可训练：\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(784))\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10),\n",
    "])\n",
    "model.load_weights(...)\n",
    "\n",
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(...)\n",
    "model.fit(...)\n",
    "```\n",
    "\n",
    "2. 将预训练模型与一些模块通过`Sequential`构建为一个整体模型，例如\n",
    "\n",
    "```python\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    include_top=False\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Dense(1000),\n",
    "])\n",
    "model.compile(...)\n",
    "model.fit(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functional API\n",
    "\n",
    "相较于`Sequential`模型而言，Keras 的 functional API 可以搭建具有更加复杂网络拓扑的模型，具体而言，模型可以是非线性模型，模型间可以共享权重，并且可以有多个输入与输出；DL 中很大一部分模型是不同模块间的有向无环图 (Directed Acyclic Graph, DAG)，而 functional API 便是构建 DAG 的一种方法；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Manipulate complex graph topologies\n",
    "\n",
    "### 2.1.1 Multiple inputs and outputs\n",
    "\n",
    "利用 functional API 可以很容易处理多个输入和输出的情况；下面以一个模型为例演示多输入输出的实现方法，该模型能够按照优先级对顾客的问题票单进行排序，并将票单分派给指定部门；这个模型的输入包括票单标题、正文、用户特征，输出包括位于 (0, 1) 区间的优先级等数、被指派的部门；实现过程如下\n",
    "\n",
    "```python\n",
    "def rank_route_model(num_tags, size_voc, num_dep):\n",
    "    title = keras.Input(shape=(None,), name=\"title\")\n",
    "    body = keras.Input(shape=(None,), name=\"body\")\n",
    "    tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "    # Embedding\n",
    "    title_feat = layers.Embedding(num_words, 64)(title)  # 64-D vector\n",
    "    body_feat = layers.Embedding(num_words, 64)(body)\n",
    "    title_feat = layers.LSTM(128)(title_feat)  # 128-D vector\n",
    "    body_feat = layers.LSTM(32)(body_feat)\n",
    "    # merge and predict\n",
    "    feat = layers.concatenate([title_feat, body_feat, tags])\n",
    "    prior_pred = layers.Dense(1, name=\"priority\")(feat)\n",
    "    dep_pred = layers.Dense(num_dep, name=\"department\")(feat)\n",
    "    model = keras.Model(\n",
    "        inputs=[title, body, tags],\n",
    "        outputs=[prior_pred, dep_pred],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = rank_route_model(12, 10000, 4)\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    \"../../test/Guide/Keras/01.Create_a_Model/multi_in_out.png\",\n",
    "    show_shapes=True,\n",
    "    dpi=512\n",
    ")\n",
    "```\n",
    "\n",
    "<img src=\"../../test/Guide/Keras/01.Create_a_Model/multi_in_out.png\" width=720>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在对模型进行编译时，可以为每个输出分配不同类型的损失，以及为每个损失分配不同的权重；在输出层指明了名称的情况下，可以以字典而非列表的形式将损失类型传递给`loss`参数，`fit()`方法也是如此；\n",
    "\n",
    "```python\n",
    "# Dummy data\n",
    "title_data = np.random.randint(num_words, size=(1280, 10))\n",
    "body_data = np.random.randint(num_words, size=(1280, 100))\n",
    "tags_data = np.random.randint(2, size=(1280, 12)).astype(\"float32\")\n",
    "priority_targets = np.random.random(size=(1280, 1))\n",
    "dept_targets = np.random.randint(2, size=(1280, 4))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"priority\": BinaryCrossentropy(from_logits=True),\n",
    "        \"department\": CategoricalCrossentropy(from_logits=True),\n",
    "    },\n",
    "    loss_weights=[1.0, 0.2],\n",
    ")\n",
    "model.fit(\n",
    "    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_targets, \"department\": dept_targets},\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    ")\n",
    "```\n",
    "更多与训练有关的细节参见[相关指导](https://www.tensorflow.org/guide/keras/train_and_evaluate/)\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.2 Non-linear connectivity topologies\n",
    "最常见的非线性拓扑便是残差结构，可参见下面示例中的通过 functional API 所实现 ResNet 模型的残差单元：\n",
    "\n",
    "```python\n",
    "def res_unit(in_shape, num_filter):\n",
    "    stride = int(num_filter / in_shape[-1])\n",
    "    shortcut = inputs = keras.Input(shape=in_shape)\n",
    "    x = layers.Conv2D(\n",
    "        num_filter, (3, 3), strides=stride,\n",
    "        padding=\"same\", use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Conv2D(\n",
    "        num_filter, (3, 3), strides=1,\n",
    "        padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if stride != 1:\n",
    "        shortcut = layers.Conv2D(\n",
    "            num_filter, (1, 1), strides=stride,\n",
    "            padding='same', use_bias=False)(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    outputs = layers.Activation(\"relu\")(x + shortcut)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "def res_block(unit, num_unit, in_shape, num_filter):\n",
    "    x = inputs = keras.Input(shape=in_shape)\n",
    "    for _ in range(num_unit):\n",
    "        x = unit(x.shape[-3:], num_filter)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "def resnet(unit, num_blocks, in_shape=(32, 32, 3), num_classes=10):\n",
    "    inputs = keras.Input(shape=in_shape)\n",
    "    x = layers.Conv2D(\n",
    "        16, (3, 3), strides=1,\n",
    "        padding=\"same\", use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = res_block(unit, num_blocks[0], x.shape[-3:], 16)(x)\n",
    "    x = res_block(unit, num_blocks[1], x.shape[-3:], 32)(x)\n",
    "    x = res_block(unit, num_blocks[2], x.shape[-3:], 64)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    outputs = layers.Dense(num_classes, use_bias=False)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "def resnet20():\n",
    "    return resnet(res_unit, [3, 3, 3])\n",
    "\n",
    "model = resnet20()\n",
    "res_block = model.get_layer(index=5)\n",
    "res_unit = res_block.get_layer(index=1)\n",
    "keras.utils.plot_model(\n",
    "    model=res_unit,\n",
    "    \"../../test/Guide/Keras/01.Create_a_Model/functional_api_resnet20_resUnit.png\",\n",
    "    show_shapes=True\n",
    ")\n",
    "```\n",
    "\n",
    "<img src=\"../../test/Guide/Keras/01.Create_a_Model/functional_api_resnet20_resUnit.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2.1.3 Shared layers\n",
    "\n",
    "通过 functional API 可以很容易实现共享网络层；共享网络层指在同一模型中重复使用某些层的实例，这些层通常用于编码相似空间的输入，例如具有相似词汇表的两段不同文本；通过在不同的输入之间共享信息，可以减少训练模型所需的数据；\n",
    "\n",
    "要通过 functional API 共享网络层，只需多次调用同一个层模型的实例，例如下面的示例\n",
    "\n",
    "```python\n",
    "# Embedding for 1000 unique words mapped to 128-dimensional vectors\n",
    "shared_embedding = layers.Embedding(1000, 128)\n",
    "text_input_a = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "text_input_b = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Reuse to encode both inputs\n",
    "encoded_input_a = shared_embedding(text_input_a)\n",
    "encoded_input_b = shared_embedding(text_input_b)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Extract and reuse nodes in the graph of layers\n",
    "由网络层构成的图是静态数据结构，进而可以对其进行访问及检查，例如通过访问将模型绘制成计算图，或者将中间层的输出用于其他任务上；参见下面提取 VGG-19 不同层的输出特征的模型的示例：\n",
    "\n",
    "```python\n",
    "vgg19 = tf.keras.applications.VGG19()\n",
    "features_list = [layer.output for layer in vgg19.layers]\n",
    "feat_extractor = keras.Model(inputs=vgg19.input, outputs=features_list)\n",
    "img = np.random.random((1, 224, 224, 3)).astype(\"float32\")\n",
    "extracted_features = feat_extraction_model(img)\n",
    "```\n",
    "\n",
    "该技术常用于风格转换等任务中；\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.3 Customize a functional API\n",
    "自定义 Functional API 的详细方式可参见下面类继承的 [3.1 小节](#3.1-Subclassing-Layer-class)，这里仅不加说明的提供一个自定义全连接层的示例\n",
    "\n",
    "```python\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, out_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def build(self, in_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(in_shape[-1], self.out_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_dim,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"out_dim\": self.out_dim}\n",
    "\n",
    "inputs = keras.Input((4,))\n",
    "outputs = Linear(10)(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "config = model.get_config()\n",
    "new_model = keras.Model.from_config(\n",
    "    config, custom_objects={\"out_dim\": out_dim}\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2.4 Pros and cons of functional API\n",
    "- **优点**\n",
    "    - 可以在定义连接图时对模型进行检查；<br>\n",
    "    由于使用 functional API 时，输入形状和数据类型均会预先声明，进而每个附加层均会检查传递给它的张量形状和数据类型是否与其自身所指定的参数相匹配；利用这种机制，除与模型收敛有关的调试外，其他调试均可以在构建模型期间完成；<br>\n",
    "    - functional 模型便于序列化及复制；<br>\n",
    "    由于functional 模型是一种数据结构而非代码，进而其可以直接被序列化并以 SavedModel 格式保存；然而继承模型则需要人工实现`get_config()`和`from_config()`方法；<br>\n",
    "    - functional 模型便于绘制和检查，如 2.2 节所述\n",
    "\n",
    "\n",
    "- **缺点**<br>\n",
    "    - functional API 无法实现动态网络架构；functional API 可以处理由层模型构成的有向无环图，而无法实现递归网络或 Tree-RNN 等模型，进而必须通过继承模型实现\n",
    "\n",
    "更多关于 functional API 和继承模型之间的区别，请参见 [TF2.x 中的符号 API 和命令式 API](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subclassing\n",
    "## 3.1 Subclassing `Layer` class\n",
    "`Layer`类封装了一个层的状态(也称为一个层的权重)和从输入到输出的映射；下面是手动实现全连接层的示例：\n",
    "```python\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, out_dim=32, in_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            w_init(shape=(in_dim, out_dim), dtype=\"float32\"),\n",
    "            trainable=True\n",
    "        )\n",
    "        # or via a quicker shortcut to add weight\n",
    "        self.b = self.add_weight(\n",
    "            shape=(out_dim,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "linear = Linear()\n",
    "assert len(linear.trainable_weights) == 2\n",
    "assert len(linear.non_trainable_weights) == 0\n",
    "```\n",
    "这里`w`和`b`均指定了`Trainable=True`，进而该层会自动追踪这两个变量；此外，也可以通过指定`Trainable=False`以使得需要的变量在梯度下降时保持不变，例如 BN 中的均值和方差；利用`trainable_weights`和`non_trainable_weights`属性可以查看该层的可训练和不可训练权重，示例如上；\n",
    "\n",
    "### 3.1.1 Deferring weight creation\n",
    "由于大多情况下无法获知输入的形状，以类似`Dense`、`Conv2D`等 API 那样将权重的创建进行延迟是一个不错的选择，延迟创建可以通过重写`.build()`方法实现；其内部机制在于调用`call()`方法时会先调用`.build()`方法，进而完成对权重的初始化，上面的示例可以改为如下的实现方式：\n",
    "\n",
    "```python\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, out_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def build(self, in_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(in_shape[-1], self.out_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_dim,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "```\n",
    "若在自定义层对象中使用了 keras 模块内置的层模型，考虑到这些内置的层模型已经实现了`build`方法，进而可以直接在`__init__()`方法中对其进行创建\n",
    "\n",
    "### 3.1.2 The `add_loss()` method\n",
    "通过调用`.add_loss(value)`可以创建训练时使用的损失；该层的损失及其内部层所定义的损失会被附加到`Layer.losses`属性，这些损失会在每次调用`__call__()`方法时重置，进而保证`losses`只含有上次前向传播的损失；\n",
    "\n",
    "```python\n",
    "class Activ_Regul(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(Activ_Regul, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "class Kernel_Regul(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Kernel_Regul, self).__init__()\n",
    "        self.activ_regul = Activ_Regul(1e-2)\n",
    "        self.dense = keras.layers.Dense(\n",
    "            32, kernel_regularizer=keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.activ_regul(inputs)\n",
    "        return self.dense(inputs)\n",
    "\n",
    "layer = Kernel_Regul()\n",
    "_ = layer(tf.ones([3, 2]))\n",
    "assert len(layer.losses) == 2\n",
    "```\n",
    "通过`add_loss()`方法声明的损失会在调用`Model.fit()`方法时自动计入总损失内 (需要注意的是，`Layer`并不含有`fit`方法)；然而若想要人工编写训练循环，则需要显示地将`losses`中的损失添加至总损失内，可参见下面的示例；\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "for x_train, y_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = layer(x_train)\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "        loss += sum(model.losses)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "```\n",
    "更多关于人工实现训练循环的细节，可参见[相关指导](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/)\n",
    "\n",
    "### 3.1.3 The `add_metric()` method\n",
    "`Layer`会在整个训练过程中对`add_metric()`方法添加的指标进行追踪，并将该指标的平均值添加至`Layer.metrics`属性中；与损失相同，`Model.fit()`同样也会对该指标进行追踪，可参见下面的示例：\n",
    "```python\n",
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.acc_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        acc = self.acc_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=\"adam\")\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)\n",
    "```\n",
    "### 3.1.3 Priviliged arguments in `call()` method\n",
    "- `training`\n",
    "\n",
    "    某些层例如 BN 和 Dropout 等，其在训练和推理过程中执行的操作并不相同，进而需要在`call()`方法中暴露一个`training`参数，以便于`fit()`方法实现训练-验证循环时能够正确地在相应模式下进行计算；一个 Dropout 层的实现示例如下：\n",
    "    ```python\n",
    "    class CustomDropout(keras.layers.Layer):\n",
    "        def __init__(self, rate, **kwargs):\n",
    "            super(CustomDropout, self).__init__(**kwargs)\n",
    "            self.rate = rate\n",
    "\n",
    "        def call(self, inputs, training=None):\n",
    "            if training:\n",
    "                return tf.nn.dropout(inputs, rate=self.rate)\n",
    "            return inputs\n",
    "    ```\n",
    "- `mask`\n",
    "\n",
    "    `mask`参数常用于 RNN 模型；`mask`是一个布尔张量，其每个元素对应时间序列在一个时间步的输入，用于指明该时间步下是否对输入进行掩码操作；在上一层生成掩码后，Keras 会自动将正确的`mask`参数传递给支持该参数的层模型的`__call__()`方法；可以生成掩码的层包括`Embedding`、`Masking`等；更多关于如何自定义一个支持掩码的层模型的方法，可参见[相关指导](https://www.tensorflow.org/guide/keras/masking_and_padding/)\n",
    "    \n",
    "\n",
    "### 3.1.4 Optionally enable serialization\n",
    "通过实现`get_config()`方法可以使自定义的层对象变得可序列化；值得一提的是，`Layer`类的初始化函数还接收一些关键字参数，例如`name`、`dtype`、`trainable`等，进而最好在`__init__()`方法中将这些参数传递给父类，同时将其写入`get_config`方法中；示例如下\n",
    "\n",
    "```python\n",
    "class Linear(layers.Layer):\n",
    "    def __init__(self, out_dim=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def build(self, in_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(in_shape[-1], self.out_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.out_dim,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"out_dim\": self.out_dim})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "new_layer = Linear.from_config(config)\n",
    "```\n",
    "如果需要对从`config`中反序列化的过程作更多细节要求，可以尝试重写`from_config()`方法，下面是该方法的基本实现\n",
    "```python\n",
    "def from_config(cls, config):\n",
    "    return cls(**config)\n",
    "```\n",
    "更多有关模型序列化及保存的内容，可参见[相关指导](http://localhost:8888/tree/Help_Viewer_Python/TensorFlow/TF2/Guide/Save_a_Model)\n",
    "\n",
    "## 3.2 The `Model` class\n",
    "官方建议使用`Layer`类定义内部模块块，使用`Model`类定义外部模型；例如对于 ResNet-50 而言，残差单元及其堆叠形成的模块可以继承`Layer`类，整个`ResNet50`模型继承`Model`类；\n",
    "\n",
    "`Model`类具有所有与`Layer`类所拥有的 API，但与`Layer`类不同的是，`Model`类还暴露了`fit()`、`evaluation()`、`predict()`、`save()`、`save_weights()`方法 (注意以上代码均将继承`Layer`类的对象作为 functional API 使用，即该对象本身不含有与训练验证等相关方法)，以及能够显示其内部层结构的属性`layers`；进而如果使用者需要调用以上方法时，则应继承`Model`类；若当前模型仅仅是一个更大模型的子模块，或者需要手动编写训练和保存模型的代码，可以使用`Layer`类；\n",
    "\n",
    "\n",
    "## 3.3 Putting it all together: an end-to-end example\n",
    "\n",
    "下面以变分自编码器 (Variational AutoEncoder, VAE) 为例，在 MNIST 数据集上示范一个端到端的训练过程，其中 VAE 继承`Model`类，其内部模块继承`Layer`类，损失为正则化损失（即 KL 散度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch 0\n",
      "step 0: mean loss = 0.3572\n",
      "step 100: mean loss = 0.1271\n",
      "step 200: mean loss = 0.1001\n",
      "step 300: mean loss = 0.0897\n",
      "step 400: mean loss = 0.0847\n",
      "step 500: mean loss = 0.0813\n",
      "step 600: mean loss = 0.0791\n",
      "step 700: mean loss = 0.0774\n",
      "step 800: mean loss = 0.0762\n",
      "step 900: mean loss = 0.0752\n",
      "==> Epoch 1\n",
      "step 0: mean loss = 0.0749\n",
      "step 100: mean loss = 0.0742\n",
      "step 200: mean loss = 0.0737\n",
      "step 300: mean loss = 0.0732\n",
      "step 400: mean loss = 0.0728\n",
      "step 500: mean loss = 0.0724\n",
      "step 600: mean loss = 0.0721\n",
      "step 700: mean loss = 0.0718\n",
      "step 800: mean loss = 0.0716\n",
      "step 900: mean loss = 0.0713\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.data as data\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Use (z_mean, z_log_var) to sample z, the vector encoding a digit\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch, dim = z_mean.shape[0:2]\n",
    "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps img vector to a triplet (z_mean, z_log_var, z)\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, inter_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(inter_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts the encoded vector back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, orig_dim, inter_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(inter_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(orig_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    def __init__(self, orig_dim, inter_dim=64, latent_dim=32, name=\"autoencoder\", **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.orig_dim = orig_dim\n",
    "        self.encoder = Encoder(latent_dim, inter_dim)\n",
    "        self.decoder = Decoder(orig_dim, inter_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "metric = keras.metrics.Mean()\n",
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "(x_train, _), _ = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255.\n",
    "train_dataset = data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(\"==> Epoch %d\" % (epoch,))\n",
    "    for step, inputs in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(inputs)\n",
    "            loss = loss_fn(inputs, reconstructed)\n",
    "            loss += sum(vae.losses)\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        metric(loss)\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, metric.result()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
