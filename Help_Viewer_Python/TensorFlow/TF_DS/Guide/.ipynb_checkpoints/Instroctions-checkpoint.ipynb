{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "olympic-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-absolute",
   "metadata": {},
   "source": [
    "# TensorFlow Datasets\n",
    "TFDS 下载的数据通常以`tf.data.Dataset`或`np.array`的形式加载至程序中；注意不要混淆TFDS 和`tf.data` API，前者是包装了后者的的高级 API；强烈推荐在熟悉`tf.data`使用后再阅读此文档；\n",
    "\n",
    "## Installation\n",
    "由于 tensorflow-datasets 仅支持最新版 tensorflow，进而应先更新 tensorflow 再进行安装，否则可能会出现 TF 版本不兼容或 TF 与 TFDS 依赖的第三方库版本不兼容的问题；安装稳定版的 TFDS 操作流程如下：\n",
    "```bash\n",
    "pip install --upgrade tensorflow -i https://pypi.douban.com/simple\n",
    "pip install tensorflow-datasets -i https://pypi.douban.com/simple\n",
    "```\n",
    "或通过`pip install tfds-nightly`安装最新的 TFDS，该版本每天更新一次；\n",
    "\n",
    "\n",
    "## Available datasets\n",
    "所有数据集构造器都是`tfds.core.DatasetBuilder`的子类，调用`tfds.list_builders()`可以获得可使用的数据集列表，也可以查看[官方数据集类别](https://www.tensorflow.org/datasets/catalog/overview)；\n",
    "\n",
    "\n",
    "## Load a dataset\n",
    "加载数据集最简单的方法是使用`tfds.load()`函数，该函数会将下载的数据以`TFRecord`格式保存，并将其以`Dataset`格式加载至程序中；\n",
    "\n",
    "```python\n",
    "ds = tfds.load('mnist', split='train', shuffle_files=True)\n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "```\n",
    "\n",
    "或使用`tfds.builder`以及 [TFDS 的命令行接口 (CLI)](https://www.tensorflow.org/datasets/cli)：\n",
    "\n",
    "```python\n",
    "builder = tfds.builder('mnist')\n",
    "builder.download_and_prepare()\n",
    "ds = builder.as_dataset(split='train', shuffle_files=True)\n",
    "```\n",
    "\n",
    "\n",
    "## Iterate over a dataset\n",
    "As dict\n",
    "By default, the tf.data.Dataset object contains a dict of tf.Tensors:\n",
    "\n",
    "ds = tfds.load('mnist', split='train')\n",
    "ds = ds.take(1)  # Only take a single example\n",
    "\n",
    "for example in ds:  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "  print(list(example.keys()))\n",
    "  image = example[\"image\"]\n",
    "  label = example[\"label\"]\n",
    "  print(image.shape, label)\n",
    "['image', 'label']\n",
    "(28, 28, 1) tf.Tensor(4, shape=(), dtype=int64)\n",
    "\n",
    "To find out the dict key names and structure, look at the dataset documentation in our catalog. For example: mnist documentation.\n",
    "\n",
    "As tuple (as_supervised=True)\n",
    "By using as_supervised=True, you can get a tuple (features, label) instead for supervised datasets.\n",
    "\n",
    "ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "ds = ds.take(1)\n",
    "\n",
    "for image, label in ds:  # example is (image, label)\n",
    "  print(image.shape, label)\n",
    "(28, 28, 1) tf.Tensor(4, shape=(), dtype=int64)\n",
    "\n",
    "As numpy (tfds.as_numpy)\n",
    "Uses tfds.as_numpy to convert:\n",
    "\n",
    "tf.Tensor -> np.array\n",
    "tf.data.Dataset -> Iterator[Tree[np.array]] (Tree can be arbitrary nested Dict, Tuple)\n",
    "ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "ds = ds.take(1)\n",
    "\n",
    "for image, label in tfds.as_numpy(ds):\n",
    "  print(type(image), type(label), label)\n",
    "<class 'numpy.ndarray'> <class 'numpy.int64'> 4\n",
    "\n",
    "As batched tf.Tensor (batch_size=-1)\n",
    "By using batch_size=-1, you can load the full dataset in a single batch.\n",
    "\n",
    "This can be combined with as_supervised=True and tfds.as_numpy to get the the data as (np.array, np.array):\n",
    "\n",
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='test',\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "print(type(image), image.shape)\n",
    "<class 'numpy.ndarray'> (10000, 28, 28, 1)\n",
    "\n",
    "Be careful that your dataset can fit in memory, and that all examples have the same shape.\n",
    "\n",
    "Benchmark your datasets\n",
    "Benchmarking a dataset is a simple tfds.benchmark call on any iterable (e.g. tf.data.Dataset, tfds.as_numpy,...).\n",
    "\n",
    "ds = tfds.load('mnist', split='train')\n",
    "ds = ds.batch(32).prefetch(1)\n",
    "\n",
    "tfds.benchmark(ds, batch_size=32)\n",
    "tfds.benchmark(ds, batch_size=32)  # Second epoch much faster due to auto-caching\n",
    "\n",
    "************ Summary ************\n",
    "\n",
    "Examples/sec (First included) 47889.92 ex/sec (total: 60000 ex, 1.25 sec)\n",
    "Examples/sec (First only) 110.24 ex/sec (total: 32 ex, 0.29 sec)\n",
    "Examples/sec (First excluded) 62298.08 ex/sec (total: 59968 ex, 0.96 sec)\n",
    "\n",
    "************ Summary ************\n",
    "\n",
    "Examples/sec (First included) 290380.50 ex/sec (total: 60000 ex, 0.21 sec)\n",
    "Examples/sec (First only) 2506.57 ex/sec (total: 32 ex, 0.01 sec)\n",
    "Examples/sec (First excluded) 309338.21 ex/sec (total: 59968 ex, 0.19 sec)\n",
    "\n",
    "\n",
    "Do not forget to normalize the results per batch size with the batch_size= kwarg.\n",
    "In the summary, the first warmup batch is separated from the other ones to capture tf.data.Dataset extra setup time (e.g. buffers initialization,...).\n",
    "Notice how the second iteration is much faster due to TFDS auto-caching.\n",
    "tfds.benchmark returns a tfds.core.BenchmarkResult which can be inspected for further analysis.\n",
    "Build end-to-end pipeline\n",
    "To go further, you can look:\n",
    "\n",
    "Our end-to-end Keras example to see a full training pipeline (with batching, shuffling,...).\n",
    "Our performance guide to improve the speed of your pipelines (tip: use tfds.benchmark(ds) to benchmark your datasets).\n",
    "Visualization\n",
    "tfds.as_dataframe\n",
    "tf.data.Dataset objects can be converted to pandas.DataFrame with tfds.as_dataframe to be visualized on Colab.\n",
    "\n",
    "Add the tfds.core.DatasetInfo as second argument of tfds.as_dataframe to visualize images, audio, texts, videos,...\n",
    "Use ds.take(x) to only display the first x examples. pandas.DataFrame will load the full dataset in-memory, and can be very expensive to display.\n",
    "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "\n",
    "tfds.as_dataframe(ds.take(4), info)\n",
    "\n",
    "tfds.show_examples\n",
    "tfds.show_examples returns a matplotlib.figure.Figure (only image datasets supported now):\n",
    "\n",
    "ds, info = tfds.load('mnist', split='train', with_info=True)\n",
    "\n",
    "fig = tfds.show_examples(ds, info)\n",
    "png\n",
    "\n",
    "Access the dataset metadata\n",
    "All builders include a tfds.core.DatasetInfo object containing the dataset metadata.\n",
    "\n",
    "It can be accessed through:\n",
    "\n",
    "The tfds.load API:\n",
    "ds, info = tfds.load('mnist', with_info=True)\n",
    "The tfds.core.DatasetBuilder API:\n",
    "builder = tfds.builder('mnist')\n",
    "info = builder.info\n",
    "The dataset info contains additional informations about the dataset (version, citation, homepage, description,...).\n",
    "\n",
    "print(info)\n",
    "tfds.core.DatasetInfo(\n",
    "    name='mnist',\n",
    "    full_name='mnist/3.0.1',\n",
    "    description=\"\"\"\n",
    "    The MNIST database of handwritten digits.\n",
    "    \"\"\",\n",
    "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
    "    data_path='gs://tensorflow-datasets/datasets/mnist/3.0.1',\n",
    "    download_size=11.06 MiB,\n",
    "    dataset_size=21.00 MiB,\n",
    "    features=FeaturesDict({\n",
    "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
    "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
    "    }),\n",
    "    supervised_keys=('image', 'label'),\n",
    "    splits={\n",
    "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
    "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
    "    },\n",
    "    citation=\"\"\"@article{lecun2010mnist,\n",
    "      title={MNIST handwritten digit database},\n",
    "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
    "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
    "      volume={2},\n",
    "      year={2010}\n",
    "    }\"\"\",\n",
    ")\n",
    "\n",
    "Features metadata (label names, image shape,...)\n",
    "Access the tfds.features.FeatureDict:\n",
    "\n",
    "info.features\n",
    "FeaturesDict({\n",
    "    'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
    "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
    "})\n",
    "Number of classes, label names:\n",
    "\n",
    "print(info.features[\"label\"].num_classes)\n",
    "print(info.features[\"label\"].names)\n",
    "print(info.features[\"label\"].int2str(7))  # Human readable version (8 -> 'cat')\n",
    "print(info.features[\"label\"].str2int('7'))\n",
    "10\n",
    "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "7\n",
    "7\n",
    "\n",
    "Shapes, dtypes:\n",
    "\n",
    "print(info.features.shape)\n",
    "print(info.features.dtype)\n",
    "print(info.features['image'].shape)\n",
    "print(info.features['image'].dtype)\n",
    "{'image': (28, 28, 1), 'label': ()}\n",
    "{'image': tf.uint8, 'label': tf.int64}\n",
    "(28, 28, 1)\n",
    "<dtype: 'uint8'>\n",
    "\n",
    "Split metadata (e.g. split names, number of examples,...)\n",
    "Access the tfds.core.SplitDict:\n",
    "\n",
    "print(info.splits)\n",
    "{'test': <SplitInfo num_examples=10000, num_shards=1>, 'train': <SplitInfo num_examples=60000, num_shards=1>}\n",
    "\n",
    "Available splits:\n",
    "\n",
    "print(list(info.splits.keys()))\n",
    "['test', 'train']\n",
    "\n",
    "Get info on individual split:\n",
    "\n",
    "print(info.splits['train'].num_examples)\n",
    "print(info.splits['train'].filenames)\n",
    "print(info.splits['train'].num_shards)\n",
    "60000\n",
    "['mnist-train.tfrecord-00000-of-00001']\n",
    "1\n",
    "\n",
    "It also works with the subsplit API:\n",
    "\n",
    "print(info.splits['train[15%:75%]'].num_examples)\n",
    "print(info.splits['train[15%:75%]'].file_instructions)\n",
    "36000\n",
    "[FileInstruction(filename='mnist-train.tfrecord-00000-of-00001', skip=9000, take=36000, num_examples=36000)]\n",
    "\n",
    "Troubleshooting\n",
    "Manual download (if download fails)\n",
    "If download fails for some reason (e.g. offline,...). You can always manually download the data yourself and place it in the manual_dir (defaults to ~/tensorflow_datasets/download/manual/.\n",
    "\n",
    "To find out which urls to download, look into:\n",
    "\n",
    "For new datasets (implemented as folder): tensorflow_datasets/<type>/<dataset_name>/checksums.tsv. For example: tensorflow_datasets/text/bool_q/checksums.tsv.\n",
    "\n",
    "You can find the dataset source location in our catalog.\n",
    "\n",
    "For old datasets: tensorflow_datasets/url_checksums/<dataset_name>.txt\n",
    "\n",
    "Fixing NonMatchingChecksumError\n",
    "TFDS ensure determinism by validating the checksums of downloaded urls. If NonMatchingChecksumError is raised, might indicate:\n",
    "\n",
    "The website may be down (e.g. 503 status code). Please check the url.\n",
    "For Google Drive URLs, try again later as Drive sometimes rejects downloads when too many people access the same URL. See bug\n",
    "The original datasets files may have been updated. In this case the TFDS dataset builder should be updated. Please open a new Github issue or PR:\n",
    "Register the new checksums with tfds build --register_checksums\n",
    "Eventually update the dataset generation code.\n",
    "Update the dataset VERSION\n",
    "Update the dataset RELEASE_NOTES: What caused the checksums to change ? Did some examples changed ?\n",
    "Make sure the dataset can still be built.\n",
    "Send us a PR\n",
    "Note: You can also inspect the downloaded file in ~/tensorflow_datasets/download/.\n",
    "Citation\n",
    "If you're using tensorflow-datasets for a paper, please include the following citation, in addition to any citation specific to the used datasets (which can be found in the dataset catalog).\n",
    "\n",
    "@misc{TFDS,\n",
    "  title = { {TensorFlow Datasets}, A collection of ready-to-use datasets},\n",
    "  howpublished = {\\url{https://www.tensorflow.org/datasets} },\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
