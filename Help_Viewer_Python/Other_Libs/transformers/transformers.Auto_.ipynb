{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoModelForQuestionAnswering()\n",
    "`transformers.AutoModelForQuestionAnswering()`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "`~transformers.AutoModelForQuestionAnswering`是一个通用的模型类，可通过类方法`AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)`将其实例化为库中的一个用于 QA 任务模型的对象，这个类不能通过`__init__()`方法实例化，否则会抛出异常\n",
    "\n",
    "**File**:     \\transformers\\modeling_auto.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## AutoModelForQuestionAnswering.from_pretrained()\n",
    "```python\n",
    "transformers.AutoModelForQuestionAnswering.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    *model_args,\n",
    "    **kwargs,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "根据预训练模型的配置，初始化库中的一个 QA 模型；\n",
    "\n",
    "`from_pretrained()`方法负责根据配置对象的`model_type`属性返回正确的模型类实例，或当该，则使用与`pretrained_model_name_or_path`字符串匹配的模式：\n",
    "The `from_pretrained()` method takes care of returning the correct model class instance based on the `model_type` property of the config object, or when it's missing, falling back to using pattern matching on the `pretrained_model_name_or_path` string:\n",
    "\n",
    "- `distilbert`: `~transformers.DistilBertForQuestionAnswering`类的实例化对象，即 DistilBERT 模型\n",
    "- `albert`: `~transformers.AlbertForQuestionAnswering`类的实例化对象，即 ALBERT 模型\n",
    "- `bert`: `~transformers.BertForQuestionAnswering`类的实例化对象，即 Bert 模型\n",
    "- `xlnet`: `~transformers.XLNetForQuestionAnswering`类的实例化对象，即 XLNet 模型\n",
    "- `xlm`: `~transformers.XLMForQuestionAnswering`类的实例化对象，即 XLM 模型\n",
    "- `flaubert`: :class:`~transformers.FlaubertForQuestionAnswering`类的实例化对象，即 XLM 模型\n",
    "\n",
    "该模型默认处于评估模式，即不使用 dropout，若需要训练该模型，需调用方法`model.train()`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- pretrained_model_name_or_path: 其可以是\n",
    "    - 一个从缓存或下载文件中加载的预训练模型的`shortcut name`的字符串，如``bert-base-uncased``.\n",
    "    - 用户上传到 S3 的预训练模型的`identifier name`的字符串，如``dbmdz/bert-base-german-cased``\n",
    "    - 一个含有通过`~transformers.PreTrainedModel.save_pretrained`方法保存权重的模型的路径，如``./my_model_directory/``\n",
    "    - 到`tensorflow index checkpoint file`的路径或 URL，如`./tf_model/model.ckpt.index`；这种情况下``from_tf``应设为 True，且`config`参数应指明一个配置对象；与使用本库提供的转换脚本将 TensorFlow 检查点转换至 PyTorch 模型再加载相比，这种加载方式更慢\n",
    "\n",
    "- model_args: 位置参数的序列，所有剩余的位置参数将会被传递到基础模型 (underlying model)的``__init__``方法\n",
    "\n",
    "- config: 应为`~transformers.PretrainedConfig`类的实例化对象；表示自定义的的模型配置而非自动加载的配置；自动加载参数会在以下情况中发生：\n",
    "    - 该模型为库中提供的模型，该模型通过预训练模型的``shortcut-name``字符串加载\n",
    "    - 该模型为通过`~transformers.PreTrainedModel.save_pretrained`方法保存的模型\n",
    "    - 该模型通过一个本地目录``pretrained_model_name_or_path``加载获得，且该目录下有一个名为`config.json`的 JSON 配置文件\n",
    "\n",
    "- state_dict: 储存着模型状态的字典，常常在需要根据预训练参数同时加载自定义参数来创建模型时使用；这种情况下应该检查是否使用了`~transformers.PreTrainedModel.save_pretrained`方法，`~transformers.PreTrainedModel.from_pretrained`并非一个简便的选项\n",
    "\n",
    "- cache_dir: 在不使用标准缓存时预训练模型配置下载的路径\n",
    "\n",
    "- force_download: 默认 False，当模型权重和配置已经存在时，是否重新下载/下载并重写这些内容\n",
    "\n",
    "- proxies: 默认为 None，一个协议或端点使用的代理服务器的字典型对象，如`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`；The proxies are used on each request.\n",
    "\n",
    "- output_loading_info: True 时返回一个包含丢失键、意外键、异常信息的字典\n",
    "\n",
    "- kwargs: 关键字参数的 Remaining dictionary，这些参数将传递给模型的配置参数\n",
    "\n",
    "**Type**:      method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
    "assert model.config.output_attention == True\n",
    "# Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
    "config = AutoConfig.from_json_file('./tf_model/bert_tf_model_config.json')\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index',\n",
    "                                                      from_tf=True,\n",
    "                                                      config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoConfig.from_pretrained()\n",
    "`AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "Instantiates one of the configuration classes of the library\n",
    "from a pre-trained model configuration.\n",
    "\n",
    "The configuration class to instantiate is selected\n",
    "based on the `model_type` property of the config object, or when it's missing,\n",
    "falling back to using pattern matching on the `pretrained_model_name_or_path` string:\n",
    "\n",
    "    - `t5`: :class:`~transformers.T5Config` (T5 model)\n",
    "    - `distilbert`: :class:`~transformers.DistilBertConfig` (DistilBERT model)\n",
    "    - `albert`: :class:`~transformers.AlbertConfig` (ALBERT model)\n",
    "    - `camembert`: :class:`~transformers.CamembertConfig` (CamemBERT model)\n",
    "    - `xlm-roberta`: :class:`~transformers.XLMRobertaConfig` (XLM-RoBERTa model)\n",
    "    - `longformer`: :class:`~transformers.LongformerConfig` (Longformer model)\n",
    "    - `roberta`: :class:`~transformers.RobertaConfig` (RoBERTa model)\n",
    "    - `reformer`: :class:`~transformers.ReformerConfig` (Reformer model)\n",
    "    - `bert`: :class:`~transformers.BertConfig` (Bert model)\n",
    "    - `openai-gpt`: :class:`~transformers.OpenAIGPTConfig` (OpenAI GPT model)\n",
    "    - `gpt2`: :class:`~transformers.GPT2Config` (OpenAI GPT-2 model)\n",
    "    - `transfo-xl`: :class:`~transformers.TransfoXLConfig` (Transformer-XL model)\n",
    "    - `xlnet`: :class:`~transformers.XLNetConfig` (XLNet model)\n",
    "    - `xlm`: :class:`~transformers.XLMConfig` (XLM model)\n",
    "    - `ctrl` : :class:`~transformers.CTRLConfig` (CTRL model)\n",
    "    - `flaubert` : :class:`~transformers.FlaubertConfig` (Flaubert model)\n",
    "    - `electra` : :class:`~transformers.ElectraConfig` (ELECTRA model)\n",
    "\n",
    "Args:\n",
    "    pretrained_model_name_or_path (:obj:`string`):\n",
    "        Is either: \\\n",
    "            - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n",
    "            - a string with the `identifier name` of a pre-trained model configuration that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
    "            - a path to a `directory` containing a configuration file saved using the :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
    "            - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n",
    "\n",
    "    cache_dir (:obj:`string`, optional, defaults to `None`):\n",
    "        Path to a directory in which a downloaded pre-trained model\n",
    "        configuration should be cached if the standard cache should not be used.\n",
    "\n",
    "    force_download (:obj:`boolean`, optional, defaults to `False`):\n",
    "        Force to (re-)download the model weights and configuration files and override the cached versions if they exist.\n",
    "\n",
    "    resume_download (:obj:`boolean`, optional, defaults to `False`):\n",
    "        Do not delete incompletely received file. Attempt to resume the download if such a file exists.\n",
    "\n",
    "    proxies (:obj:`Dict[str, str]`, optional, defaults to `None`):\n",
    "        A dictionary of proxy servers to use by protocol or endpoint, e.g.: :obj:`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}`.\n",
    "        The proxies are used on each request. See `the requests documentation <https://requests.readthedocs.io/en/master/user/advanced/#proxies>`__ for usage.\n",
    "\n",
    "    return_unused_kwargs (:obj:`boolean`, optional, defaults to `False`):\n",
    "        - If False, then this function returns just the final configuration object.\n",
    "        - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n",
    "\n",
    "    kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`): key/value pairs with which to update the configuration object after loading.\n",
    "        - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n",
    "        - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n",
    "\n",
    "\n",
    "Examples::\n",
    "\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased')  # Download configuration from S3 and cache.\n",
    "    config = AutoConfig.from_pretrained('./test/bert_saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
    "    config = AutoConfig.from_pretrained('./test/bert_saved_model/my_configuration.json')\n",
    "    config = AutoConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n",
    "    assert config.output_attention == True\n",
    "    config, unused_kwargs = AutoConfig.from_pretrained('bert-base-uncased', output_attention=True,\n",
    "                                                       foo=False, return_unused_kwargs=True)\n",
    "    assert config.output_attention == True\n",
    "    assert unused_kwargs == {'foo': False}\n",
    "File:      d:\\programmefiles\\python\\anaconda3\\envs\\tensorflow2.2\\lib\\site-packages\\transformers\\configuration_auto.py\n",
    "Type:      method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
