{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'Scrapy_Tutorial', using template directory 'D:\\ProgrammeFiles\\Python\\miniconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    D:\\Coding\\Python\\JupyterProjects\\Help_Viewer_Python\\scrapy\\Guides\\Scrapy_Tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd Scrapy_Tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "! scrapy startproject Scrapy_Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Tutorial\n",
    "\n",
    "本教程以爬取 http://quotes.toscrape.com/ 网站为示例演示爬取的流程，该网站包含了众多知名作家语录；\n",
    "\n",
    "This tutorial will walk you through these tasks:\n",
    "\n",
    "1. Creating a new Scrapy project\n",
    "2. Writing a spider to crawl a site and extract data\n",
    "3. Exporting the scraped data using the command line\n",
    "4. Changing spider to recursively follow links\n",
    "5. Using spider arguments\n",
    "\n",
    "\n",
    "\n",
    "## Creating a project\n",
    "依照官方流程，首先在终端输入下面的命令行以创建一个 Scrapy 项目：\n",
    "```bash\n",
    "scrapy startproject <project_name>\n",
    "```\n",
    "\n",
    "例如`Scrapy_Tutorial`作为项目名称，创建后的文件夹目录结构如下：\n",
    "- Scrapy_Tutorial\n",
    "    - scrapy.cfg：部署配置文件\n",
    "    - Scrapy_Tutorial：该项目的 Python 模块\n",
    "        - \\_\\_init__.py\n",
    "        - items.py：项目内容定义文件\n",
    "        - middlewares.py：项目中间设备文件\n",
    "        - pipelines.py：项目 pipeline 文件\n",
    "        - settings.py：项目设置文件\n",
    "        - spiders：定义爬虫的文件的目录\n",
    "            - \\_\\_init__.py\n",
    "\n",
    "Scrapy 利用人为定义的爬虫来从一个网站或一组网站获取信息；爬虫对象必须继承`Spider`类，并定义要发出的初始请求，需要时还应定义如何跟踪页面中的链接，以及如何解析下载的页面内容以进行数据的提取；下面是文件夹`Scrapy_Tutorial/spiders`下`quotes_spider.py`中定义的爬虫类的示例：\n",
    "```python\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log(f'Saved file {filename}')\n",
    "```\n",
    "\n",
    "其中\n",
    "- `name`为每个爬虫对象特有的属性；\n",
    "- `start_requests()`必须返回一个由`Request`形成的可迭代对象，爬虫从该对象开始爬取，随后的请求从这些初始请求中依次生成；\n",
    "- `parse()`是处理每个请求所下载的相应的方法；`response`参数应为一个`TextResponse`实例，该对象含有内面内容以及对该内容进行进一步处理的方法；`parse()`方法通常对响应进行解析，将爬取到的数据提取为字典形式，并查找要追踪的新 url，并从中创建新请求；\n",
    "\n",
    "### How to run our spider\n",
    "To put our spider to work, go to the project’s top level directory and run:\n",
    "\n",
    "scrapy crawl quotes\n",
    "This command runs the spider with name quotes that we’ve just added, that will send some requests for the quotes.toscrape.com domain. You will get an output similar to this:\n",
    "\n",
    "... (omitted for brevity)\n",
    "2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "...\n",
    "Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our parse method instructs.\n",
    "\n",
    "Note\n",
    "\n",
    "If you are wondering why we haven’t parsed the HTML yet, hold on, we will cover that soon.\n",
    "\n",
    "### What just happened under the hood?\n",
    "Scrapy schedules the scrapy.Request objects returned by the start_requests method of the Spider. Upon receiving a response for each one, it instantiates Response objects and calls the callback method associated with the request (in this case, the parse method) passing the response as argument.\n",
    "\n",
    "### A shortcut to the start_requests method\n",
    "Instead of implementing a start_requests() method that generates scrapy.Request objects from URLs, you can just define a start_urls class attribute with a list of URLs. This list will then be used by the default implementation of start_requests() to create the initial requests for your spider:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "The parse() method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because parse() is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback.\n",
    "\n",
    "Extracting data\n",
    "The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. Run:\n",
    "\n",
    "scrapy shell 'http://quotes.toscrape.com/page/1/'\n",
    "Note\n",
    "\n",
    "Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (i.e. & character) will not work.\n",
    "\n",
    "On Windows, use double quotes instead:\n",
    "\n",
    "scrapy shell \"http://quotes.toscrape.com/page/1/\"\n",
    "You will see something like:\n",
    "\n",
    "[ ... Scrapy log here ... ]\n",
    "2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET http://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 http://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>\n",
    "[s] Useful shortcuts:\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   fetch(req_or_url) Fetch request (or URL) and update local objects\n",
    "[s]   view(response)    View response in a browser\n",
    "Using the shell, you can try selecting elements using CSS with the response object:\n",
    "\n",
    ">>> response.css('title')\n",
    "[<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "The result of running response.css('title') is a list-like object called SelectorList, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "\n",
    ">>> response.css('title::text').getall()\n",
    "['Quotes to Scrape']\n",
    "There are two things to note here: one is that we’ve added ::text to the CSS query, to mean we want to select only the text elements directly inside <title> element. If we don’t specify ::text, we’d get the full title element, including its tags:\n",
    "\n",
    ">>> response.css('title').getall()\n",
    "['<title>Quotes to Scrape</title>']\n",
    "The other thing is that the result of calling .getall() is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "\n",
    ">>> response.css('title::text').get()\n",
    "'Quotes to Scrape'\n",
    "As an alternative, you could’ve written:\n",
    "\n",
    ">>> response.css('title::text')[0].get()\n",
    "'Quotes to Scrape'\n",
    "However, using .get() directly on a SelectorList instance avoids an IndexError and returns None when it doesn’t find any element matching the selection.\n",
    "\n",
    "There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data.\n",
    "\n",
    "Besides the getall() and get() methods, you can also use the re() method to extract using regular expressions:\n",
    "\n",
    ">>> response.css('title::text').re(r'Quotes.*')\n",
    "['Quotes to Scrape']\n",
    ">>> response.css('title::text').re(r'Q\\w+')\n",
    "['Quotes']\n",
    ">>> response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "['Quotes', 'Scrape']\n",
    "In order to find the proper CSS selectors to use, you might find useful opening the response page from the shell in your web browser using view(response). You can use your browser’s developer tools to inspect the HTML and come up with a selector (see Using your browser’s Developer Tools for scraping).\n",
    "\n",
    "Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers.\n",
    "\n",
    "XPath: a brief intro\n",
    "Besides CSS, Scrapy selectors also support using XPath expressions:\n",
    "\n",
    ">>> response.xpath('//title')\n",
    "[<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]\n",
    ">>> response.xpath('//title/text()').get()\n",
    "'Quotes to Scrape'\n",
    "XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read closely the text representation of the selector objects in the shell.\n",
    "\n",
    "While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: select the link that contains the text “Next Page”. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.\n",
    "\n",
    "We won’t cover much of XPath here, but you can read more about using XPath with Scrapy Selectors here. To learn more about XPath, we recommend this tutorial to learn XPath through examples, and this tutorial to learn “how to think in XPath”.\n",
    "\n",
    "### Extracting quotes and authors\n",
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in http://quotes.toscrape.com is represented by HTML elements that look like this:\n",
    "\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "Let’s open up scrapy shell and play a bit to find out how to extract the data we want:\n",
    "\n",
    "$ scrapy shell 'http://quotes.toscrape.com'\n",
    "We get a list of selectors for the quote HTML elements with:\n",
    "\n",
    ">>> response.css(\"div.quote\")\n",
    "[<Selector xpath=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector xpath=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " ...]\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "\n",
    ">>> quote = response.css(\"div.quote\")[0]\n",
    "Now, let’s extract text, author and the tags from that quote using the quote object we just created:\n",
    "\n",
    ">>> text = quote.css(\"span.text::text\").get()\n",
    ">>> text\n",
    "'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    ">>> author = quote.css(\"small.author::text\").get()\n",
    ">>> author\n",
    "'Albert Einstein'\n",
    "Given that the tags are a list of strings, we can use the .getall() method to get all of them:\n",
    "\n",
    ">>> tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    ">>> tags\n",
    "['change', 'deep-thoughts', 'thinking', 'world']\n",
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:\n",
    "\n",
    ">>> for quote in response.css(\"div.quote\"):\n",
    "...     text = quote.css(\"span.text::text\").get()\n",
    "...     author = quote.css(\"small.author::text\").get()\n",
    "...     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "...     print(dict(text=text, author=author, tags=tags))\n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "...\n",
    "Extracting data in our spider\n",
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the yield Python keyword in the callback, as you can see below:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "If you run this spider, it will output the extracted data with the log:\n",
    "\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}\n",
    "2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\"}\n",
    "Storing the scraped data\n",
    "The simplest way to store the scraped data is by using Feed exports, with the following command:\n",
    "\n",
    "scrapy crawl quotes -O quotes.json\n",
    "That will generate an quotes.json file containing all scraped items, serialized in JSON.\n",
    "\n",
    "The -O command-line switch overwrites any existing file; use -o instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines:\n",
    "\n",
    "scrapy crawl quotes -o quotes.jl\n",
    "The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in tutorial/pipelines.py. Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n",
    "\n",
    "## Following links\n",
    "Let’s say, instead of just scraping the stuff from the first two pages from http://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "We can try extracting it in the shell:\n",
    "\n",
    ">>> response.css('li.next a').get()\n",
    "'<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>'\n",
    "This gets the anchor element, but we want the attribute href. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "\n",
    ">>> response.css('li.next a::attr(href)').get()\n",
    "'/page/2/'\n",
    "There is also an attrib property available (see Selecting element attributes for more):\n",
    "\n",
    ">>> response.css('li.next a').attrib['href']\n",
    "'/page/2/'\n",
    "Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "Now, after extracting the data, the parse() method looks for the link to the next page, builds a full absolute URL using the urljoin() method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination.\n",
    "\n",
    "A shortcut for creating Requests\n",
    "As a shortcut for creating Request objects you can use response.follow:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('span small::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "Unlike scrapy.Request, response.follow supports relative URLs directly - no need to call urljoin. Note that response.follow just returns a Request instance; you still have to yield this Request.\n",
    "\n",
    "You can also pass a selector to response.follow instead of a string; this selector should extract necessary attributes:\n",
    "\n",
    "for href in response.css('ul.pager a::attr(href)'):\n",
    "    yield response.follow(href, callback=self.parse)\n",
    "For <a> elements there is a shortcut: response.follow uses their href attribute automatically. So the code can be shortened further:\n",
    "\n",
    "for a in response.css('ul.pager a'):\n",
    "    yield response.follow(a, callback=self.parse)\n",
    "To create multiple requests from an iterable, you can use response.follow_all instead:\n",
    "\n",
    "anchors = response.css('ul.pager a')\n",
    "yield from response.follow_all(anchors, callback=self.parse)\n",
    "or, shortening it further:\n",
    "\n",
    "yield from response.follow_all(css='ul.pager a', callback=self.parse)\n",
    "More examples and patterns\n",
    "Here is another spider that illustrates callbacks and following links, this time for scraping author information:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css('.author + a')\n",
    "        yield from response.follow_all(author_page_links, self.parse_author)\n",
    "\n",
    "        pagination_links = response.css('li.next a')\n",
    "        yield from response.follow_all(pagination_links, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default='').strip()\n",
    "\n",
    "        yield {\n",
    "            'name': extract_with_css('h3.author-title::text'),\n",
    "            'birthdate': extract_with_css('.author-born-date::text'),\n",
    "            'bio': extract_with_css('.author-description::text'),\n",
    "        }\n",
    "This spider will start from the main page, it will follow all the links to the authors pages calling the parse_author callback for each of them, and also the pagination links with the parse callback as we saw before.\n",
    "\n",
    "Here we’re passing callbacks to response.follow_all as positional arguments to make the code shorter; it also works for Request.\n",
    "\n",
    "The parse_author callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data.\n",
    "\n",
    "Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.\n",
    "\n",
    "Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.\n",
    "\n",
    "As yet another example spider that leverages the mechanism of following links, check out the CrawlSpider class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.\n",
    "\n",
    "Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks.\n",
    "\n",
    "Using spider arguments\n",
    "You can provide command line arguments to your spiders by using the -a option when running them:\n",
    "\n",
    "scrapy crawl quotes -O quotes-humor.json -a tag=humor\n",
    "These arguments are passed to the Spider’s __init__ method and become spider attributes by default.\n",
    "\n",
    "In this example, the value provided for the tag argument will be available via self.tag. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'http://quotes.toscrape.com/'\n",
    "        tag = getattr(self, 'tag', None)\n",
    "        if tag is not None:\n",
    "            url = url + 'tag/' + tag\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "If you pass the tag=humor argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as http://quotes.toscrape.com/tag/humor.\n",
    "\n",
    "You can learn more about handling spider arguments here.\n",
    "\n",
    "Next steps\n",
    "This tutorial covered only the basics of Scrapy, but there’s a lot of other features not mentioned here. Check the What else? section in Scrapy at a glance chapter for a quick overview of the most important ones.\n",
    "\n",
    "You can continue from the section Basic concepts to know more about the command-line tool, spiders, selectors and other things the tutorial hasn’t covered like modeling the scraped data. If you prefer to play with an example project, check the Examples section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
