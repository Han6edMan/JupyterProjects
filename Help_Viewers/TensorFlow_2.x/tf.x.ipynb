{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Variable()\n",
    "```python\n",
    "tf.Variable(\n",
    "    initial_value=None,\n",
    "    trainable=None,\n",
    "    validate_shape=True,\n",
    "    caching_device=None,\n",
    "    name=None,\n",
    "    variable_def=None,\n",
    "    dtype=None,\n",
    "    import_scope=None,\n",
    "    constraint=None,\n",
    "    synchronization=VariableSynchronization.AUTO,\n",
    "    aggregation=VariableAggregation.NONE,\n",
    "    shape=None)\n",
    "```\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "用于声明一个由程序操作的、保持着共享性和持久性的`Variable`对象。此外在`tf.device`作用域下可以通过调用`tf.Variable.read_value()`来手动缓存变量的值。更多细节详见[variable guide](https://tensorflow.org/guide/variable)\n",
    "\n",
    "**Args:**\n",
    "\n",
    "- initial_value: 张量或可转换为张量的 Python 对象，是`Variable`的初始化取值；`initial_value`也可以是也可以是不带参数的可调用函数，其在调用时返回用于初始化的对象，这种情况下`dtype`必须被指明；需要注意的是，若调用`init_ops.py`模块中的初始化函数，则该函数在调用前必须首先绑定到一个 shape 上\n",
    "\n",
    "- trainable: 当 `synchronization`被设定为`ON_READ`时为`False`，否则默认为`True`；当其取值为`True`时，`GradientTapes`将自动记录这个变量的使用情况。\n",
    "\n",
    "- validate_shape: 取值为`False`时，该变量可以用一个形状未知的值初始化，否则变量形状必须被指明\n",
    "\n",
    "- caching_device: 字符串型变量，已被弃用；用于指明应在何处缓存变量以进行读取，默认为`Variable`所在的设备；取值不为`None`时则缓存到另一个设备上；通常缓存在`Ops`使用的变量驻留的设备上，以能够通过`Switch`和其他条件语句来减少重复的拷贝\n",
    "\n",
    "- name: 变量的名称，默认为`'Variable'`并自动使该名称唯一化\n",
    "\n",
    "- variable_def: `VariableDef`协议缓冲区；若取值不为`None`，则用于根据该变量在计算图中的节点，重新创建变量对象及其内容，但不会对计算图进行更改。`variable_def`和其他参数是互斥的，当其与`inital_value`同时被指明时会报错\n",
    "\n",
    "- dtype: 当没有指明类型时，即其取值为`None`时，若`initial_value`为一张量，则该变量的数据类型将被保留；若`initial_value`不为张量，则数据类型由`convert_to_tensor`决定\n",
    "\n",
    "- import_scope: 加给`Variable`的名称作用域，仅在从协议缓冲区初始化时使用\n",
    "\n",
    "- constraint: 被`Optimizer`更新后作用在变量上的投影函数，如用于对该层权重的范数约束或值约束的函数；该函数必须以表示变量值的未投影张量作为输入，并返回投影值的张量，返回张量应与原张量形状相同。在进行异步分布式训练时，使用约束并不安全\n",
    "\n",
    "- synchronization: 用于指明分布式变量是什么时候被聚集的，支持由`tf.VariableSynchronization`类定义的变量；默认情况下，同步状态设置为`AUTO`，并由当前的`DistributionStrategy`决定何时同步\n",
    "\n",
    "- aggregation: 用于指明分布式变量是如何被聚集的，支持由`tf.VariableAggregation`类定义的变量\n",
    "\n",
    "- shape: `None`时使用`initial_value`的形状，当设定为`tf.TensorShape(None)`时，即表示一个未指定的形状，该变量可被设定为任意形状\n",
    "\n",
    "\n",
    "**File**:      \\tensorflow\\python\\ops\\variables.py\n",
    "\n",
    "**Type**: VariableMetaclass\n",
    "\n",
    "**Subclasses**: VariableV1, DistributedVariable, AggregatingVariable, AutoCastVariable\n",
    "\n",
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n",
      "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0>\n",
      "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.5>\n",
      "tf.Tensor(3.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.)  # => <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n",
    "print(x)\n",
    "x = x + 1  # 该操作使 x 变为 tf.Tensor 类型，进而无法使用assign修改及赋值\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=<unknown> dtype=float32, numpy=1.0>\n",
      "<tf.Variable 'UnreadVariable' shape=<unknown> dtype=float32, numpy=array([[1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1., shape=tf.TensorShape(None))\n",
    "print(x)\n",
    "print(x.assign([[1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([[1.], [2.]])\n",
    "x = tf.constant([[3., 4.]])\n",
    "y = tf.matmul(w, x)\n",
    "z = tf.sigmoid(w + x)\n",
    "print(w, x, y, z, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    trainable = tf.Variable(1.)\n",
    "    non_trainable = tf.Variable(2., trainable=False)\n",
    "    x1 = trainable * 2.\n",
    "    x2 = non_trainable * 3.\n",
    "print(tape.gradient(x1, trainable))\n",
    "print(tape.gradient(x2, non_trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Module()\n",
    "m.v = tf.Variable([1.])\n",
    "print(m.trainable_variables)\n",
    "print(m.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable(0.)\n",
    "read_and_decrement = tf.function(lambda: v.assign_sub(0.1))\n",
    "print(read_and_decrement())\n",
    "print(read_and_decrement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(tf.Module):\n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n",
    "        self.v = tf.Variable(x)\n",
    "        return self.v * x\n",
    "m = M()\n",
    "m(2.)\n",
    "m(3.)\n",
    "m.v\n",
    "# See the `tf.function` documentation for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\n",
    "# 张量生成\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.constant\n",
    "\n",
    "`tf.constant(value, dtype=None, shape=None, name='Const')`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "Creates a constant tensor from a tensor-like object.\n",
    "\n",
    "Note: All eager `tf.Tensor` values are immutable (in contrast to\n",
    "`tf.Variable`). There is nothing especially _constant_ about the value\n",
    "returned from `tf.constant`. This function it is not fundamentally different\n",
    "from `tf.convert_to_tensor`. The name `tf.constant` comes from the symbolic\n",
    "APIs (like `tf.data` or keras functional models) where the `value` is embeded\n",
    "in a `Const` node in the `tf.Graph`. `tf.constant` is useful for asserting\n",
    "that the value can be embedded that way.\n",
    "\n",
    "**Args**\n",
    "\n",
    "- value: 由任意数据类型构成的标量或张量\n",
    "\n",
    "- dtype: `None`时，输出数据类型由`value`数据类型推断得出；否则将`value`转换为指明的数据类型\n",
    "\n",
    "- shape: `value`为一标量时，则输出为指明`shape`的元素由`value`构成的张量，`value`为张量时，输出`shape`应为*可由原张量 reshape 得到的形状*\n",
    "\n",
    "- name: 张量名称\n",
    "\n",
    "\n",
    "\n",
    "Raises:\n",
    "  TypeError: if shape is incorrectly specified or unsupported.\n",
    "  ValueError: if called on a symbolic tensor.\n",
    "\n",
    "\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(True, shape=[2, 3])\n",
    "x = tf.constant([True, False], dtype = tf.float32)\n",
    "x = tf.constant([1., 2., 3., 4., 5., 6.], shape=[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于`tf.constant`将值嵌入到了`tf.Graph`，对于符号型张量会抛出异常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> i = tf.keras.layers.Input(shape=[None, None])\n",
    ">>> t = tf.constant(i)\n",
    "Traceback (most recent call last):\n",
    "...\n",
    "NotImplementedError: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.constant` will _always_ create CPU (host) tensors. In order to create\n",
    "tensors on other devices, use `tf.identity`. (If the `value` is an eager\n",
    "Tensor, however, the tensor will be returned unmodified as mentioned above.)\n",
    "\n",
    "Related Ops:\n",
    "\n",
    "* `tf.convert_to_tensor` is similar but:\n",
    "  * It has no `shape` argument.\n",
    "  * Symbolic tensors are allowed to pass through.\n",
    "\n",
    "    >>> i = tf.keras.layers.Input(shape=[None, None])\n",
    "    >>> t = tf.convert_to_tensor(i)\n",
    "\n",
    "* `tf.fill`: differs in a few ways:\n",
    "  *   `tf.constant` supports arbitrary constants, not just uniform scalar\n",
    "      Tensors like `tf.fill`.\n",
    "  *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n",
    "      can efficiently represent large tensors.\n",
    "  *   Since `tf.fill` does not embed the value, it can produce dynamically\n",
    "      sized outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.ones()\n",
    "\n",
    "`tf.ones(shape, dtype=tf.float32, name=None)`\n",
    "\n",
    "__Args__\n",
    "\n",
    "- shape: 当为一标量时，返回一维张量\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "- name: operation的\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5. 5. 5. 5.]\n",
      " [5. 5. 5. 5.]\n",
      " [5. 5. 5. 5.]], shape=(3, 4), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "x = 3 * tf.ones(1, dtype=tf.int8, name=\"test\")\n",
    "x = 5 + tf.zeros([3, 4], dtype=tf.float16, name=None)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.fill()\n",
    "\n",
    "`tf.fill(dims, value, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "与将取值以`Const`节点嵌入计算图的`tf.constant(value, shape=dims)`不同，`tf.fill`在图运行环境中进行衡量，并支持基于其他运行环境的`tf.Tensors`的动态形状(`tf.fill` evaluates at graph runtime and supports dynamic shapes based on\n",
    "other runtime `tf.Tensors`)\n",
    "\n",
    "__Args__\n",
    "\n",
    "- dims: 取标量时仍返回张量\n",
    "\n",
    "- value: 略\n",
    "\n",
    "- name: `tf.Tensor`的\n",
    "\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = 4 * tf.fill(1, value=1.2, name=None)  # ≈ np.fill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "\n",
    "# 张量运算\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.add()\n",
    "\n",
    "`tf.add(x, y, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "`math.add`支持broadcasting，而`AddN`不支持；更多有关 broadcasting\n",
    "[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "`tf.subtract()`, `tf.divide()`类似\n",
    "\n",
    "__Args__\n",
    "\n",
    "- x: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`类型的`Tensor`\n",
    "\n",
    "- y: 与`x`形状相同的`Tensor`.\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply()\n",
    "tf.square()\n",
    "tf.pow()\n",
    "tf.sqrt()\n",
    "tf.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.matmul()\n",
    "```python\n",
    "tf.matmul(\n",
    "    a,\n",
    "    b,\n",
    "    transpose_a=False,\n",
    "    transpose_b=False,\n",
    "    adjoint_a=False,\n",
    "    adjoint_b=False,\n",
    "    a_is_sparse=False,\n",
    "    b_is_sparse=False,\n",
    "    name=None,\n",
    ")\n",
    "```\n",
    "**Docstring**:\n",
    "\n",
    "Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
    "\n",
    "The inputs must, following any transpositions, be tensors of rank >= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.\n",
    "\n",
    "If one or both of the matrices contain a lot of zeros, a more efficient\n",
    "multiplication algorithm can be used by setting the corresponding\n",
    "`a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
    "This optimization is only available for plain matrices (rank-2 tensors) with\n",
    "datatypes `bfloat16` or `float32`.\n",
    "\n",
    "由于 python 3.5 及以上的版本支持`@`操作符([PEP 465](https://www.python.org/dev/peps/pep-0465/))，而在 TensorFlow 中其直接等价于`tf.matmul()`函数，故以下两行代码是等价的\n",
    "\n",
    "```python\n",
    "    d = a @ b @ [[10], [11]]\n",
    "    d = tf.matmul(tf.matmul(a, b), [[10], [11]])\n",
    "```\n",
    "**Args**:\n",
    "\n",
    "- a, b: 应为秩大于 1 的`float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`类型的`tf.Tensor`，其中最后 2 维是矩阵乘法发生的维数\n",
    "\n",
    "- transpose_a, transpose_b: `True`时在计算前先对张量进行转置操作\n",
    "\n",
    "- adjoint_a: `True`时在计算前先对张量进行共轭转置操作，若`transpose_a`和`adjoint_a`，或`transpose_b`和`adjoint_b`同时设为`True`时会抛出异常\n",
    "\n",
    "- a_is_sparse, b_is_sparse: `True`时认为张量为一稀疏张量，但不支持`tf.sparse.SparseTensor`类型，即只认为该张量大多元素为 0；更多关于`tf.SparseTensor`的乘法参见`tf.sparse.sparse_dense_matmul`\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "**File**:   \\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
    "y = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
    "z = tf.matmul(a, b)\n",
    "\n",
    "\n",
    "A batch matrix multiplication with batch shape [2]:\n",
    "\n",
    ">>> a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])\n",
    ">>> a  # 3-D tensor\n",
    "<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\n",
    "array([[[ 1,  2,  3],\n",
    "        [ 4,  5,  6]],\n",
    "       [[ 7,  8,  9],\n",
    "        [10, 11, 12]]], dtype=int32)>\n",
    ">>> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2])\n",
    ">>> b  # 3-D tensor\n",
    "<tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy=\n",
    "array([[[13, 14],\n",
    "        [15, 16],\n",
    "        [17, 18]],\n",
    "       [[19, 20],\n",
    "        [21, 22],\n",
    "        [23, 24]]], dtype=int32)>\n",
    ">>> c = tf.matmul(a, b)\n",
    ">>> c  # `a` * `b`\n",
    "<tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy=\n",
    "array([[[ 94, 100],\n",
    "        [229, 244]],\n",
    "       [[508, 532],\n",
    "        [697, 730]]], dtype=int32)>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.cast()\n",
    "\n",
    "`tf.cast(x, dtype, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "对于`Tensor`，将`x`数据类型转换为`dtype`，对于`SparseTensor`或`IndexedSlices`，将`x.values`数据类型转换为`dtype`。\n",
    "\n",
    "`x`与`dtype`支持的数据类型有`uint8`、`uint16`、`uint32`、`uint64`、`int8`、`int16`、`int32`、`int64`、`float16`、`float32`、`float64`、`complex64`、`complex128`、`bfloat16`。当将虚数转为实数时，只返回实部；当将实数转换为虚数时，虚部为0\n",
    "\n",
    "``name``为运算名称\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 2 2 2]\n",
      " [2 2 2 2]\n",
      " [2 2 2 2]], shape=(3, 4), dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "x = tf.fill([3, 4], 2.99)\n",
    "y = tf.cast(x, tf.int8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.argmax()\n",
    "\n",
    "`tf.argmax(input, axis=None, output_type=tf.int64, name=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "`input`只能为`float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`的`Tensor`；`axis`和`output_type`只能为`tf.int32`、`tf.int64`\n",
    "\n",
    "Note that in case of ties the identity of the return value is not guaranteed\n",
    "\n",
    "**similarities**\n",
    "- tf.argmin()\n",
    "- \n",
    "\n",
    "**File**:  \\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_min()\n",
    "\n",
    "`tf.reduce_min(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "等价于`np.min()`\n",
    "\n",
    "Reduces `input_tensor` along the dimensions given in `axis`.\n",
    "Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
    "entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
    "are retained with length 1.\n",
    "\n",
    "If `axis` is None, all dimensions are reduced, and a\n",
    "tensor with a single element is returned.\n",
    "\n",
    "__Args__\n",
    "\n",
    "- input_tensor: 应为实数类型张量\n",
    "\n",
    "- axis, keepdims: 略\n",
    "\n",
    "- name: operation的\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1, 3, 4, 0, 3], [2, 5, 1, 8, 2]])\n",
    "y = tf.reduce_min(x, axis=0)  # == np.min()\n",
    "y = tf.reduce_max(x, axis=1)  # == np.max()\n",
    "y = tf.reduce_sum(x, axis=0)  # == np.sum()\n",
    "y = tf.reduce_mean(x, axis=0)  # == np.mean()\n",
    "print(y)\n",
    "y = tf.reduce_prod(x, axis=0)  # == np.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_any\n",
    "\n",
    "`tf.reduce_any(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "沿张量某一维度执行`or`计算\n",
    "\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "x = tf.constant([[True,  True], [False, False]])\n",
    "tf.reduce_any(x)  # True\n",
    "tf.reduce_any(x, 0)  # [True, True]\n",
    "tf.reduce_any(x, 1)  # [True, False]\n",
    "```\n",
    "\n",
    "__Args__\n",
    "\n",
    "- input_tensor: 布尔型张量\n",
    "\n",
    "- axis, keepdims, name: 略\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(\n",
    "    [[ True, False, True, False, False, True],\n",
    "     [False, True, True, False, False, True]])\n",
    "y = tf.reduce_any(x, axis=0)  # == np.any()\n",
    "print(y)\n",
    "y = tf.reduce_all(x, axis=0)  # == np.all()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_logsumexp()\n",
    "\n",
    "`tf.reduce_logsumexp(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "计算\n",
    "\n",
    "$$\\ln \\left( \\sum\\exp(input[shape[i]])\\right)$$\n",
    "\n",
    "该函数比直接计算$\\ln\\left(\\sum\\exp(input)\\right)$要稳定，其避免了指数函数和对数函数的溢出问题. \n",
    "\n",
    "__Args__: 略\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[0., 0., 0.], [0., 0., 0.]])\n",
    "y = tf.reduce_logsumexp(x)  # log(6)\n",
    "y = tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]\n",
    "y = tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]\n",
    "y = tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]\n",
    "y = tf.reduce_logsumexp(x, [0, 1])  # log(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.GradientTape()\n",
    "`tf.GradientTape(persistent=False, watch_accessed_variables=True)`\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "创建一个用于记录对变量的操作以便于进行自动微分的上下文管理器，若某操作该上下文管理器中执行，则这些操作将会被记录下来；\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- persistent: 用于指明是否创建一个 persistent 的 gradient tape；默认 False，即最多可以调用这个对象上的`gradient()`方法一次\n",
    "\n",
    "- watch_accessed_variables: 用于指明是否自动追踪上下文管理器中的所有可训练变量，默认为 True；若为 False，则使用者需对每个希望被追踪的变量调用`watch`方法\n",
    "\n",
    "**File**:    \\tensorflow\\python\\eager\\backprop.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     LossScaleGradientTape\n",
    "\n",
    "### Example\n",
    "通过嵌套计算高阶导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as Gradtape:\n",
    "    Gradtape.watch(x)\n",
    "    with tf.GradientTape() as gradtape:\n",
    "        gradtape.watch(x)\n",
    "        y = x * x\n",
    "        dy_dx = gradtape.gradient(y, x)   # => 6.0\n",
    "    d2y_dx2 = Gradtape.gradient(dy_dx, x) # => 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(114.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "dz_dx = g.gradient(z, x)  # 108.0\n",
    "dy_dx = g.gradient(y, x)  # 6.0\n",
    "del g\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "grads = g.gradient([z, y], x)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default GradientTape will automatically watch any trainable variables that\n",
    "are accessed inside the context. If you want fine grained control over which\n",
    "variables are watched you can disable automatic tracking by passing\n",
    "`watch_accessed_variables=False` to the tape constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(variable_a)\n",
    "    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n",
    "    z = variable_b ** 3  # No gradients will be available since `variable_b` is\n",
    "                       # not being watched.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.GradientTape.gradient()\n",
    "```python\n",
    "<GradientTape>.gradient(\n",
    "    target,\n",
    "    sources,\n",
    "    output_gradients=None,\n",
    "    unconnected_gradients=<UnconnectedGradients.NONE: 'none'>,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "利用`GradientTape`上下文管理器中记录的操作计算梯度，返回由`Tensor`或`IndexedSlices`或 `None`构成的列表或嵌套结构，每个与`sources`中的元素对应，即返回对象的结构与`sources`相同；若该方法在`GradientTape`上下文管理器中调或在一个非 persistent 的上下文管理器中多次调用则会报错；ValueError: if the target is a variable or if unconnected gradients is called with an unknown value\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- target: 要微分的由`Tensor`或`Variable`构成的列表或嵌套结构\n",
    "\n",
    "- sources: 由`Tensor`或`Variable`构成的列表或嵌套结构，`target`会相对`sources`中的元素进行微分\n",
    "\n",
    "- output_gradients: 包含了梯度的列表，每一个元素对应`target`中的一个元素，默认 None\n",
    "\n",
    "- unconnected_gradients: a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.\n",
    "\n",
    "**File**:    \\tensorflow\\python\\eager\\backprop.py\n",
    "\n",
    "**Type**:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.GradientTape.gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.convert_to_tensor()\n",
    "\n",
    "`tf.convert_to_tensor(value, dtype=None, dtype_hint=None, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "Converts the given `value` to a `Tensor`.\n",
    "\n",
    "该函数将各种类型的 Python 对象转化为`Tensor`类型的对象，其支持`Tensor`对象、Numpy数组、Python 列表、Python 标量。当`float`和`string`类型的 Python 列表或标量中出现`None`时，与 Numpy 将`None`转换为数值的行为不同，该函数会抛出异常\n",
    "\n",
    "**Args**\n",
    "\n",
    "- value: 满足其类型有已注册的`Tensor`的转换函数的对象(An object whose type has a registered `Tensor` conversion function)\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "- dtype_hint: 用于`dtype`为`None`时；在某些情况下，调用者在转换为张量时可能没有想到dtype，所以dtype_hint可以用作一个软首选项；若目标转换类型无法转换，则该参数等同于无效\n",
    "\n",
    "- name: 张量名称\n",
    "\n",
    "**Type**\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12)\n",
    "y = tf.convert_to_tensor(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.one_hot()\n",
    "\n",
    "```python\n",
    "tf.one_hot(\n",
    "    indices,\n",
    "    depth,\n",
    "    on_value=None,\n",
    "    off_value=None,\n",
    "    axis=None,\n",
    "    dtype=None,\n",
    "    name=None,\n",
    ")\n",
    "```\n",
    "__Docstring__\n",
    "\n",
    "Returns a one-hot tensor.\n",
    "\n",
    "`indices`中由索引表示的位置取值为`on_value`，而其他所有位置取值为`off_value`，若期望输出为非数字类型变量，则`on_value`和`off_value`必须指明。\n",
    "\n",
    "- `indices`为标量时，输出为长度为`depth`的向量；\n",
    "\n",
    "- 若`indices`为长度为`features`的向量，则输出形状\n",
    "\n",
    "    - 在axis = -1时为 $features \\times depth$\n",
    "\n",
    "    - 在axis = 0时为 $depth \\times features$\n",
    "\n",
    "- 若`indices`是形状为`[batch, features]`的矩阵，则输出形状\n",
    "    \n",
    "    - 在`axis == -1`时为 $batch \\times features \\times depth$\n",
    "\n",
    "    - 在`axis == 1`时为 $batch \\times depth \\times features$\n",
    "\n",
    "    - 在`axis == 0`时为 $batch \\times batch \\times features$\n",
    "\n",
    "- 若`indices`为不规则张量，`axis`必须为正且需指代一个规则的维度，结果等价于将`one_hot`应用在不规则张量的取值上，并生成一个新的不规则张量\n",
    "\n",
    "__Args__\n",
    "\n",
    "- indices: 内容代表索引的`Tensor`；若`indices`的秩为 N，则输出数组的秩为 N+1，新的维度会添加在`axis`指明位置中；\n",
    "\n",
    "- depth: 定义 one hot 维度的标量\n",
    "\n",
    "- on_value: 输出结果中，当`indices[j] = i`时的取值，默认为 1，需与`off_value`和`dtype`数据类型匹配\n",
    "\n",
    "- off_value: 输出结果中，当`indices[j] != i`时的取值，默认为 0，需与`on_value`和`dtype`数据类型匹配\n",
    "\n",
    "- axis: 在shape中添加索引的位置，默认 -1\n",
    "\n",
    "- dtype: 默认为`tf.float32`，需与`on_value`和`off_value`匹配\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.one_hot(indices=3, depth=7, on_value=6, off_value=-2, dtype=tf.int8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    axis=0时返回(6,4)数组，对于indices的元素“2”来说，其之前的索引为(1)\n",
    "    则将索引“2”添加到新索引(x,y)的索引为0的位置，即x的位置，进而得到新索引(2,1)\n",
    "'''\n",
    "classes = 6\n",
    "targets = [1, 2, 4, 3]\n",
    "x = tf.one_hot(indices=targets, depth=classes, axis=0)\n",
    "print(x)\n",
    "'''\n",
    "    axis=0时返回(6,4)数组，对于indices的元素“2”来说，其之前的索引为(1)\n",
    "    则将索引“2”添加到新索引(x,y)的索引为1的位置，即y的位置，进而得到新索引(1,2)\n",
    "''' \n",
    "y = tf.one_hot(indices=[5, 2, 4, 3], depth=6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    axis=0时返回(5,2,3)数组，对于indices的元素“4”来说，其之前的索引为(0,2)\n",
    "    则将索引“4”添加到新索引(x,y,z)的索引为0的位置，即x的位置，进而得到新索引(4,0,2)\n",
    "''' \n",
    "x = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=0)\n",
    "y = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=1)\n",
    "z = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=2)\n",
    "print(x, y, z, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], [[0.0, 0.0, 1.0]]]>\n"
     ]
    }
   ],
   "source": [
    "indices = tf.ragged.constant([[0, 1], [2]])  # (2, 1)\n",
    "x = tf.one_hot(indices=indices, depth=3)  # output: (2, None, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
