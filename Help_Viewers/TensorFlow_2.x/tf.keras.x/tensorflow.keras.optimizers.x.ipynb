{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizers.SGD()\n",
    "Init signature:\n",
    "optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    name='SGD',\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:     \n",
    "Stochastic gradient descent and momentum optimizer.\n",
    "\n",
    "The update rule for $\\theta$ with gradient $g$ when `momentum` is 0.0:\n",
    "$$\\theta_t = \\theta_{t-1} - \\mathrm{learning\\_rate} * g_t$$\n",
    "\n",
    "The update rule when `momentum` is larger than 0.0:\n",
    "$$v_t = \\mathrm{momentum} * v_{t-1} - \\mathrm{learning\\_rate} * g_t$$\n",
    "$$\\theta_t = \\theta_{t-1} + v_t$$\n",
    "if `nesterov` is False, gradient is evaluated at $\\theta_t$.\n",
    "if `nesterov` is True, gradient is evaluated at $\\theta_t + momentum * v_t$,\n",
    "  and the variables always store $\\theta + m v$ instead of $theta$\n",
    "\n",
    "Usage:\n",
    "\n",
    ">>> opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    ">>> var = tf.Variable(1.0)\n",
    ">>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
    ">>> step_count = opt.minimize(loss, [var]).numpy()\n",
    ">>> # Step is `-learning_rate*grad`\n",
    ">>> var.numpy()\n",
    "0.9\n",
    "\n",
    ">>> opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    ">>> var = tf.Variable(1.0)\n",
    ">>> val0 = var.value()\n",
    ">>> loss = lambda: (var ** 2)/2.0         # d(loss)/d(var1) = var1\n",
    ">>> # First step is `-learning_rate*grad`\n",
    ">>> step_count = opt.minimize(loss, [var]).numpy()\n",
    ">>> val1 = var.value()\n",
    ">>> (val0 - val1).numpy()\n",
    "0.1\n",
    ">>> # On later steps, step-size increases because of momentum\n",
    ">>> step_count = opt.minimize(loss, [var]).numpy()\n",
    ">>> val2 = var.value()\n",
    ">>> (val1 - val2).numpy()\n",
    "0.18\n",
    "\n",
    "Some of the args below are hyperparameters, where a hyperparameter is\n",
    "defined as a scalar Tensor, a regular Python value, or a callable (which\n",
    "will be evaluated when `apply_gradients` is called) returning a scalar\n",
    "Tensor or a Python value.\n",
    "\n",
    "**References**\n",
    "\n",
    "    nesterov = True, See [Sutskever et al., 2013](\n",
    "      http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).\n",
    "Init docstring:\n",
    "Construct a new Stochastic Gradient Descent or Momentum optimizer.\n",
    "\n",
    "Arguments:\n",
    "  learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "    `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "    that takes no arguments and returns the actual value to use. The\n",
    "    learning rate. Defaults to 0.01.\n",
    "  momentum: float hyperparameter >= 0 that accelerates SGD in the relevant\n",
    "    direction and dampens oscillations. Defaults to 0.0, i.e., SGD.\n",
    "  nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "    Defaults to `False`.\n",
    "  name: Optional name prefix for the operations created when applying\n",
    "    gradients.  Defaults to 'SGD'.\n",
    "  **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "    `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "    gradients by value, `decay` is included for backward compatibility to\n",
    "    allow time inverse decay of learning rate. `lr` is included for backward\n",
    "    compatibility, recommended to use `learning_rate` instead.\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\gradient_descent.py\n",
    "Type:           ABCMeta\n",
    "Subclasses: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizers.Adagrad()\n",
    "Init signature:\n",
    "optimizers.Adagrad(\n",
    "    learning_rate=0.001,\n",
    "    initial_accumulator_value=0.1,\n",
    "    epsilon=1e-07,\n",
    "    name='Adagrad',\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:     \n",
    "Optimizer that implements the Adagrad algorithm.\n",
    "\n",
    "Adagrad is an optimizer with parameter-specific learning rates,\n",
    "which are adapted relative to how frequently a parameter gets\n",
    "updated during training. The more updates a parameter receives,\n",
    "the smaller the updates.\n",
    "\n",
    "Initialization:\n",
    "$$accum_{g_0} := \\text{initial_accumulator_value}$$\n",
    "\n",
    "Update step:\n",
    "$$t := t + 1$$\n",
    "$$accum_{g_t} := accum_{g_{t-1}} + g^2$$\n",
    "$$\\theta_t := \\theta_{t-1} - lr * g / (\\sqrt{accum_{g_t}} + \\epsilon)$$\n",
    "\n",
    "References:\n",
    "\n",
    "* [Paper](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf).\n",
    "* [Introduction]\n",
    "  (https://ppasupat.github.io/a9online/uploads/proximal_notes.pdf).\n",
    "Init docstring:\n",
    "Construct a new Adagrad optimizer.\n",
    "\n",
    "Args:\n",
    "  learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "    `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.\n",
    "  initial_accumulator_value: A floating point value.\n",
    "    Starting value for the accumulators, must be non-negative.\n",
    "  epsilon: A small floating point value to avoid zero denominator.\n",
    "  name: Optional name prefix for the operations created when applying\n",
    "    gradients.  Defaults to \"Adagrad\".\n",
    "  **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "    `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "    gradients by value, `decay` is included for backward compatibility to\n",
    "    allow time inverse decay of learning rate. `lr` is included for backward\n",
    "    compatibility, recommended to use `learning_rate` instead.\n",
    "\n",
    "Raises:\n",
    "  ValueError: If the `initial_accumulator_value` or `epsilon` is invalid.\n",
    "\n",
    "@compatibility(eager)\n",
    "When eager execution is enabled, `learning_rate` can be a callable that\n",
    "takes no arguments and returns the actual value to use. This can be useful\n",
    "for changing these values across different invocations of optimizer\n",
    "functions.\n",
    "@end_compatibility\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py\n",
    "Type:           ABCMeta\n",
    "Subclasses:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizers.Adadelta()\n",
    "Init signature:\n",
    "optimizers.Adadelta(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.95,\n",
    "    epsilon=1e-07,\n",
    "    name='Adadelta',\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:     \n",
    "Optimizer that implements the Adadelta algorithm.\n",
    "\n",
    "Adadelta optimization is a stochastic gradient descent method that is based on\n",
    "adaptive learning rate per dimension to address two drawbacks:\n",
    "  1) the continual decay of learning rates throughout training\n",
    "  2) the need for a manually selected global learning rate\n",
    "\n",
    "Two accumulation steps are required:\n",
    "  1) the accumulation of gradients squared,\n",
    "  2) the accumulation of updates squared.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "$$E[g^2]_0 := 0 \\text{(Initialize gradient 2nd order moment vector)}$$\n",
    "$$E[\\Delta x^2]_0 := 0 \\text{(Initialize 2nd order variable update)}$$\n",
    "\n",
    "$$t := t + 1$$\n",
    "$$E[g^2]_t := \\rho * E[g^2]_{t-1} + (1 - \\rho) * g^2$$\n",
    "$$\\Delta x_t = -RMS[\\Delta x]_{t-1} * g_t / RMS[g]_t$$\n",
    "$$E[\\Delta x^2]_t := \\rho * E[\\Delta x^2]_{t-1} + (1 - \\rho) * \\Delta x_t^2$$\n",
    "$$x_t := x_{t-1} + \\Delta x_{t}$$\n",
    "\n",
    "References\n",
    "  See [M. D. Zeiler](http://arxiv.org/abs/1212.5701)\n",
    "    ([pdf](http://arxiv.org/pdf/1212.5701v1.pdf))\n",
    "Init docstring:\n",
    "Construct a new Adadelta optimizer.\n",
    "\n",
    "Adadelta is a more robust extension of Adagrad that adapts learning rates\n",
    "based on a moving window of gradient updates, instead of accumulating all\n",
    "past gradients. This way, Adadelta continues learning even when many updates\n",
    "have been done. Compared to Adagrad, in the original version of Adadelta you\n",
    "don't have to set an initial learning rate. In this version, initial\n",
    "learning rate can be set, as in most other Keras optimizers.\n",
    "\n",
    "Args:\n",
    "  learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "    `tf.keras.optimizers.schedules.LearningRateSchedule`. The learning rate.\n",
    "    To match the exact form in the original paper use 1.0.\n",
    "  rho: A `Tensor` or a floating point value. The decay rate.\n",
    "  epsilon: A `Tensor` or a floating point value.  A constant epsilon used\n",
    "           to better conditioning the grad update.\n",
    "  name: Optional name prefix for the operations created when applying\n",
    "    gradients.  Defaults to \"Adadelta\".\n",
    "  **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "    `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "    gradients by value, `decay` is included for backward compatibility to\n",
    "    allow time inverse decay of learning rate. `lr` is included for backward\n",
    "    compatibility, recommended to use `learning_rate` instead.\n",
    "\n",
    "@compatibility(eager)\n",
    "When eager execution is enabled, `learning_rate`, `rho`, and `epsilon` can\n",
    "each be a callable that takes no arguments and returns the actual value to\n",
    "use. This can be useful for changing these values across different\n",
    "invocations of optimizer functions.\n",
    "@end_compatibility\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adadelta.py\n",
    "Type:           ABCMeta\n",
    "Subclasses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizers.Adam()\n",
    "Init signature:\n",
    "optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    name='Adam',\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:     \n",
    "Optimizer that implements the Adam algorithm.\n",
    "\n",
    "Adam optimization is a stochastic gradient descent method that is based on\n",
    "adaptive estimation of first-order and second-order moments.\n",
    "According to the paper\n",
    "[Adam: A Method for Stochastic Optimization. Kingma et al.,\n",
    "2014](http://arxiv.org/abs/1412.6980), the method is \"*computationally\n",
    "efficient, has little memory requirement, invariant to diagonal rescaling of\n",
    "gradients, and is well suited for problems that are large in terms of\n",
    "data/parameters*\".\n",
    "\n",
    "For AMSGrad see [On The Convergence Of Adam And Beyond.\n",
    "Reddi et al., 5-8](https://openreview.net/pdf?id=ryQu7f-RZ).\n",
    "Init docstring:\n",
    "Construct a new Adam optimizer.\n",
    "\n",
    "If amsgrad = False:\n",
    "\n",
    "  initialize $m_0$ as 1st moment vector\n",
    "  initialize $v_0$ as 2nd moment vector\n",
    "\n",
    "  The update rule for $\\theta$ with gradient $g$ uses an optimization\n",
    "  described at the end of section 2 of the paper:\n",
    "\n",
    "  $$lr_t = \\mathrm{learning\\_rate} *\n",
    "    \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$$\n",
    "  $$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$$\n",
    "  $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2) * g^2$$\n",
    "  $$\\theta_t = \\theta_{t-1} - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$$\n",
    "\n",
    "If amsgrad = True:\n",
    "\n",
    "  initialize $m_0$ as 1st moment vector\n",
    "  initialize $v_0$ as 2nd moment vector\n",
    "  initialize $\\hat{v}_0$ as 2nd moment vector\n",
    "\n",
    "  The update rule for $\\theta$ with gradient $g$ uses an optimization\n",
    "  described at the end of section 2 of the paper:\n",
    "\n",
    "  $$lr_t = \\mathrm{learning\\_rate} *\n",
    "    \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$$\n",
    "\n",
    "  $$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$$\n",
    "  $$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2) * g^2$$\n",
    "  $$\\hat{v}_t = \\max(\\hat{v}_{t-1}, v_t)$$\n",
    "  $$\\theta_t = \\theta_{t-1} - lr_t * m_t / (\\sqrt{\\hat{v}_t} + \\epsilon)$$\n",
    "\n",
    "The default value of 1e-7 for epsilon might not be a good default in\n",
    "general. For example, when training an Inception network on ImageNet a\n",
    "current good choice is 1.0 or 0.1. Note that since AdamOptimizer uses the\n",
    "formulation just before Section 2.1 of the Kingma and Ba paper rather than\n",
    "the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n",
    "hat\" in the paper.\n",
    "\n",
    "The sparse implementation of this algorithm (used when the gradient is an\n",
    "IndexedSlices object, typically because of `tf.gather` or an embedding\n",
    "lookup in the forward pass) does apply momentum to variable slices even if\n",
    "they were not used in the forward pass (meaning they have a gradient equal\n",
    "to zero). Momentum decay (beta1) is also applied to the entire momentum\n",
    "accumulator. This means that the sparse behavior is equivalent to the dense\n",
    "behavior (in contrast to some momentum implementations which ignore momentum\n",
    "unless a variable slice was actually used).\n",
    "\n",
    "Usage:\n",
    "\n",
    ">>> opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    ">>> var1 = tf.Variable(10.0)\n",
    ">>> loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    ">>> step_count = opt.minimize(loss, [var1]).numpy()\n",
    ">>> # The first step is `-learning_rate*sign(grad)`\n",
    ">>> var1.numpy()\n",
    "9.9\n",
    "\n",
    "Args:\n",
    "  learning_rate: A `Tensor`, floating point value, or a schedule that is a\n",
    "    `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n",
    "    that takes no arguments and returns the actual value to use, The\n",
    "    learning rate. Defaults to 0.001.\n",
    "  beta_1: A float value or a constant float tensor, or a callable\n",
    "    that takes no arguments and returns the actual value to use. The\n",
    "    exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n",
    "  beta_2: A float value or a constant float tensor, or a callable\n",
    "    that takes no arguments and returns the actual value to use, The\n",
    "    exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n",
    "  epsilon: A small constant for numerical stability. This epsilon is\n",
    "    \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n",
    "    Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n",
    "    1e-7.\n",
    "  amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n",
    "    the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n",
    "  name: Optional name for the operations created when applying gradients.\n",
    "    Defaults to \"Adam\".\n",
    "  **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n",
    "    `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n",
    "    gradients by value, `decay` is included for backward compatibility to\n",
    "    allow time inverse decay of learning rate. `lr` is included for backward\n",
    "    compatibility, recommended to use `learning_rate` instead.\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\n",
    "Type:           ABCMeta\n",
    "Subclasses:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
