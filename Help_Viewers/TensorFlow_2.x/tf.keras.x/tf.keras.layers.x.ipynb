{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow.keras\n",
    "\n",
    "Implementation of the Keras API meant to be a high-level API for TensorFlow.\n",
    "\n",
    "**DESCRIPTION**\n",
    "\n",
    "TensorFlow 内置的 Keras API 实现库，帮助文档参见[tensorflow.org](https://www.tensorflow.org/guide/keras).\n",
    "\n",
    "**PACKAGE CONTENTS**\n",
    "\n",
    "    activations (package)\n",
    "    applications (package)\n",
    "    backend (package)\n",
    "    callbacks (package)\n",
    "    constraints (package)\n",
    "    datasets (package)\n",
    "    estimator (package)\n",
    "    experimental (package)\n",
    "    initializers (package)\n",
    "    layers (package)\n",
    "    losses (package)\n",
    "    metrics (package)\n",
    "    mixed_precision (package)\n",
    "    models (package)\n",
    "    optimizers (package)\n",
    "    premade (package)\n",
    "    preprocessing (package)\n",
    "    regularizers (package)\n",
    "    utils (package)\n",
    "    wrappers (package)\n",
    "\n",
    "**FILE**：  \\tensorflow\\lib\\site-packages\\tensorflow\\keras\\\\\\_\\_init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Flatten()\n",
    "`layers.Flatten(*args, **kwargs)`\n",
    "Docstring:     \n",
    "Flattens the input. Does not affect the batch size.\n",
    "\n",
    "If inputs are shaped `(batch,)` without a channel dimension, then flattening\n",
    "adds an extra channel dimension and output shapes are `(batch, 1)`.\n",
    "\n",
    "Arguments:\n",
    "  data_format: A string,\n",
    "    one of `channels_last` (default) or `channels_first`.\n",
    "    The ordering of the dimensions in the inputs.\n",
    "    `channels_last` corresponds to inputs with shape\n",
    "    `(batch, ..., channels)` while `channels_first` corresponds to\n",
    "    inputs with shape `(batch, channels, ...)`.\n",
    "    It defaults to the `image_data_format` value found in your\n",
    "    Keras config file at `~/.keras/keras.json`.\n",
    "    If you never set it, then it will be \"channels_last\".\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3,\n",
    "                        border_mode='same',\n",
    "                        input_shape=(3, 32, 32)))\n",
    "# now: model.output_shape == (None, 64, 32, 32)\n",
    "model.add(Flatten())\n",
    "# now: model.output_shape == (None, 65536)\n",
    "```\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\n",
    "Type:           type\n",
    "Subclasses:     Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Dense()\n",
    "```python\n",
    "layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**:\n",
    "\n",
    "在`layers.Dense`的属性中除`trainable`之外，在该层被调用一次之后便不能被更改了\n",
    "\n",
    "**Args**:\n",
    "- units: 输出神经元个数\n",
    "- activation: 激活函数，默认不附加激活函数\n",
    "- use_bias: 是否使用偏置\n",
    "- kernel_initializer, kernel_regularizer: 权重矩阵的初始化函数及正则化函数\n",
    "- bias_initializer, bias_regularizer: 偏置的初始化函数及正则化函数\n",
    "- activity_regularizer: 作用于输出层神经元的正则化函数\n",
    "- kernel_constraint: 作用在权重矩阵的约束函数\n",
    "- bias_constraint: 作用在偏置的约束函数\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     Dense\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as first layer in a sequential model:\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(16,)))\n",
    "# now the model will take as input arrays of shape (*, 16)\n",
    "# and output arrays of shape (*, 32)\n",
    "\n",
    "# after the first layer, you don't need to specify\n",
    "# the size of the input anymore:\n",
    "model.add(Dense(32))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layers.Conv2D()\n",
    "```python\n",
    "layers.Conv2D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    data_format=None,\n",
    "    dilation_rate=(1, 1),\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "**Docstring**:\n",
    "\n",
    "在第一层使用时需将`input_shape`以关键字参数传递给构造函数\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- filters: filter 的个数\n",
    "- kernel_size: 可以是一元整数或二元元组\n",
    "- strides: 可以是一元整数或二元元组；`strides != 1`与`dilation_rate!= 1`不兼容\n",
    "- padding: 可以是`\"valid\"`或`\"same\"`；`\"casual\"`时会报错\n",
    "- data_format: 应为`'channels_last'`或`'channels_first'`，即用于说明数据输入的形状是`(batch_size, height, width, channels)`还是`(batch_size, channels, rows, cols)`；默认为 Keras 的配置文件`~/.keras/keras.json`中的`image_data_format`的值，该值在未更改情况下为`channels_last`；`channels_last`时输出特征图的形状为`(batch_size, new_rows, new_cols, filters)`\n",
    "- dilation_rate: 可以是一元整数或二元元组；`strides != 1`与`dilation_rate!= 1`不兼容\n",
    "- activation: pass，更多函数详见`keras.activations`\n",
    "- use_bias: pass\n",
    "- kernel_initializer, kernel_regularizer, bias_initializer, bias_regularizer: pass，更多函数详见`keras.initializers`和`keras.regularizers`\n",
    "- activity_regularizer: 对输出层应用的正则化函数\n",
    "- kernel_constraint, bias_constraint: 对卷积核及偏置应用的约束函数\n",
    "\n",
    "**File**: \\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\n",
    "\n",
    "**Type**:           type\n",
    "\n",
    "**Subclasses**:     Conv2DTranspose, DepthwiseConv2D, Conv2D\n",
    "\n",
    "### Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n",
    ">>> # size is 4.\n",
    ">>> input_shape = (4, 28, 28, 3)\n",
    ">>> x = tf.random.normal(input_shape)\n",
    ">>> y = tf.keras.layers.Conv2D(\n",
    "... 2, 3, activation='relu', input_shape=input_shape)(x)\n",
    ">>> print(y.shape)\n",
    "(4, 26, 26, 2)\n",
    "\n",
    ">>> # With `dilation_rate` as 2.\n",
    ">>> input_shape = (4, 28, 28, 3)\n",
    ">>> x = tf.random.normal(input_shape)\n",
    ">>> y = tf.keras.layers.Conv2D(\n",
    "... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape)(x)\n",
    ">>> print(y.shape)\n",
    "(4, 24, 24, 2)\n",
    "\n",
    ">>> # With `padding` as \"same\".\n",
    ">>> input_shape = (4, 28, 28, 3)\n",
    ">>> x = tf.random.normal(input_shape)\n",
    ">>> y = tf.keras.layers.Conv2D(\n",
    "... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape)(x)\n",
    ">>> print(y.shape)\n",
    "(4, 28, 28, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers.LSTM()\n",
    "Init signature: layers.LSTM(*args, **kwargs)\n",
    "Docstring:     \n",
    "Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n",
    "for details about the usage of RNN API.\n",
    "\n",
    "Based on available runtime hardware and constraints, this layer\n",
    "will choose different implementations (cuDNN-based or pure-TensorFlow)\n",
    "to maximize the performance. If a GPU is available and all\n",
    "the arguments to the layer meet the requirement of the CuDNN kernel\n",
    "(see below for details), the layer will use a fast cuDNN implementation.\n",
    "\n",
    "The requirements to use the cuDNN implementation are:\n",
    "\n",
    "1. `activation` == `tanh`\n",
    "2. `recurrent_activation` == `sigmoid`\n",
    "3. `recurrent_dropout` == 0\n",
    "4. `unroll` is `False`\n",
    "5. `use_bias` is `True`\n",
    "6. Inputs are not masked or strictly right padded.\n",
    "\n",
    "For example:\n",
    "\n",
    ">>> inputs = tf.random.normal([32, 10, 8])\n",
    ">>> lstm = tf.keras.layers.LSTM(4)\n",
    ">>> output = lstm(inputs)\n",
    ">>> print(output.shape)\n",
    "(32, 4)\n",
    ">>> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    ">>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    ">>> print(whole_seq_output.shape)\n",
    "(32, 10, 4)\n",
    ">>> print(final_memory_state.shape)\n",
    "(32, 4)\n",
    ">>> print(final_carry_state.shape)\n",
    "(32, 4)\n",
    "\n",
    "Arguments:\n",
    "  units: Positive integer, dimensionality of the output space.\n",
    "  activation: Activation function to use.\n",
    "    Default: hyperbolic tangent (`tanh`). If you pass `None`, no activation\n",
    "    is applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "  recurrent_activation: Activation function to use for the recurrent step.\n",
    "    Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n",
    "    applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "  use_bias: Boolean (default `True`), whether the layer uses a bias vector.\n",
    "  kernel_initializer: Initializer for the `kernel` weights matrix, used for\n",
    "    the linear transformation of the inputs. Default: `glorot_uniform`.\n",
    "  recurrent_initializer: Initializer for the `recurrent_kernel` weights\n",
    "    matrix, used for the linear transformation of the recurrent state.\n",
    "    Default: `orthogonal`.\n",
    "  bias_initializer: Initializer for the bias vector. Default: `zeros`.\n",
    "  unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n",
    "    the forget gate at initialization. Setting it to true will also force\n",
    "    `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n",
    "        al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
    "  kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
    "    matrix. Default: `None`.\n",
    "  recurrent_regularizer: Regularizer function applied to the\n",
    "    `recurrent_kernel` weights matrix. Default: `None`.\n",
    "  bias_regularizer: Regularizer function applied to the bias vector. Default:\n",
    "    `None`.\n",
    "  activity_regularizer: Regularizer function applied to the output of the\n",
    "    layer (its \"activation\"). Default: `None`.\n",
    "  kernel_constraint: Constraint function applied to the `kernel` weights\n",
    "    matrix. Default: `None`.\n",
    "  recurrent_constraint: Constraint function applied to the `recurrent_kernel`\n",
    "    weights matrix. Default: `None`.\n",
    "  bias_constraint: Constraint function applied to the bias vector. Default:\n",
    "    `None`.\n",
    "  dropout: Float between 0 and 1. Fraction of the units to drop for the linear\n",
    "    transformation of the inputs. Default: 0.\n",
    "  recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n",
    "    the linear transformation of the recurrent state. Default: 0.\n",
    "  implementation: Implementation mode, either 1 or 2. Mode 1 will structure\n",
    "    its operations as a larger number of smaller dot products and additions,\n",
    "    whereas mode 2 will batch them into fewer, larger operations. These modes\n",
    "    will have different performance profiles on different hardware and for\n",
    "    different applications. Default: 2.\n",
    "  return_sequences: Boolean. Whether to return the last output. in the output\n",
    "    sequence, or the full sequence. Default: `False`.\n",
    "  return_state: Boolean. Whether to return the last state in addition to the\n",
    "    output. Default: `False`.\n",
    "  go_backwards: Boolean (default `False`). If True, process the input sequence\n",
    "    backwards and return the reversed sequence.\n",
    "  stateful: Boolean (default `False`). If True, the last state for each sample\n",
    "    at index i in a batch will be used as initial state for the sample of\n",
    "    index i in the following batch.\n",
    "  time_major: The shape format of the `inputs` and `outputs` tensors.\n",
    "    If True, the inputs and outputs will be in shape\n",
    "    `[timesteps, batch, feature]`, whereas in the False case, it will be\n",
    "    `[batch, timesteps, feature]`. Using `time_major = True` is a bit more\n",
    "    efficient because it avoids transposes at the beginning and end of the\n",
    "    RNN calculation. However, most TensorFlow data is batch-major, so by\n",
    "    default this function accepts input and emits output in batch-major\n",
    "    form.\n",
    "  unroll: Boolean (default `False`). If True, the network will be unrolled,\n",
    "    else a symbolic loop will be used. Unrolling can speed-up a RNN, although\n",
    "    it tends to be more memory-intensive. Unrolling is only suitable for short\n",
    "    sequences.\n",
    "\n",
    "Call arguments:\n",
    "  inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n",
    "  mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n",
    "    a given timestep should be masked (optional, defaults to `None`).\n",
    "  training: Python boolean indicating whether the layer should behave in\n",
    "    training mode or in inference mode. This argument is passed to the cell\n",
    "    when calling it. This is only relevant if `dropout` or\n",
    "    `recurrent_dropout` is used (optional, defaults to `None`).\n",
    "  initial_state: List of initial state tensors to be passed to the first\n",
    "    call of the cell (optional, defaults to `None` which causes creation\n",
    "    of zero-filled initial state tensors).\n",
    "File:           d:\\programfiles\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\n",
    "Type:           type\n",
    "Subclasses:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
