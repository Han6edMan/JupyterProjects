{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.add()\n",
    "***\n",
    "`tf.add(x, y, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "`math.add`支持broadcasting，而`AddN`不支持；更多有关 broadcasting\n",
    "[here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
    "\n",
    "`tf.subtract()`, `tf.divide()`类似\n",
    "\n",
    "__Args__\n",
    "\n",
    "- x: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`类型的`Tensor`\n",
    "\n",
    "- y: 与`x`形状相同的`Tensor`.\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.multiply()\n",
    "tf.square()\n",
    "tf.pow()\n",
    "tf.sqrt()\n",
    "tf.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.constant\n",
    "\n",
    "`tf.constant(value, dtype=None, shape=None, name='Const')`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "Creates a constant tensor from a tensor-like object.\n",
    "\n",
    "Note: All eager `tf.Tensor` values are immutable (in contrast to\n",
    "`tf.Variable`). There is nothing especially _constant_ about the value\n",
    "returned from `tf.constant`. This function it is not fundamentally different\n",
    "from `tf.convert_to_tensor`. The name `tf.constant` comes from the symbolic\n",
    "APIs (like `tf.data` or keras functional models) where the `value` is embeded\n",
    "in a `Const` node in the `tf.Graph`. `tf.constant` is useful for asserting\n",
    "that the value can be embedded that way.\n",
    "\n",
    "**Args**\n",
    "\n",
    "- value: 由任意数据类型构成的标量或张量\n",
    "\n",
    "- dtype: `None`时，输出数据类型由`value`数据类型推断得出；否则将`value`转换为指明的数据类型\n",
    "\n",
    "- shape: `value`为一标量时，则输出为指明`shape`的元素由`value`构成的张量，`value`为张量时，输出`shape`应为*可由原张量 reshape 得到的形状*\n",
    "\n",
    "- name: 张量名称\n",
    "\n",
    "\n",
    "\n",
    "Raises:\n",
    "  TypeError: if shape is incorrectly specified or unsupported.\n",
    "  ValueError: if called on a symbolic tensor.\n",
    "\n",
    "\n",
    "\n",
    "**Type**: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(True, shape=[2, 3])\n",
    "x = tf.constant([True, False], dtype = tf.float32)\n",
    "x = tf.constant([1., 2., 3., 4., 5., 6.], shape=[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于`tf.constant`将值嵌入到了`tf.Graph`，对于符号型张量会抛出异常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> i = tf.keras.layers.Input(shape=[None, None])\n",
    ">>> t = tf.constant(i)\n",
    "Traceback (most recent call last):\n",
    "...\n",
    "NotImplementedError: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.constant` will _always_ create CPU (host) tensors. In order to create\n",
    "tensors on other devices, use `tf.identity`. (If the `value` is an eager\n",
    "Tensor, however, the tensor will be returned unmodified as mentioned above.)\n",
    "\n",
    "Related Ops:\n",
    "\n",
    "* `tf.convert_to_tensor` is similar but:\n",
    "  * It has no `shape` argument.\n",
    "  * Symbolic tensors are allowed to pass through.\n",
    "\n",
    "    >>> i = tf.keras.layers.Input(shape=[None, None])\n",
    "    >>> t = tf.convert_to_tensor(i)\n",
    "\n",
    "* `tf.fill`: differs in a few ways:\n",
    "  *   `tf.constant` supports arbitrary constants, not just uniform scalar\n",
    "      Tensors like `tf.fill`.\n",
    "  *   `tf.fill` creates an Op in the graph that is expanded at runtime, so it\n",
    "      can efficiently represent large tensors.\n",
    "  *   Since `tf.fill` does not embed the value, it can produce dynamically\n",
    "      sized outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.ones()\n",
    "***\n",
    "`tf.ones(shape, dtype=tf.float32, name=None)`\n",
    "\n",
    "__Args__\n",
    "\n",
    "- shape: 当为一标量时，返回一维张量\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "- name: operation的\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5. 5. 5. 5.]\n",
      " [5. 5. 5. 5.]\n",
      " [5. 5. 5. 5.]], shape=(3, 4), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "x = 3 * tf.ones(1, dtype=tf.int8, name=\"test\")\n",
    "x = 5 + tf.zeros([3, 4], dtype=tf.float16, name=None)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.fill()\n",
    "\n",
    "`tf.fill(dims, value, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "与将取值以`Const`节点嵌入计算图的`tf.constant(value, shape=dims)`不同，`tf.fill`在图运行环境中进行衡量，并支持基于其他运行环境的`tf.Tensors`的动态形状(`tf.fill` evaluates at graph runtime and supports dynamic shapes based on\n",
    "other runtime `tf.Tensors`)\n",
    "\n",
    "__Args__\n",
    "\n",
    "- dims: 取标量时仍返回张量\n",
    "\n",
    "- value: 略\n",
    "\n",
    "- name: `tf.Tensor`的\n",
    "\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = 4 * tf.fill(1, value=1.2, name=None)  # ≈ np.fill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.cast()\n",
    "\n",
    "`tf.cast(x, dtype, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "对于`Tensor`，将`x`数据类型转换为`dtype`，对于`SparseTensor`或`IndexedSlices`，将`x.values`数据类型转换为`dtype`。\n",
    "\n",
    "`x`与`dtype`支持的数据类型有`uint8`、`uint16`、`uint32`、`uint64`、`int8`、`int16`、`int32`、`int64`、`float16`、`float32`、`float64`、`complex64`、`complex128`、`bfloat16`。当将虚数转为实数时，只返回实部；当将实数转换为虚数时，虚部为0\n",
    "\n",
    "``name``为运算名称\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 2 2 2]\n",
      " [2 2 2 2]\n",
      " [2 2 2 2]], shape=(3, 4), dtype=int8)\n"
     ]
    }
   ],
   "source": [
    "x = tf.fill([3, 4], 2.99)\n",
    "y = tf.cast(x, tf.int8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.argmax()\n",
    "\n",
    "`tf.argmax(input, axis=None, output_type=tf.int64, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "返回张量各坐标轴上最大值的索引。在数值绑定的情况下，返回值不能保证\n",
    "\n",
    "__Args__\n",
    "\n",
    "- input: 略\n",
    "\n",
    "- axis: 同大多函数的`axis`设定\n",
    "\n",
    "- output_type: 只能为`tf.int32`、`tf.int64`\n",
    "\n",
    "- name: 操作的\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2 0 2 2], shape=(5,), dtype=int64)\n",
      "tf.Tensor([2 2 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8], [14, 45, 23, 5, 27]])\n",
    "print(tf.argmax(x, axis=0))\n",
    "print(tf.argmax(x, axis=1))\n",
    "x = tf.constant([2, 2, 2])\n",
    "print(tf.argmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.GradientTape()\n",
    "Init signature: tf.GradientTape(persistent=False, watch_accessed_variables=True)\n",
    "Docstring:     \n",
    "Record operations for automatic differentiation.\n",
    "\n",
    "Operations are recorded if they are executed within this context manager and\n",
    "at least one of their inputs is being \"watched\".\n",
    "\n",
    "Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n",
    "where `trainable=True` is default in both cases) are automatically watched.\n",
    "Tensors can be manually watched by invoking the `watch` method on this context\n",
    "manager.\n",
    "\n",
    "For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n",
    "be computed as:\n",
    "\n",
    "```python\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, x) # Will compute to 6.0\n",
    "```\n",
    "\n",
    "GradientTapes can be nested to compute higher-order derivatives. For example,\n",
    "\n",
    "```python\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  with tf.GradientTape() as gg:\n",
    "    gg.watch(x)\n",
    "    y = x * x\n",
    "  dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n",
    "d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n",
    "```\n",
    "\n",
    "By default, the resources held by a GradientTape are released as soon as\n",
    "GradientTape.gradient() method is called. To compute multiple gradients over\n",
    "the same computation, create a persistent gradient tape. This allows multiple\n",
    "calls to the gradient() method as resources are released when the tape object\n",
    "is garbage collected. For example:\n",
    "\n",
    "```python\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
    "dy_dx = g.gradient(y, x)  # 6.0\n",
    "del g  # Drop the reference to the tape\n",
    "```\n",
    "\n",
    "By default GradientTape will automatically watch any trainable variables that\n",
    "are accessed inside the context. If you want fine grained control over which\n",
    "variables are watched you can disable automatic tracking by passing\n",
    "`watch_accessed_variables=False` to the tape constructor:\n",
    "\n",
    "```python\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(variable_a)\n",
    "  y = variable_a ** 2  # Gradients will be available for `variable_a`.\n",
    "  z = variable_b ** 3  # No gradients will be available since `variable_b` is\n",
    "                       # not being watched.\n",
    "```\n",
    "\n",
    "Note that when using models you should ensure that your variables exist when\n",
    "using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n",
    "first iteration not have any gradients:\n",
    "\n",
    "```python\n",
    "a = tf.keras.layers.Dense(32)\n",
    "b = tf.keras.layers.Dense(32)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n",
    "                           # `a.variables` will return an empty list and the\n",
    "                           # tape will not be watching anything.\n",
    "  result = b(a(inputs))\n",
    "  tape.gradient(result, a.variables)  # The result of this computation will be\n",
    "                                      # a list of `None`s since a's variables\n",
    "                                      # are not being watched.\n",
    "```\n",
    "\n",
    "Note that only tensors with real or complex dtypes are differentiable.\n",
    "Init docstring:\n",
    "Creates a new GradientTape.\n",
    "\n",
    "Args:\n",
    "  persistent: Boolean controlling whether a persistent gradient tape\n",
    "    is created. False by default, which means at most one call can\n",
    "    be made to the gradient() method on this object.\n",
    "  watch_accessed_variables: Boolean controlling whether the tape will\n",
    "    automatically `watch` any (trainable) variables accessed while the tape\n",
    "    is active. Defaults to True meaning gradients can be requested from any\n",
    "    result computed in the tape derived from reading a trainable `Variable`.\n",
    "    If False users must explicitly `watch` any `Variable`s they want to\n",
    "    request gradients from.\n",
    "File:           d:\\programmefiles\\python\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\n",
    "Type:           type\n",
    "Subclasses:     LossScaleGradientTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.convert_to_tensor()\n",
    "\n",
    "`tf.convert_to_tensor(value, dtype=None, dtype_hint=None, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "Converts the given `value` to a `Tensor`.\n",
    "\n",
    "该函数将各种类型的 Python 对象转化为`Tensor`类型的对象，其支持`Tensor`对象、Numpy数组、Python 列表、Python 标量。当`float`和`string`类型的 Python 列表或标量中出现`None`时，与 Numpy 将`None`转换为数值的行为不同，该函数会抛出异常\n",
    "\n",
    "**Args**\n",
    "\n",
    "- value: 满足其类型有已注册的`Tensor`的转换函数的对象(An object whose type has a registered `Tensor` conversion function)\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "- dtype_hint: 用于`dtype`为`None`时；在某些情况下，调用者在转换为张量时可能没有想到dtype，所以dtype_hint可以用作一个软首选项；若目标转换类型无法转换，则该参数等同于无效\n",
    "\n",
    "- name: 张量名称\n",
    "\n",
    "**Type**\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12)\n",
    "y = tf.convert_to_tensor(x)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_min()\n",
    "\n",
    "`tf.reduce_min(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "等价于`np.min()`\n",
    "\n",
    "Reduces `input_tensor` along the dimensions given in `axis`.\n",
    "Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
    "entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
    "are retained with length 1.\n",
    "\n",
    "If `axis` is None, all dimensions are reduced, and a\n",
    "tensor with a single element is returned.\n",
    "\n",
    "__Args__\n",
    "\n",
    "- input_tensor: 应为实数类型张量\n",
    "\n",
    "- axis, keepdims: 略\n",
    "\n",
    "- name: operation的\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[1, 3, 4, 0, 3], [2, 5, 1, 8, 2]])\n",
    "y = tf.reduce_min(x, axis=0)  # == np.min()\n",
    "y = tf.reduce_max(x, axis=1)  # == np.max()\n",
    "y = tf.reduce_sum(x, axis=0)  # == np.sum()\n",
    "y = tf.reduce_mean(x, axis=0)  # == np.mean()\n",
    "print(y)\n",
    "y = tf.reduce_prod(x, axis=0)  # == np.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_any\n",
    "\n",
    "`tf.reduce_any(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "沿张量某一维度执行`or`计算\n",
    "\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "x = tf.constant([[True,  True], [False, False]])\n",
    "tf.reduce_any(x)  # True\n",
    "tf.reduce_any(x, 0)  # [True, True]\n",
    "tf.reduce_any(x, 1)  # [True, False]\n",
    "```\n",
    "\n",
    "__Args__\n",
    "\n",
    "- input_tensor: 布尔型张量\n",
    "\n",
    "- axis, keepdims, name: 略\n",
    "\n",
    "__Type__\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(\n",
    "    [[ True, False, True, False, False, True],\n",
    "     [False, True, True, False, False, True]])\n",
    "y = tf.reduce_any(x, axis=0)  # == np.any()\n",
    "print(y)\n",
    "y = tf.reduce_all(x, axis=0)  # == np.all()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.reduce_logsumexp()\n",
    "\n",
    "`tf.reduce_logsumexp(input_tensor, axis=None, keepdims=False, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "计算\n",
    "\n",
    "$$\\ln \\left( \\sum\\exp(input[shape[i]])\\right)$$\n",
    "\n",
    "该函数比直接计算$\\ln\\left(\\sum\\exp(input)\\right)$要稳定，其避免了指数函数和对数函数的溢出问题. \n",
    "\n",
    "__Args__: 略\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[0., 0., 0.], [0., 0., 0.]])\n",
    "y = tf.reduce_logsumexp(x)  # log(6)\n",
    "y = tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]\n",
    "y = tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]\n",
    "y = tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]\n",
    "y = tf.reduce_logsumexp(x, [0, 1])  # log(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.one_hot()\n",
    "\n",
    "```python\n",
    "tf.one_hot(\n",
    "    indices,\n",
    "    depth,\n",
    "    on_value=None,\n",
    "    off_value=None,\n",
    "    axis=None,\n",
    "    dtype=None,\n",
    "    name=None,\n",
    ")\n",
    "```\n",
    "__Docstring__\n",
    "\n",
    "Returns a one-hot tensor.\n",
    "\n",
    "`indices`中由索引表示的位置取值为`on_value`，而其他所有位置取值为`off_value`，若期望输出为非数字类型变量，则`on_value`和`off_value`必须指明。\n",
    "\n",
    "- `indices`为标量时，输出为长度为`depth`的向量；\n",
    "\n",
    "- 若`indices`为长度为`features`的向量，则输出形状\n",
    "\n",
    "    - 在axis = -1时为 $features \\times depth$\n",
    "\n",
    "    - 在axis = 0时为 $depth \\times features$\n",
    "\n",
    "- 若`indices`是形状为`[batch, features]`的矩阵，则输出形状\n",
    "    \n",
    "    - 在`axis == -1`时为 $batch \\times features \\times depth$\n",
    "\n",
    "    - 在`axis == 1`时为 $batch \\times depth \\times features$\n",
    "\n",
    "    - 在`axis == 0`时为 $batch \\times batch \\times features$\n",
    "\n",
    "- 若`indices`为不规则张量，`axis`必须为正且需指代一个规则的维度，结果等价于将`one_hot`应用在不规则张量的取值上，并生成一个新的不规则张量\n",
    "\n",
    "__Args__\n",
    "\n",
    "- indices: 内容代表索引的`Tensor`；若`indices`的秩为 N，则输出数组的秩为 N+1，新的维度会添加在`axis`指明位置中；\n",
    "\n",
    "- depth: 定义 one hot 维度的标量\n",
    "\n",
    "- on_value: 输出结果中，当`indices[j] = i`时的取值，默认为 1，需与`off_value`和`dtype`数据类型匹配\n",
    "\n",
    "- off_value: 输出结果中，当`indices[j] != i`时的取值，默认为 0，需与`on_value`和`dtype`数据类型匹配\n",
    "\n",
    "- axis: 在shape中添加索引的位置，默认 -1\n",
    "\n",
    "- dtype: 默认为`tf.float32`，需与`on_value`和`off_value`匹配\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.one_hot(indices=3, depth=7, on_value=6, off_value=-2, dtype=tf.int8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    axis=0时返回(6,4)数组，对于indices的元素“2”来说，其之前的索引为(1)\n",
    "    则将索引“2”添加到新索引(x,y)的索引为0的位置，即x的位置，进而得到新索引(2,1)\n",
    "'''\n",
    "classes = 6\n",
    "targets = [1, 2, 4, 3]\n",
    "x = tf.one_hot(indices=targets, depth=classes, axis=0)\n",
    "print(x)\n",
    "'''\n",
    "    axis=0时返回(6,4)数组，对于indices的元素“2”来说，其之前的索引为(1)\n",
    "    则将索引“2”添加到新索引(x,y)的索引为1的位置，即y的位置，进而得到新索引(1,2)\n",
    "''' \n",
    "y = tf.one_hot(indices=[5, 2, 4, 3], depth=6, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    axis=0时返回(5,2,3)数组，对于indices的元素“4”来说，其之前的索引为(0,2)\n",
    "    则将索引“4”添加到新索引(x,y,z)的索引为0的位置，即x的位置，进而得到新索引(4,0,2)\n",
    "''' \n",
    "x = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=0)\n",
    "y = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=1)\n",
    "z = tf.one_hot(indices=[[0, 2, 4], [1, 0, 3]], depth=5, axis=2)\n",
    "print(x, y, z, sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], [[0.0, 0.0, 1.0]]]>\n"
     ]
    }
   ],
   "source": [
    "indices = tf.ragged.constant([[0, 1], [2]])  # (2, 1)\n",
    "x = tf.one_hot(indices=indices, depth=3)  # output: (2, None, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.where()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'bool'>\n"
     ]
    }
   ],
   "source": [
    "tf.bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
