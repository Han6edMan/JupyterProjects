{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.data.Dataset()\n",
    "`tf.data.Dataset(variant_tensor)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "创建一个`DatasetV2`对象，其表示一个可能含有很多元素的数据集；`tf.data.Dataset` 支持编写高效的描述性的 input pipeline，其使用遵循以下模式：\n",
    "1. 从输入数据创建源数据集；例如通过一个`list`对象创建`data.Dataset.from_tensor_slices([1, 2, 3])`，通过文件内容创建`data.TextLineDataset([\"file1.txt\", \"file2.txt\"])`，通过`TFRecord`文件创建`data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])`，创建与某一模式匹配的所有文件的生成数据集`data.Dataset.list_files(\"/path/*.txt\")`；更多创建数据集的方式详见`tf.data.FixedLengthRecordDataset`、`tf.data.Dataset.from_generator`\n",
    "\n",
    "\n",
    "2. 通过数据集进行各种变换来对数据进行预处理，例如`dataset = dataset.map(lambda x: x*2)`\n",
    "\n",
    "\n",
    "3. 遍历数据集以处理数据元素\n",
    "\n",
    "其中遍历以数据流的模式进行，进而整个数据集不必小于内存容量；\n",
    "\n",
    "**Args**:\n",
    "\n",
    "- variant_tensor: 表示某一数据集的`DT_VARIANT`张量\n",
    "\n",
    "\n",
    "**Common Terms**:\n",
    "\n",
    "- Element: 在数据集迭代器上调用`next()`得到的输出变量，其可以是包含了元祖、字典`namedtuple`等具有嵌套结构的对象\n",
    "\n",
    "- Component：指`element`嵌套结构中的每一个叶节点，可以是`tf.TypeSpec`所表示的任何类型，例如`tf.Tensor`、`tf.data.Dataset`、`tf.SparseTensor`、`tf.RaggedTensor`、`tf.TensorArray`\n",
    "\n",
    "**File**:   \\tensorflow\\python\\data\\ops\\dataset_ops.py\n",
    "\n",
    "**Type**:           ABCMeta\n",
    "\n",
    "**Subclasses**:     DatasetV1, DatasetSource, UnaryDataset, \\_VariantDataset, ZipDataset, ConcatenateDataset, TFRecordDatasetV2, \\_DirectedInterleaveDataset, \\_PerDeviceGenerator, \\_ReincarnatedPerDeviceGenerator, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices(tensors)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "`tensors`为张量构成的数据集，所有张量第一维形状必须相同；该函数沿着它们的第一维度将`tensors`进行切片；这个操作保留了各张量的结构，移除了每个张量的第一个维数，并使用它作为数据集维数；\n",
    "\n",
    "注意：若`tensors`包含 Numpy 数组并且预期的操作无法执行，这些值会作为一个或多个`tf.constant`嵌入至计算图中；对于大型数据集(> 1 GB)，这可能会浪费内存并遇到图形序列化的字节限制；若`tensors`包含一个或多个大型 Numpy 数组，须考虑另外的可行方案[this guide](\n",
    "https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
    "\n",
    "__Type__: function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a 1D tensor produces scalar tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for k, v in dataset.__dict__.items():\n",
    "    print(k, v, sep=\"\\n\")\n",
    "    print()\n",
    "print(list(dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a 2D tensor produces 1D tensor elements.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [3, 4, 5], [4, 5, 6]])\n",
    "print(dataset._tensors)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing a tuple of 1D tensors produces tuple elements containing\n",
    "dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3], [3, 4, 5], [5, 6, 7]))\n",
    "print(dataset._tensors, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary structure is also preserved.\n",
    "dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2, 3], \"b\": [3, 4, 5]})\n",
    "print(dataset._tensors, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "for data in dataset:\n",
    "    print(data, end=\"\\n\\n\")\n",
    "for k, v in dataset.__dict__.items():\n",
    "    print(k, v, sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3])>, <tf.Tensor: shape=(), dtype=string, numpy=b'A'>)\n",
      "\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1])>, <tf.Tensor: shape=(), dtype=string, numpy=b'B'>)\n",
      "\n",
      "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3])>, <tf.Tensor: shape=(), dtype=string, numpy=b'A'>)\n",
      "\n",
      "(array([1, 3]), b'A')\n",
      "\n",
      "(array([2, 1]), b'B')\n",
      "\n",
      "(array([3, 3]), b'A')\n",
      "\n",
      "[<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "array([[1, 3],\n",
      "       [2, 1],\n",
      "       [3, 3]])>, <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'A', b'B', b'A'], dtype=object)>]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors can be combined into one Dataset object.\n",
    "features = tf.constant([[1, 3], [2, 1], [3, 3]])\n",
    "labels = tf.constant(['A', 'B', 'A'])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "\n",
    "for data in dataset:\n",
    "    print(data, end=\"\\n\\n\")\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element, end=\"\\n\\n\")\n",
    "print(dataset._tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([1, 3]), b'A'), (array([2, 1]), b'B'), (array([3, 3]), b'A')]\n",
      "\n",
      "(array([1, 3]), b'A')\n",
      "(array([2, 1]), b'B')\n",
      "(array([3, 3]), b'A')\n"
     ]
    }
   ],
   "source": [
    "# Both the features and the labels tensors can be converted to a Dataset object separately and combined after.\n",
    "features_dataset = Dataset.from_tensor_slices(features)\n",
    "labels_dataset = Dataset.from_tensor_slices(labels)\n",
    "dataset = Dataset.zip((features_dataset, labels_dataset))\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1, 3],\n",
      "       [2, 3]]), array([[b'A'],\n",
      "       [b'A']], dtype=object))\n",
      "(array([[2, 1],\n",
      "       [1, 2]]), array([[b'B'],\n",
      "       [b'B']], dtype=object))\n",
      "(array([[3, 3],\n",
      "       [3, 2]]), array([[b'A'],\n",
      "       [b'B']], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "# A batched feature and label set can be converted to a Dataset in similar fashion.\n",
    "batched_features = tf.constant([[[1, 3], [2, 3]],\n",
    "                                [[2, 1], [1, 2]],\n",
    "                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
    "batched_labels = tf.constant([['A', 'A'],\n",
    "                              ['B', 'B'],\n",
    "                              ['A', 'B']], shape=(3, 2, 1))\n",
    "dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
