{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgrammeFiles\\python\\Anaconda3\\envs\\tensorflow2.2\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow as tf2\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 编程模型\n",
    "\n",
    "　　程序开发过程中，根据解决问题的不同思路，通常会采取不同的编程方式。常见的编程方式有两种，即命令式编程 imperative programming 和声明式编程 declarative programming；前者更关注程序执行的具体步骤，计算机按照代码中的顺序一步步执行具体运算，常用于交互式界面程序或操作系统中；后者则先告诉计算机想要达成的目标，但不指定具体的实现步骤，只通过函数及推论规则等来描述数据之间的关系，常用于深度学习的算法实现。TensorFlow 即使用了声明式编程的方式。\n",
    "\n",
    "　　TensorFlow 使用**计算图** Computational [Graph](#tf.Graph())来描述机器学习算法的计算过程，其中各种计算被定义为**操作** [Operation](#tf.Operation())，所有数据被视作**张量** [Tensor](#tf.Tensor())，张量在计算图的节点之间传递；**TensorFlow 程序可简单的概括为构建计算图和执行计算图两部分**，即在构建计算图之后并不执行具体的运算，而具体的运算需通过定义**会话** [Session](#tf.Session())来执行；对于计算图中类似于模型参数这样的有状态参数（如需要学习的权重等），TensorFlow 会以**变量** [Variable](#tf.Variable()) 的形式表示，而对于一些不可变的参数，则以**常量 [constant](#tf.constant())** 的形式表示；此外 TensorFlow 还提供了**队列 Queue**机制来处理数据读取和计算图的异步执行等功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Graph()\n",
    "\n",
    "`tf.Graph()`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "创建一个表示 TensorFlow 计算时数据流的空的有向图，即计算图。\n",
    "\n",
    "计算图通过众多`tf.function`来表示函数运算，每个计算图由一系列`tf.Operation`和`tf.Tensor`的节点组成，前者表示计算单元，后者表示计算之间流动的数据；计算图中的边也有两种类型，一种代表数据流动的方向，另一种则是单纯代表节点执行顺序；默认计算图则可以使用`tf.Graph.as_default`的上下文管理器来注册，随后计算等操作将被添加到计算图中，但并不立即执行；默认计算图也可以通过`tf.compat.v1.get_default_graph()`获得；一个`tf.Graph`实例支持任意数量的“collection”（有关 collection 详见`colleciton`），这些 collection 通过名称进行标识；在构建大型图时，为了方便起见，一个集合可以存储许多相关对象，如`tf.Variable`类会使用`tf.GraphKeys.GLOBAL_VARIABLES`集合来表示在计算图构造期间创建的所有变量；使用者可以通过指定新名称来定义其他集合；\n",
    "\n",
    "通过计算图来执行计算，可以方便的保留中间节点的计算结果，进而有助于反向传播时链式法则求导；\n",
    "\n",
    "TensorFlow 1中`tf.Graph`可以在不使用`tf.function`情况下直接构建和使用，但在 TensorFlow 2 中这种使用方式已经被摒弃，进而更建议通过`tf.function`来构建。如果直接使用`tf.Graph`，则还应使用其他 TensorFlow 1 的类来执行该计算图，如`tf.compat.v1.Session`等；\n",
    "\n",
    "**note**: `tf.Graph`类对于计算图构造来说不是线程安全的；所有操作都应该从单个线程创建，否则必须提供外部同步；除非另有说明，否则所有方法都不是线程安全的；\n",
    "\n",
    "**Type**\n",
    "\n",
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.constant(30.0)\n",
    "    print(x.graph == g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.constant(3, name=\"x\")\n",
    "y = tf.constant(2, name=\"y\")\n",
    "z = tf.add(x, y, name=\"add_x_y\")\n",
    "writer = tf.summary.FileWriter(\"../009_TensorBoard/graphs/test1\", tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.function()\n",
    "```python\n",
    "tf.function(\n",
    "    func=None,\n",
    "    input_signature=None,\n",
    "    autograph=True,\n",
    "    experimental_implements=None,\n",
    "    experimental_autograph_options=None,\n",
    "    experimental_relax_shapes=False,\n",
    "    experimental_compile=None,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "Compiles a function into a callable TensorFlow graph.\n",
    "\n",
    "`tf.function`会构造一个执行 TensorFlow 计算图的可调用函数，这个计算图是通过跟踪编译`func`中 TensorFlow 的操作而创建的，这样能够高效地将`func`作为 TensorFlow 计算图来执行\n",
    "\n",
    "`tf.function` constructs a callable that executes a TensorFlow graph (`tf.Graph`) created by trace-compiling the TensorFlow operations in `func`, effectively executing `func` as a TensorFlow graph.\n",
    "\n",
    "\n",
    "Example usage:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x, y):\n",
    "...   return x ** 2 + y\n",
    ">>> x = tf.constant([2, 3])\n",
    ">>> y = tf.constant([3, -2])\n",
    ">>> f(x, y)\n",
    "<tf.Tensor: ... numpy=array([7, 7], ...)>\n",
    "\n",
    "_Features_\n",
    "\n",
    "`func` may use data-dependent control flow, including `if`, `for`, `while`\n",
    "`break`, `continue` and `return` statements:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   if tf.reduce_sum(x) > 0:\n",
    "...     return x * x\n",
    "...   else:\n",
    "...     return -x // 2\n",
    ">>> f(tf.constant(-2))\n",
    "<tf.Tensor: ... numpy=1>\n",
    "\n",
    "`func`'s closure may include `tf.Tensor` and `tf.Variable` objects:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f():\n",
    "...   return x ** 2 + y\n",
    ">>> x = tf.constant([-2, -3])\n",
    ">>> y = tf.Variable([3, -2])\n",
    ">>> f()\n",
    "<tf.Tensor: ... numpy=array([7, 7], ...)>\n",
    "\n",
    "`func` may also use ops with side effects, such as `tf.print`, `tf.Variable`\n",
    "and others:\n",
    "\n",
    ">>> v = tf.Variable(1)\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   for i in tf.range(x):\n",
    "...     v.assign_add(i)\n",
    ">>> f(3)\n",
    ">>> v\n",
    "<tf.Variable ... numpy=4>\n",
    "\n",
    "Important: Any Python side-effects (appending to a list, printing with\n",
    "`print`, etc) will only happen once, when `func` is traced. To have\n",
    "side-effects executed into your `tf.function` they need to be written\n",
    "as TF ops:\n",
    "\n",
    ">>> l = []\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   for i in x:\n",
    "...     l.append(i + 1)    # Caution! Will only happen once when tracing\n",
    ">>> f(tf.constant([1, 2, 3]))\n",
    ">>> l\n",
    "[<tf.Tensor ...>]\n",
    "\n",
    "Instead, use TensorFlow collections like `tf.TensorArray`:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "...   for i in range(len(x)):\n",
    "...     ta = ta.write(i, x[i] + 1)\n",
    "...   return ta.stack()\n",
    ">>> f(tf.constant([1, 2, 3]))\n",
    "<tf.Tensor: ..., numpy=array([2, 3, 4], ...)>\n",
    "\n",
    "_`tf.function` is polymorphic_\n",
    "\n",
    "Internally, `tf.function` can build more than one graph, to support arguments\n",
    "with different data types or shapes, since TensorFlow can build more\n",
    "efficient graphs that are specialized on shapes and dtypes. `tf.function`\n",
    "also treats any pure Python value as opaque objects, and builds a separate\n",
    "graph for each set of Python arguments that it encounters.\n",
    "\n",
    "To obtain an individual graph, use the `get_concrete_function` method of\n",
    "the callable created by `tf.function`. It can be called with the same\n",
    "arguments as `func` and returns a special `tf.Graph` object:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   return x + 1\n",
    ">>> isinstance(f.get_concrete_function(1).graph, tf.Graph)\n",
    "True\n",
    "\n",
    "Caution: Passing python scalars or lists as arguments to `tf.function` will\n",
    "always build a new graph. To avoid this, pass numeric arguments as Tensors\n",
    "whenever possible:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   return tf.abs(x)\n",
    ">>> f1 = f.get_concrete_function(1)\n",
    ">>> f2 = f.get_concrete_function(2)  # Slow - builds new graph\n",
    ">>> f1 is f2\n",
    "False\n",
    ">>> f1 = f.get_concrete_function(tf.constant(1))\n",
    ">>> f2 = f.get_concrete_function(tf.constant(2))  # Fast - reuses f1\n",
    ">>> f1 is f2\n",
    "True\n",
    "\n",
    "Python numerical arguments should only be used when they take few distinct\n",
    "values, such as hyperparameters like the number of layers in a neural network.\n",
    "\n",
    "_Input signatures_\n",
    "\n",
    "For Tensor arguments, `tf.function` instantiates a separate graph for every\n",
    "unique set of input shapes and datatypes. The example below creates two\n",
    "separate graphs, each specialized to a different shape:\n",
    "\n",
    ">>> @tf.function\n",
    "... def f(x):\n",
    "...   return x + 1\n",
    ">>> vector = tf.constant([1.0, 1.0])\n",
    ">>> matrix = tf.constant([[3.0]])\n",
    ">>> f.get_concrete_function(vector) is f.get_concrete_function(matrix)\n",
    "False\n",
    "\n",
    "An \"input signature\" can be optionally provided to `tf.function` to control\n",
    "the graphs traced. The input signature specifies the shape and type of each\n",
    "Tensor argument to the function using a `tf.TensorSpec` object. More general\n",
    "shapes can be used. This is useful to avoid creating multiple graphs when\n",
    "Tensors have dynamic shapes. It also restricts the shape and datatype of\n",
    "Tensors that can be used:\n",
    "\n",
    ">>> @tf.function(\n",
    "...     input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
    "... def f(x):\n",
    "...   return x + 1\n",
    ">>> vector = tf.constant([1.0, 1.0])\n",
    ">>> matrix = tf.constant([[3.0]])\n",
    ">>> f.get_concrete_function(vector) is f.get_concrete_function(matrix)\n",
    "True\n",
    "\n",
    "_Variables may only be created once_\n",
    "\n",
    "`tf.function` only allows creating new `tf.Variable` objects when it is called\n",
    "for the first time:\n",
    "\n",
    ">>> class MyModule(tf.Module):\n",
    "...   def __init__(self):\n",
    "...     self.v = None\n",
    "...\n",
    "...   @tf.function\n",
    "...   def call(self, x):\n",
    "...     if self.v is None:\n",
    "...       self.v = tf.Variable(tf.ones_like(x))\n",
    "...     return self.v * x\n",
    "\n",
    "In general, it is recommended to create stateful objects like `tf.Variable`\n",
    "outside of `tf.function` and passing them as arguments.\n",
    "\n",
    "**Args**\n",
    "\n",
    "- func: 要编译的函数；若`func`为 None，`tf.function`返回一个 decorator，它可以通过参数`func`调用，即`tf.function(input_signature=...)(func)`和`tf.function(func, input_signature=...)`是等价的，前者可以用作 decorator.\n",
    "\n",
    "- input_signature: 一个可能具有嵌套结构的`tf.TensorSpec`对象序列，该序列指定提供给函数的张量的形状和 dtype；如果为`None`，则为每个 inferred input signature 单独实例化一个函数。若指明了`input_signature`，`func`的每个输入必须是一个`Tensor`，且`func`不能接受`**kwargs`\n",
    "\n",
    "- autograph: Whether autograph should be applied on `func` before tracing a\n",
    "    graph. Data-dependent control flow requires `autograph=True`. For more\n",
    "    information, see the [tf.function and AutoGraph guide](https://www.tensorflow.org/guide/function)\n",
    "\n",
    "- experimental_implements: If provided, contains a name of a \"known\" function\n",
    "    this implements. For example \"mycompany.my_recurrent_cell\".\n",
    "    This is stored as an attribute in inference function,\n",
    "    which can then be detected when processing serialized function.\n",
    "    See [standardizing composite ops](https://github.com/tensorflow/community/blob/master/rfcs/20190610-standardizing-composite_ops.md)  # pylint: disable=line-too-long\n",
    "    for details.  For an example of utilizing this attribute see this\n",
    "    [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/lite/transforms/prepare_composite_functions_tf.cc)\n",
    "    The code above automatically detects and substitutes function that\n",
    "    implements \"embedded_matmul\" and allows TFLite to substitute its own\n",
    "    implementations. For instance, a tensorflow user can use this\n",
    "     attribute to mark that their function also implements\n",
    "    `embedded_matmul` (perhaps more efficiently!)\n",
    "    by specifying it using this parameter:\n",
    "    `@tf.function(experimental_implements=\"embedded_matmul\")`\n",
    "  experimental_autograph_options: Optional tuple of\n",
    "    `tf.autograph.experimental.Feature` values.\n",
    "  experimental_relax_shapes: When True, `tf.function` may generate fewer,\n",
    "    graphs that are less specialized on input shapes.\n",
    "  experimental_compile: If True, the function is always compiled by\n",
    "    [XLA](https://www.tensorflow.org/xla). XLA may be more efficient in some\n",
    "    cases (e.g. TPU, XLA_GPU, dense tensor computations).\n",
    "\n",
    "Returns:\n",
    "   If `func` is not None, returns a callable that will execute the compiled\n",
    "   function (and return zero or more `tf.Tensor` objects).\n",
    "   If `func` is None, returns a decorator that, when invoked with a single\n",
    "   `func` argument, returns a callable equivalent to the case above.\n",
    "\n",
    "Raises:\n",
    "   ValueError when attempting to use experimental_compile, but XLA support is\n",
    "   not enabled.\n",
    "\n",
    "**Type**\n",
    "\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.function()\n",
    "tf2.function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Operation()\n",
    "```python\n",
    "tf.Operation(\n",
    "    node_def,\n",
    "    g,\n",
    "    inputs=None,\n",
    "    output_types=None,\n",
    "    control_inputs=None,\n",
    "    input_types=None,\n",
    "    original_op=None,\n",
    "    op_def=None,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "`Operation`是`tf.Graph`中对张量执行计算的一个节点，其输入输出均可以是 0 或多个`Tensor`，该对象可以在`tf.function`内或`tf.Graph.as_default`的上下文管理器中通过调用 Python 的 op 构造函数来创建，例如在`tf.function`中，`c = tf.matmul(a, b)`创建了一个`MatMul`类型的`Operation`；其构造函数会验证以`node_def.name`形式传递的`Operation`的名称，有效的`Operation`名称应与表达式`[A-Za-z0-9.][A-Za-z0-9_.\\\\-/]*`匹配\n",
    "\n",
    "若调用了`tf.Session`，可以将`tf.Graph`中的`Operation`传递给`tf.Session.run`来执行；`op.run()`是调用`tf.get_default_session().run(op)`的简便形式\n",
    "\n",
    "有关`tf.function`参见 [here](#tf.function())\n",
    "\n",
    "**Args**\n",
    "\n",
    "- node_def: 代表此操作的`node_def_pb2.NodeDef`，用于`node_def_pb2.NodeDef`属性，通常是`name`、`op`和`device`；`input`属性在这里无关紧要，因为它会在生成模型时计算\n",
    "    \n",
    "- g: `Graph`，包含该操作的计算图\n",
    "\n",
    "- inputs: `Tensor`对象组成的列表，`Operation`的输入\n",
    "\n",
    "- output_types: `DType`对象组成的列表，即该操作输出的`Tensor`的类型组成的列表。这个列表的长度表示`Operation`的 output endpoints 数量\n",
    "\n",
    "- control_inputs: 与之有控制依赖关系的`Operation`或`Tensor`组成的列表\n",
    "\n",
    "- input_types: 该操作所接收的张量的`DType`对象组成的列表，默认使用`[x.dtype.base_dtype for x in inputs]`，期望是同特定类型变量的操作必须显式地指定这些输入，其与`inputs`不兼容会报错\n",
    "\n",
    "- original_op: Optional. 用于将新声明的`Operation`与现有的`Operation`相关联，如 a replica with the op that was replicated).\n",
    "\n",
    "- op_def: `op_def_pb2.OpDef`协议缓冲区，它描述了此`Operation`所表示的操作类型\n",
    "\n",
    "**File**\n",
    "\n",
    "... \\tensorflow\\python\\framework\\ops.py\n",
    "\n",
    "**Type**\n",
    "\n",
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='x')\n",
    "y = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='y')\n",
    "add = tf.matmul(x, y, name='add')\n",
    "dft_g = tf.get_default_graph()\n",
    "operations = dft_g.get_operations()\n",
    "operation_add = dft_g.get_operation_by_name(\"add\")  # equivalent to operations[2]\n",
    "\n",
    "print(operations)  # => [<tf.Operation 'x' type=Const>, <tf.Operation 'y' type=Const>, <tf.Operation 'add' type=MatMul>]\n",
    "print(type(operation_add))  # => <class 'tensorflow.python.framework.ops.Operation'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Tensor()\n",
    "\n",
    "`tf.Tensor(op, value_index, dtype)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "\n",
    "TensorFlow 程序运行时操作和传递的主要对象是`tf.Tensor`，`tf.Tensor`对象可以代表一任意维数的矩形数组；TensorFlow 可以在不立即执行的情况下定义计算，最常见的是在`tf.function`及计算图模式中，在这些情况下，张量的秩和每个维度的大小可能只是部分已知的；如果操作输入的形状是已知的，则大多操作会产生形状已知的张量，但在某些情况下，只能在计算图执行时确定一个张量的具体形状。\n",
    "\n",
    "有一些专用的张量，参见`tf.Variable`, `tf.constant`, `tf.placeholder`, `tf.SparseTensor`, `tf.RaggedTensor`；更多有关`Tensor`信息参见[guide](https://tensorflow.org/guide/tensor) 及 [here](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor)\n",
    "\n",
    "**Args**\n",
    "\n",
    "- op: 一个计算此张量的`Operation`对象，不为`Operation`对象时会抛出 TypeError 异常\n",
    "\n",
    "- value_index: `int`. Index of the operation's endpoint that produces this tensor\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "**Type**\n",
    "\n",
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "const = tf.constant(2, shape=[1], name=\"const\")\n",
    "plhd = tf.placeholder(tf.float32, [1], name=\"placeholder\")\n",
    "print(const)\n",
    "print(plhd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.constant()\n",
    "```python\n",
    "tf.constant(\n",
    "    value,\n",
    "    dtype=None,\n",
    "    shape=None,\n",
    "    name='Const',\n",
    "    verify_shape=False,\n",
    ")\n",
    "```\n",
    "\n",
    "返回一个常量`Tensor`对象，如果形状指定不正确或不被支持，则抛出`TypeError`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- value: 具有输出类型`dtype`的常数值或列表，`value`是列表时，其长度须小于或等于`shape`所隐含的元素数量(如果指定了的话)，列表长度小于`shape`指定元素数量时，列表中的最后一个元素将用于填充其余空位\n",
    "\n",
    "- dtype, name: 略\n",
    "\n",
    "- shape: 指明时会将`value` reshape 成指明形状；否则使用`value`形状\n",
    "\n",
    "- verify_shape: 常量的形状是否可以被更改，默认不可更改\n",
    "\n",
    "\n",
    "`tf.constant`与`tf.fill`在某些方面是不同的：\n",
    "\n",
    "- `tf.constant`支持任意常数，而不像`tf.fill`仅支持 uniform scalar Tensors\n",
    "\n",
    "- `tf.constant`会在计算图中创建一个`Const`节点，该节点在图构建时便具有确切的值；而`tf.fill` 在计算图中创建一个操作，这个操作在运行时会展开\n",
    "\n",
    "- 由于`tf.constant`只在计算图中嵌入了常量值，进而它不支持 dynamic shapes based on other runtime Tensors，而`tf.fill`是支持的\n",
    "\n",
    "\n",
    "**Type**\n",
    "\n",
    "function\n",
    "\n",
    "**Example**\n",
    "\n",
    "由`tf.constant`创建的常量返回类型为`tf.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x0:0\", shape=(2, 3), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x0 = tf.constant([1.0, -1.0], shape=[2, 3], name=\"x0\")\n",
    "x1 = tf.constant([1.0, 2., 3., 4., 5., 6.], shape=[2, 3], name=\"x1\")\n",
    "add = tf.add(x0, x1, name=\"my_add\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(add)\n",
    "    print(x0)\n",
    "    print(type(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.constant`也处理为`tf.Operation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_g = tf.get_default_graph()\n",
    "operation = dft_g.get_operations()\n",
    "print(operation)\n",
    "print(operation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的`constant`如`tf.zeros`, `tf.zeros_like`, `tf.ones`, `tf.ones_like`, `tf.fill`, `tf.lin_space`, ``tf.range``, `tf.random_x`等，详见[here](http://localhost:8888/notebooks/Help_Viewers/006_TensorFlow_1.x/tf_1.Tensor.ipynb#tf.ones())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Variable()\n",
    "```python\n",
    "tf.Variable(\n",
    "    initial_value=None,\n",
    "    trainable=True,\n",
    "    collections=None,\n",
    "    validate_shape=True,\n",
    "    caching_device=None,\n",
    "    name=None,\n",
    "    variable_def=None,\n",
    "    dtype=None,\n",
    "    expected_shape=None,\n",
    "    import_scope=None)\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "官方教程：[Variables Guide](https://tensorflow.org/guide/variables).\n",
    "\n",
    "变量 variable 会在调用`run()`的整个过程中保留其曾经经历过的所有状态，一般通过实例化`Variable`类来向计算图中添加变量；\n",
    "\n",
    "这个构造函数通过创建`variable`操作和一个`assign`操作来将变量设置为初始值\n",
    "\n",
    "**Args**\n",
    "\n",
    "- initial_value:`Tensor`类型或可转换为`Tensor`类型的 Python 对象；若`validate_shape`没有指明，则此初始值必须指定一个形状；初始值也可以是不带参数的可调用函数，该函数在调用时返回初始值，此时必须指定`dtype`；需要注意的是，若函数为init_ops.py 中的初始化函数，则在这里使用之前必须先绑定一个形状\n",
    "\n",
    "- trainable: `synchronization`被指明`ON_READ`时默认 False，否则默认 True，True时将该变量添加到计算图集合`GraphKeys.TRAINABLE_VARIABLES`中，该集合常被用于`Optimizer`类默认的变量列表，调用`trainable_variables()`函数可以返回此集合的内容\n",
    "\n",
    "- collections: graph collections keys 列表，新创建的变量将会被添加在这些列表中，默认`[GraphKeys.GLOBAL_VARIABLES]`，调用`global_variables()`函数可以返回此集合的内容\n",
    "\n",
    "- validate_shape: `False`时允许变量以未知形状的状态被初始化，否则形状必须指明\n",
    "\n",
    "- caching_device: Optional device string describing where the Variable should be cached for reading.  Defaults to the Variable's device. If not `None`, caches on another device.  Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through `Switch` and other conditional statements.\n",
    "\n",
    "- name: variable's，默认`'Variable'`，且通过添加后缀等方式加以区分\n",
    "\n",
    "- variable_def: `VariableDef` protocol buffer. If not `None`, recreates the Variable object with its contents, referencing the variable's nodes in the graph, which must already exist. The graph is not changed. `variable_def` and the other arguments are mutually exclusive.\n",
    "\n",
    "- dtype: 不指明时，若`initial_value`为`Tensor`，则保留原数据类型，不为`Tensor`则由`convert_to_tensor`结果决定\n",
    "\n",
    "- expected_shape: A TensorShape. 指明时传入的`initial_value`应具有这种形状，否则报错\n",
    "\n",
    "- import_scope: `string`. Name scope to add to the `Variable.` Only used when initializing from protocol buffer.\n",
    "\n",
    "- constraint: 在变量经`Optimizer`更新后作用于其上的投影函数(例如用于实现权重的范数约束或值约束的函数)；该函数的输入值必须是表示变量值的、未经投影的张量，且返回相应投影值的张量必须与输入具有相同的形状；在进行异步分布式训练（asynchronous distributed training）时，使用约束是不安全的。\n",
    "\n",
    "- use_resource: whether to use resource variables.\n",
    "\n",
    "- synchronization: 即同步性，Indicates when a distributed a variable will be aggregated；取值可以是`tf.VariableSynchronization`类定义的常量。默认情况下为`AUTO`，且由当下的`DistributionStrategy`决定何时同步。\n",
    "\n",
    "- aggregation: Indicates how a distributed variable will be aggregated. 取值可以是`tf.VariableAggregation`类定义的常量\n",
    "\n",
    "- shape: None 则使用`initial_value`的形状；若设置为`tf.TensorShape(None)`，即代表一个为指明的形状，则该变量可以被赋予任意形状的值\n",
    "\n",
    "\n",
    "Raises\n",
    "- ValueError: If both `variable_def` and initial_value are specified.\n",
    "- ValueError: If the initial value is not specified, or does not have a shape and `validate_shape` is `True`.\n",
    "- RuntimeError: If eager execution is enabled.\n",
    "\n",
    "**Type**\n",
    "\n",
    "VariableMetaclass\n",
    "\n",
    "在`Variable()`构造函数初始化变量后，变量的类型和形状便是固定的，不过可以 assign 方法修改其取值和形状，若要修改形状，初始化时应指明`validate_shape=False`.\n",
    "\n",
    "`Variable()`创建的变量可以作为计算图中其他操作的输入，又所有为`Tensor`类重载的运算符也适用于变量，进而可以通过直接对变量进行算术符号操作来给计算图添加节点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.Variable(1.0, name=\"x\")\n",
    "x2 = x.assign(x + 1.0)\n",
    "x3 = tf.assign(x, x - 1.0)\n",
    "x4 = tf.assign_add(x, 1.5)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(x))\n",
    "    print(sess.run(x2))\n",
    "    print(sess.run(x3))\n",
    "    print(sess.run(x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`constant`不同，启动计算图时必须显式初始化变量，然后才能运行那些需要其变量值的运算，否则会抛出异常；初始化可以是任意形式的`initializer`方法，或是从已保存的文件中 restore 变量，或运行为变量赋值的`assign`操作；其实变量的`initializer`方法就是一个`assign`操作，它将变量的初始值赋给变量本身："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(tf.random_normal([1]))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(x.initializer)\n",
    "    y = tf.assign(x, [1.0])\n",
    "    x = sess.run(x)\n",
    "    y = sess.run(y)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to create a variable with an initial value dependent on another\n",
    "variable, use the other variable's `initialized_value()`. This ensures that\n",
    "variables are initialized in the right order.\n",
    "\n",
    "**注意！！！**`tf.Variable`对象默认情况下具有非直观的内存模型（non-intuitive memory model），即其实例化的变量在内部以一个可变张量形式存在，该张量可能与计算图中其他张量发生混叠（alias）；那些可能导致混叠的操作是不确定的，且可能随着 TensorFlow 不同而改变！故应避免编写依赖那些可变的或不可变的变量的取值的代码！例如，在`tf.cond`中使用`Variable`对象或其函数作为predicates 易出错：\n",
    "\n",
    "```python\n",
    "v = tf.Variable(True)\n",
    "tf.cond(v, lambda: v.assign(False), my_false_fn)  # Note: this is broken.\n",
    "```\n",
    "\n",
    "Here, adding `use_resource=True` when constructing the variable will\n",
    "fix any nondeterminism issues:\n",
    "```\n",
    "v = tf.Variable(True, use_resource=True)\n",
    "tf.cond(v, lambda: v.assign(False), my_false_fn)\n",
    "```\n",
    "\n",
    "To use the replacement for variables which does\n",
    "not have these issues:\n",
    "\n",
    "* Add `use_resource=True` when constructing `tf.Variable`;\n",
    "* Call `tf.compat.v1.get_variable_scope().set_use_resource(True)` inside a\n",
    "  `tf.compat.v1.variable_scope` before the `tf.compat.v1.get_variable()` call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Session()\n",
    "\n",
    "`tf.Session(target='', graph=None, config=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "`Session`对执行`Operation`、计算`Tensor`的环境进行了封装；程序搭建的过程应为，首先由`tf.Graph`定义计算图（即可理解为流程），它不计算任何值、不包含任何值，它仅仅一种指定的操作；接着通过创建一个`Session`来**执行**`Graph`或`Graph`的一部分，`Session`可以将资源分配到一台或多台机器上，同时能够保存中间结果和变量的实际值\n",
    "\n",
    "**Args**\n",
    "\n",
    "- target: 要连接的执行设备；默认使用正在运行的 engine，更多示例参见 [Distributed TensorFlow](https://tensorflow.org/deploy/distributed)\n",
    "\n",
    "- graph: 要启用的`Graph`；如果在构建会话时没有指定`graph`参数，则启动默认计算图；如果在同一个进程中使用多个由`tf.Graph()`创建的计算图，则每个计算图应使用不同的会话；但原理上每个计算图可以在多个会话中使用，这种情况下指明`graph`可以使程序显得更清楚\n",
    "\n",
    "- config: 具有`Session`的配置选项的 [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto) 协议缓冲区，此参数可定义执行计算的设备数量、并行计算的线程数、GPU 的配置等参数\n",
    "\n",
    "**Type**: type\n",
    "\n",
    "### Examples\n",
    "\n",
    "`ConfigProto`协议缓冲区可以公开会话的各种配置选项；例如创建一个使用软约束（soft constraints）来放置设备的会话，并要求其能够记录结果的放置决策（resulting placement decisions），创建过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                        log_device_placement=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个会话可能含有很多资源，如`tf.Variable`、`tf.queue.QueueBase`、`tf.ReaderBase`；当不再需要这些资源时需要释放它们，进而可通过上下文管理器或``tf.Session.close``方法来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "a = tf.constant(5.0)\n",
    "a.eval()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "\n",
    "# equivalent to:\n",
    "tf.reset_default_graph()\n",
    "a = tf.constant(5.0)\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Session.run()\n",
    "\n",
    "`sess.run(fetches, feed_dict=None, options=None, run_metadata=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "运算`fetches`指定的操作及计算张量\n",
    "\n",
    "This method runs one \"step\" of TensorFlow computation, by running the necessary graph fragment to execute every `Operation` and evaluate every `Tensor` in `fetches`, substituting the values in `feed_dict` for the corresponding input values.\n",
    "\n",
    "The `fetches` argument may be a single graph element, or an arbitrarily\n",
    "nested list, tuple, namedtuple, dict, or OrderedDict containing graph\n",
    "elements at its leaves.  A graph element can be one of the following types:\n",
    "\n",
    "* A `tf.Operation`.\n",
    "  The corresponding fetched value will be `None`.\n",
    "* A `tf.Tensor`.\n",
    "  The corresponding fetched value will be a numpy ndarray containing the\n",
    "  value of that tensor.\n",
    "* A `tf.SparseTensor`.\n",
    "  The corresponding fetched value will be a\n",
    "  `tf.compat.v1.SparseTensorValue`\n",
    "  containing the value of that sparse tensor.\n",
    "* A `get_tensor_handle` op.  The corresponding fetched value will be a\n",
    "  numpy ndarray containing the handle of that tensor.\n",
    "* A `string` which is the name of a tensor or operation in the graph.\n",
    "\n",
    "The value returned by `run()` has the same shape as the `fetches` argument,\n",
    "where the leaves are replaced by the corresponding values returned by\n",
    "TensorFlow.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "   a = tf.constant([10, 20])\n",
    "   b = tf.constant([1.0, 2.0])\n",
    "   # 'fetches' can be a singleton\n",
    "   v = session.run(a)\n",
    "   # v is the numpy array [10, 20]\n",
    "   # 'fetches' can be a list.\n",
    "   v = session.run([a, b])\n",
    "   # v is a Python list with 2 numpy arrays: the 1-D array [10, 20] and the\n",
    "   # 1-D array [1.0, 2.0]\n",
    "   # 'fetches' can be arbitrary lists, tuples, namedtuple, dicts:\n",
    "   MyData = collections.namedtuple('MyData', ['a', 'b'])\n",
    "   v = session.run({'k1': MyData(a, b), 'k2': [b, a]})\n",
    "   # v is a dict with\n",
    "   # v['k1'] is a MyData namedtuple with 'a' (the numpy array [10, 20]) and\n",
    "   # 'b' (the numpy array [1.0, 2.0])\n",
    "   # v['k2'] is a list with the numpy array [1.0, 2.0] and the numpy array\n",
    "   # [10, 20].\n",
    "```\n",
    "\n",
    "The optional `feed_dict` argument allows the caller to override\n",
    "the value of tensors in the graph. Each key in `feed_dict` can be\n",
    "one of the following types:\n",
    "\n",
    "* If the key is a `tf.Tensor`, the\n",
    "  value may be a Python scalar, string, list, or numpy ndarray\n",
    "  that can be converted to the same `dtype` as that\n",
    "  tensor. Additionally, if the key is a\n",
    "  `tf.compat.v1.placeholder`, the shape of\n",
    "  the value will be checked for compatibility with the placeholder.\n",
    "* If the key is a\n",
    "  `tf.SparseTensor`,\n",
    "  the value should be a\n",
    "  `tf.compat.v1.SparseTensorValue`.\n",
    "* If the key is a nested tuple of `Tensor`s or `SparseTensor`s, the value\n",
    "  should be a nested tuple with the same structure that maps to their\n",
    "  corresponding values as above.\n",
    "\n",
    "Each value in `feed_dict` must be convertible to a numpy array of the dtype\n",
    "of the corresponding key.\n",
    "\n",
    "The optional `options` argument expects a [`RunOptions`] proto. The options\n",
    "allow controlling the behavior of this particular step (e.g. turning tracing\n",
    "on).\n",
    "\n",
    "The optional `run_metadata` argument expects a [`RunMetadata`] proto. When\n",
    "appropriate, the non-Tensor output of this step will be collected there. For\n",
    "example, when users turn on tracing in `options`, the profiled info will be\n",
    "collected into this argument and passed back.\n",
    "\n",
    "Args:\n",
    "  fetches: A single graph element, a list of graph elements, or a dictionary\n",
    "    whose values are graph elements or lists of graph elements (described\n",
    "    above).\n",
    "  feed_dict: A dictionary that maps graph elements to values (described\n",
    "    above).\n",
    "  options: A [`RunOptions`] protocol buffer\n",
    "  run_metadata: A [`RunMetadata`] protocol buffer\n",
    "\n",
    "Returns:\n",
    "  Either a single value if `fetches` is a single graph element, or\n",
    "  a list of values if `fetches` is a list, or a dictionary with the\n",
    "  same keys as `fetches` if that is a dictionary (described above).\n",
    "  Order in which `fetches` operations are evaluated inside the call\n",
    "  is undefined.\n",
    "\n",
    "Raises:\n",
    "  RuntimeError: If this `Session` is in an invalid state (e.g. has been\n",
    "    closed).\n",
    "  TypeError: If `fetches` or `feed_dict` keys are of an inappropriate type.\n",
    "  ValueError: If `fetches` or `feed_dict` keys are invalid or refer to a\n",
    "    `Tensor` that doesn't exist.\n",
    "File:      d:\\programmefiles\\python\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\n",
    "Type:      method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    variable = tf.Variable(42, name=\"foo\")\n",
    "    initialize = tf.initialize_all_variables()\n",
    "    assign = variable.assign(13)\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(initialize)\n",
    "    print(sess.run(variable))  # 42\n",
    "    print(sess.run(assign))  # 13\n",
    "    print(sess.run(variable))  # 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.InteractiveSession()\n",
    "`tf.InteractiveSession(target='', graph=None, config=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "创建一个上下文交互式 Tensorflow 会话，与常规`Session`的唯一区别在于，`InteractiveSession`在构造时将自己设置为默认会话，`tf.Tensor.eval`和`tf.Operation.run`等方法将使用该会话来运行`op`；这在交互式 shell 和 [IPython notebooks](http://ipython.org) 中非常方便，因为它避免了要通过传递显式`Session`来运行`op`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- target: 要连接的执行设备；默认使用正在运行的 engine，更多示例参见 [Distributed TensorFlow](https://tensorflow.org/deploy/distributed)\n",
    "\n",
    "- graph: 要启用的`Graph`；如果在构建会话时没有指定`graph`参数，则启动默认计算图；如果在同一个进程中使用多个由`tf.Graph()`创建的计算图，则每个计算图应使用不同的会话；但原理上每个计算图可以在多个会话中使用，这种情况下指明`graph`可以使程序显得更清楚\n",
    "\n",
    "- config: 具有`Session`的配置选项的 [`ConfigProto`](https://www.tensorflow.org/code/tensorflow/core/protobuf/config.proto) 协议缓冲区，此参数可定义执行计算的设备数量、并行计算的线程数、GPU 的配置等参数\n",
    "\n",
    "**File**:           \\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\n",
    "\n",
    "**Type**:           type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "print(c.eval())  # you need to use `c.eval()` under the `with` context or `c.eval(session=sess)` using the tf.Session()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.placeholder()\n",
    "\n",
    "`tf.placeholder(dtype, shape=None, name=None)`\n",
    "\n",
    "__Docstring__\n",
    "\n",
    "为一个要输入的张量提供占位符。该张量直接进行计算时会报错，它的值必须通过`Session.run()`、`Tensor.eval()`或`Operation.run()`的可选参数`feed_dict`可选参数传递；**与即时执行不兼容**\n",
    "\n",
    "**Args**\n",
    "\n",
    "- dtype: 略\n",
    "\n",
    "- shape: 要输入的张量的形状，若未指明，则可以输入任意形状的张量\n",
    "\n",
    "- name: operation's\n",
    "\n",
    "__Type__:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(8, 8))\n",
    "print(x)\n",
    "y = tf.matmul(x, x)\n",
    "print(y)\n",
    "rand_array = np.random.rand(8, 8)\n",
    "with tf.Session() as sess:\n",
    "#     print(sess.run(y)), will raise an error\n",
    "    print(sess.run(y, feed_dict={x: rand_array}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.get_variable()\n",
    "```python\n",
    "tf.get_variable(\n",
    "    name,\n",
    "    shape=None,\n",
    "    dtype=None,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    trainable=None,\n",
    "    collections=None,\n",
    "    caching_device=None,\n",
    "    partitioner=None,\n",
    "    validate_shape=True,\n",
    "    use_resource=None,\n",
    "    custom_getter=None,\n",
    "    constraint=None,\n",
    "    synchronization=<VariableSynchronization.AUTO: 0>,\n",
    "    aggregation=<VariableAggregation.NONE: 0>,\n",
    ")\n",
    "```\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "当指明的`name`的`Variable`已存在时，则该函数用于获得已有变量；若指明的`name`的`Variable`不存在，则该函数用于创建新变量；此外若提供了`partitioner`参数，则创建一个`PartitionedVariable`对象。\n",
    "\n",
    "该函数提供重用检查，同时会在变量名称前加上当前变量作用域名称以区分不同变量\n",
    "\n",
    "注意`tf.get_variable()`要配合`reuse`和[`tf.variable_scope()`](#tf.variable_scope())使用，有关重用 reuse 的工作机制的详细描述，请参阅[Variable Scope How To](https://tensorflow.org/guide/variables)\n",
    "\n",
    "\n",
    "If a partitioner is provided, a `PartitionedVariable` is returned.\n",
    "Accessing this object as a `Tensor` returns the shards concatenated along\n",
    "the partition axis.\n",
    "\n",
    "Some useful partitioners are available.  See, e.g.,\n",
    "`variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n",
    "\n",
    "**Args**\n",
    "\n",
    "- name: 新变量或现有变量的名称；若指明的`name`的`Variable`不存在，则该函数用于创建新变量，when violating reuse during variable creation，会抛出异常；当指明的`name`的`Variable`已存在时，则该函数用于获得已有变量，这种情况下所提供的其他参数应与原变量完全相同，否则会抛出异常；且获取变量时所在的变量作用域应处于 reuse 状态，详见`variable_scope`\n",
    "\n",
    "- shape: 新变量或现有变量的形状，**创建新变量时未指明形状会报错**\n",
    "\n",
    "- dtype: 新变量或现有变量的数据类型，默认`DT_FLOAT`\n",
    "\n",
    "- initializer: 对于已创建的变量，其应是`initializer`对象；对于未创建的变量，该参数也可以是`Tensor`，这种情况下变量被初始化为这个`Tensor`的值，且若`validate_shape`为假，则`Tensor`形状必须是已知的；若`initializer`为`None`，则默认使用变量作用域所传递的`initializer`，若变量作用域`initializer`也为`None`，则使用`glorot_uniform_initializer`；`initializer`的返回值应与`dtype`匹配，否则会抛出异常\n",
    "\n",
    "- regularizer: 一个输入为`Tensor`、输出为`Tensor`或`None`的函数，该函数作用于变量的结果会被添加在`tf.GraphKeys.REGULARIZATION_LOSSES`集合内，进而可用于正则化；若为`None`，则默认使用变量作用域所传递的`regularizer`，若变量作用域`regularizer`也为`None`，则不使用正则化；\n",
    "\n",
    "- trainable: `True`时将此变量添加至计算图集合`GraphKeys.TRAINABLE_VARIABLES`\n",
    "\n",
    "- collections: 此变量要添加到其中的 graph collections keys 列表，默认为`[GraphKeys.GLOBAL_VARIABLES]`\n",
    "\n",
    "- caching_device: Optional device string or function describing where the Variable should be cached for reading.  Defaults to the Variable's device.  If not `None`, caches on another device.  Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through `Switch` and other conditional statements.\n",
    "\n",
    "- partitioner: 一个可调用函数，该函数接收内容为要创建的变量的 fully defined 的`TensorShape`和`dtype`，输出为每个 axis 上的 partition 列表（目前只能 partition 一个轴）\n",
    "\n",
    "- validate_shape: False 时允许用未知形状的值初始化该变量，True 时`initial_value`形状必须已知的，使用此参数时，`initializer`必须是一个`Tensor`而不是`initializer`对象\n",
    "\n",
    "- use_resource: False 时创建一个常规的`Variable`，True 时 creates an experimental ResourceVariable with well-defined semantics；默认为 False，但在之后的版本可能会改为 True；当允许即使执行时，该参数总是被迫为True\n",
    "\n",
    "- custom_getter: 以 true getter 作为第一个参数的可调用对象，并允许其覆盖内部的`get_variable`，`custom_getter`定义时的 signature 应与此方法匹配，然而最新版也允许一些变动：`def custom_getter(getter, *args, **kwargs)`，也允许直接获得所有`get_variable`参数：`def custom_getter(getter, name, *args, **kwargs)`.  A simple identity custom getter that simply creates variables with modified names is:\n",
    "\n",
    "    ```python\n",
    "    def custom_getter(getter, name, *args, **kwargs):\n",
    "        return getter(name + '_suffix', *args, **kwargs)\n",
    "    ```\n",
    "- constraint: 在变量经`Optimizer`更新后作用于其上的投影函数(例如用于实现权重的范数约束或值约束的函数)；该函数的输入值必须是表示变量值的、未经投影的张量，且返回相应投影值的张量必须与输入具有相同的形状；在进行异步分布式训练（asynchronous distributed training）时，使用约束是不安全的\n",
    "\n",
    "- synchronization: Indicates when a distributed a variable will be aggregated. 取值可以是`tf.VariableSynchronization`类定义的常量。默认情况下为`AUTO`，且由当下的`DistributionStrategy`决定何时同步。\n",
    "\n",
    "- aggregation: Indicates how a distributed variable will be aggregated. 取值可以是`tf.VariableAggregation`类定义的常量\n",
    "\n",
    "\n",
    "**Type**\n",
    "\n",
    "function\n",
    "\n",
    "### Example\n",
    "\n",
    "考虑到该函数返回`Variable`对象，故其用于创建新变量时初始化方式与`tf.Variable`类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "w = tf.get_variable(\"w\", shape=[3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数用于获得已有变量时，对于原变量所在的变量作用域，其至少在获取该已有变量时应处于 reuse 状态，否则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.get_variable(\"x\", [1], initializer=tf.constant_initializer(1))\n",
    "x1 = tf.get_variable(\"x\", [1], dtype=tf.float16)\n",
    "\"\"\" Out: \n",
    "    ValueError: Variable x already exists, disallowed.\n",
    "    Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "    x = tf.get_variable(\"x\", [1], initializer=tf.constant_initializer(1.0))\n",
    "    x2 = tf.get_variable(\"x\", [1])\n",
    "x3 = tf.get_variable(\"x\", [1])\n",
    "print(x2 is x, x3 is x)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    x = tf.get_variable(\"x\", [1], initializer=tf.constant_initializer(1.0))\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    x2 = tf.get_variable(\"x\", [1])\n",
    "print(x2 is x)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.get_variable(\"x\", [1])\n",
    "        return x\n",
    "\n",
    "x1 = foo()\n",
    "x2 = foo()\n",
    "assert v1 == v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而变量作用域处于 `reuse=True` 模式时不允许创建新的变量；更多有关 reuse 详见 [here](#tf.variable_scope())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.variable_scope()\n",
    "```python\n",
    "tf.variable_scope(\n",
    "    name_or_scope,\n",
    "    default_name=None,\n",
    "    values=None,\n",
    "    initializer=None,\n",
    "    regularizer=None,\n",
    "    caching_device=None,\n",
    "    partitioner=None,\n",
    "    custom_getter=None,\n",
    "    reuse=None,\n",
    "    dtype=None,\n",
    "    use_resource=None,\n",
    "    constraint=None,\n",
    "    auxiliary_name_scope=True,\n",
    ")\n",
    "```\n",
    "**Docstring**\n",
    "\n",
    "初始化一个上下文管理器，该上下文管理器主要用于定义创建变量的操作，它同时会验证`values`是否来自同一个计算图，并确保该计算图是默认计算图，and pushes a name scope and a variable scope. `variable_scope`允许创建新变量、共享已创建变量\n",
    "\n",
    "更多细节参见[Variable Scope How To](https://tensorflow.org/guide/variables)\n",
    "\n",
    "__Args__\n",
    "\n",
    "- name_or_scope: `string`或`VariableScope`类型，要打开的作用域；若为None，则使用`default_name`，这种情况下，若有同样的`default_name`以前在相同的作用域（即上下文管理器`variable_scope`所在作用域）中使用过，则自动在所给名称上附加`_N`加以区分；\n",
    "\n",
    "- default_name: `name_or_scope`为`None`时的默认名称，这种情况下，若之前有相同的名称被用在了相同的作用域，该名称将通过附加`_N`来区分，若`name_or_scope`不为`None`，则此参数无效\n",
    "\n",
    "- values: 传递给操作函数的`Tensor`列表\n",
    "\n",
    "- initializer: 该作用域中变量的默认初始化器\n",
    "\n",
    "- regularizer: 此作用域中变量的默认正则化方式\n",
    "\n",
    "- caching_device: 此作用域中变量的默认缓存设备\n",
    "\n",
    "- partitioner: 此作用域中变量的默认 partitioner\n",
    "\n",
    "\n",
    "- custom_getter: 此作用域中变量默认的自定义getter\n",
    "\n",
    "- reuse: 可以为`True`、`None`、`tf.compat.v1.AUTO_REUSE`；其取值为`True`时，这个作用域及其子作用域内将进入 reuse 模式；为`tf.compat.v1.AUTO_REUSE`时，可以创建不存在的变量并返回他们；若为`None`，将继承父域的重用标志；当允许即时执行时，除非 an `EagerVariableStore` or template is currently active，否则总是会创建新的变量\n",
    "\n",
    "- dtype: 在此作用域中创建的变量的类型，默认为所传参数`name_or_scope`中的类型，或从父作用域继承的类型\n",
    "\n",
    "- use_resource: False 时所有变量都是常规的`Variable`；If True, experimental ResourceVariables with well-defined semantics will be used instead；默认 False，之后的版本将改为True；当允许即时执行时，这个参数总是为真\n",
    "\n",
    "- constraint: 在变量被`Optimizer`更新后作用域于该变量的投影函数，如用于实现权重的范数约束或值约束的函数；该函数必须以表示变量值的未投影张量作为输入，并返回投影值的张量(必须与输入张量具有相同的形状)。在进行异步分布式训练时，使用约束是不安全的\n",
    "\n",
    "- auxiliary_name_scope: (/ɔːɡˈzɪliəri/，辅助的) `True`时创建一个辅助的 `name scope`；`False`则不创建；默认为`True`；该参数是不可继承的，并且它只在创建时生效一次，故应该使用它来重新进入一个预先声明的variable scope\n",
    "\n",
    "\n",
    "**Type**\n",
    " \n",
    "type\n",
    "\n",
    "\n",
    "### Examples\n",
    "\n",
    "重新进入已创建的 variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\") as vs:\n",
    "    pass\n",
    "with tf.variable_scope(vs, auxiliary_name_scope=False) as vs1:\n",
    "    #恢复原始的 name_scope\n",
    "    with tf.name_scope(vs1.original_name_scope):\n",
    "        v = tf.get_variable(\"v\", [1])\n",
    "        print(v.name)  # \"foo/v:0\"\n",
    "        c = tf.constant([1], name=\"c\")\n",
    "        print(c.name)  # \"foo/c:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦父作用域退出，`default_name`的计数器(counter)将被丢弃；因此当代码重新进入作用域时，所有嵌套的`default_name`计数器都将重新启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"foo\") as vs:\n",
    "    with tf.variable_scope(None, default_name=\"bar\"):\n",
    "        v = tf.get_variable(\"a\", [1])\n",
    "        assert v.name == \"foo/bar/a:0\"\n",
    "    with tf.variable_scope(None, default_name=\"bar\"):\n",
    "        v = tf.get_variable(\"b\", [1])\n",
    "        assert v.name == \"foo/bar_1/b:0\"\n",
    "\n",
    "# reenter\n",
    "with tf.variable_scope(vs):\n",
    "    with tf.variable_scope(None, default_name=\"bar\"):\n",
    "        v = tf.get_variable(\"c\", [1])\n",
    "        assert v.name == \"foo/bar/c:0\"   # Uses bar instead of bar_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variable_scope的 reuse 机制\n",
    "\n",
    "- `reuse=None`或`reuse=False`时只允许创建新的变量，使用`get_variable`获得已有变量会报错\n",
    "\n",
    "- `reuse=True`时，只允许使用`get_variable`获得已存在变量，但可以使用`tf.Variable`创建新的变量，更一般的来讲，设置 reuse 不会影响其他操作的命名，相关讨论见[github#6189](https://github.com/tensorflow/tensorflow/issues/6189)\n",
    "\n",
    "- `reuse=tf.AUTO_REUSE`时，作用域会自动判断获得已有变量或创建新的变量；**但要慎用**，常见 bug 就是本要重用，结果却是重新初始化；有关`get_variable`机制[here](#tf.get_variable())\n",
    "\n",
    "reuse 使用的经典案例即在测试神经网络时，需要使用之前的权重，进而需要 reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"foo\"):\n",
    "    x = tf.get_variable(\"x\", [1])\n",
    "    y1 = tf.Variable(1, name=\"y1\")\n",
    "    \n",
    "    # x1 = tf.get_variable(\"x\", [1])\n",
    "    # => ValueError: Variable foo/x already exists, disallowed.\n",
    "\n",
    "with tf.variable_scope(\"foo\", reuse=True):\n",
    "    y2 = tf.Variable(1, name=\"y2\")\n",
    "    x2 = tf.get_variable(\"x\", [1])\n",
    "    \n",
    "    # x3 = tf.get_variable(\"x3\", [1])\n",
    "    # => ValueError: Variable foo/x3 does not exist, or was not created with tf.get_variable().\n",
    "    \n",
    "print(x2 is x)  # => True\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def foo():\n",
    "    with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
    "        x = tf.get_variable(\"x\", [1])\n",
    "    return x\n",
    "\n",
    "\n",
    "x1 = foo()  # create\n",
    "x2 = foo()  # get a same variable\n",
    "print(x1 is x2)  # => True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过类方法设置 reuse\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"foo\") as scope:\n",
    "    x = tf.get_variable(\"x\", [1])\n",
    "    scope.reuse_variables()\n",
    "    x1 = tf.get_variable(\"x\", [1])\n",
    "print(x1 is x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种设置 reuse 的方法，[参考内容](https://blog.csdn.net/JNingWei/article/details/78124526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(inp, in_channel, out_channel):\n",
    "    with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(name=\"weights\",\n",
    "                                  shape=[2, 2, in_channel, out_channel])\n",
    "        conv = tf.nn.conv2d(input=inp, filter=weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        input_x = tf.placeholder(dtype=tf.float32, shape=[1, 4, 4, 2])\n",
    "        for _ in range(5):\n",
    "            output = func(input_x, 1, 1)\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                _output = sess.run(output,\n",
    "                                   feed_dict={input_x: tf.random_uniform([1, 4, 4, 1], 0, 225)})\n",
    "                print(_output)\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    main()  # 会报错，但目前还不知道为啥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import variable_scope as vs\n",
    "import numpy as np\n",
    "\n",
    "def func(inp, in_channel, out_channel, reuse=False):\n",
    "    if reuse:\n",
    "        vs.get_variable_scope().reuse_variables()\n",
    "    \n",
    "    weights = tf.get_variable(name=\"weights\",\n",
    "                              shape=[2, 2, in_channel, out_channel])\n",
    "    output = tf.nn.conv2d(input=inp, filter=weights, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    with tf.Graph().as_default():\n",
    "        input_x = tf.placeholder(dtype=tf.float32, shape=[1, 4, 4, 2])\n",
    "        for _ in range(5):\n",
    "            output = func(input_x, 1, 1, reuse=(_ != 0))  # 对 reuse 的设置\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                _output = sess.run(output,\n",
    "                                   feed_dict={input_x: tf.random_uniform([1, 4, 4, 1], 0, 225)})\n",
    "                print(_output)\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    main()  # 会报错，但目前还不知道为啥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于在多线程环境中使用变量作用域的注意事项\n",
    "\n",
    "变量作用域是 thread local 的，因此一个线程无法获得另一个线程的当前作用域；此外，当使用`default_name`时，也只在每个线程的基础上生成相应的作用域名称，即不同的线程可以创建名称相同的作用域；但在同一个计算图中，底层变量存储是跨线程共享的，进而如果另一个线程试图创建与前一个线程创建的变量同名的新变量，除非在 reuse 模式，否则会失败；进一步地，每个线程起初都会创建一个空的变量作用域，进而如果希望从主线程中保留一个作用域的名称前缀，应该先捕获主线程的作用域，在其他线程中在重新进入，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "main_thread_scope = tf.get_variable_scope()\n",
    "\n",
    "# Thread's target function:\n",
    "def thread_target_fn(captured_scope):\n",
    "    with variable_scope.variable_scope(captured_scope):\n",
    "        x = tf.get_variable(\"x\", [1])\n",
    "\n",
    "thread = threading.Thread(target=thread_target_fn, args=(main_thread_scope,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 tensorboard 可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable_scope/c1:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"variable_scope\"):\n",
    "    c1 = tf.constant([10.8, 1.2], name=\"c1\")\n",
    "    v1 = tf.Variable(tf.ones([2]), name=\"Var1\")\n",
    "    v2 = tf.get_variable(\"get_var\", shape=[2], initializer=tf.random_normal_initializer(0, 1))\n",
    "\n",
    "v3 = tf.Variable([3.2, 6.4], name=\"Var3\")\n",
    "output = tf.add_n([c1, v1, v2, v3], name=\"add\")\n",
    "\n",
    "writer = tf.summary.FileWriter(\"../009_Tensorboard/graphs/variable_scope\", tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(c1.name)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上操作在 tensorboard 中为\n",
    "<img src=\"./imgs/var_scope.png\">\n",
    "可见`variable_scope`中由`Variable`和`get_variable`创建的变量均仍在`variable_scope`中；此外，尽管常量位于`variable_scope`中，计算图中仍将其表示在了`variable_scope`外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'var_scope/get_var:0' shape=(2,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"var_scope\", reuse=tf.AUTO_REUSE):\n",
    "    v1 = tf.get_variable(\"get_var\", shape=[2], initializer=tf.random_normal_initializer(0, 1))\n",
    "    v2 = tf.get_variable(\"get_var\", shape=[2], initializer=tf.random_normal_initializer(0, 1))\n",
    "    print(v1)\n",
    "    print(v2)\n",
    "\n",
    "# with tf.variable_scope(\"var_scope\", reuse=True):\n",
    "#     v3 = tf.get_variable(\"get_var\", shape=[2], initializer=tf.random_normal_initializer(0, 1))\n",
    "#     print(v3 is v1)\n",
    "#     v4 = tf.Variable(1, name=\"var\")\n",
    "#     v5 = tf.Variable(1, name=\"var\")\n",
    "#     print(v3 is v1)\n",
    "#     print(v4 is v5)\n",
    "#     print(v5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.name_scope()\n",
    "`tf.name_scope(name, default_name=None, values=None)`\n",
    "\n",
    "**Docstring**\n",
    "\n",
    "初始化一个上下文管理器，该上下文管理器主要用于定义 Python 操作，同时验证给定的`values`是否来自同一计算图，并使该计算图成为默认图，and pushes a name scope in that graph，更多细节参见`tf.Graph.name_scope`\n",
    "\n",
    "**Args**\n",
    "\n",
    "- name: 传递给操作函数的名称参数\n",
    "\n",
    "- default_name: `name`为`None`时使用的名称\n",
    "\n",
    "- values: 传递给操作函数的 `Tensor`列表\n",
    "\n",
    "**Type**\n",
    "\n",
    "type\n",
    "\n",
    "### Example\n",
    "\n",
    "定义一个`my_op`的 Python 操作\n",
    "\n",
    "利用 tensorboard 可视化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_cope/get_var1:0\n",
      "var_cope/add0:0\n",
      "name_scope/Var:0\n",
      "name_scope/ones:0\n",
      "name_scope/add1:0\n",
      "get_var1:0\n",
      "get_var3:0\n",
      "add2:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"var_cope\", reuse=tf.AUTO_REUSE):\n",
    "    get_var1 = tf.get_variable(\"get_var1\", shape=[3], initializer=tf.constant_initializer(2.))\n",
    "    add0 = tf.add(get_var1, get_var1, name=\"add0\")\n",
    "    \n",
    "with tf.name_scope(\"name_scope\") as scope:\n",
    "    Var = tf.Variable([1., 2., 3.], name=\"Var\")\n",
    "    get_var2 = tf.get_variable(\"get_var1\", shape=[3])\n",
    "    get_var3 = tf.get_variable(\"get_var3\", shape=[3], initializer=tf.constant_initializer(2.))\n",
    "    ones = tf.ones([3], name=\"ones\")\n",
    "    add1 = tf.add_n([Var, get_var2, ones, get_var3], name=\"add1\")\n",
    "\n",
    "add2 = tf.add_n([add1, add0], name=\"add2\")\n",
    "writer = tf.summary.FileWriter(\"../009_Tensorboard/graphs/name_scope\", tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(add2)\n",
    "    print(get_var1.name)\n",
    "    print(add0.name)\n",
    "    print(Var.name)\n",
    "    print(ones.name)\n",
    "    print(add1.name)\n",
    "    print(get_var2.name)\n",
    "    print(get_var3.name)\n",
    "    print(add2.name)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上操作在 tensorboard 中为\n",
    "\n",
    "<img src=\"./imgs/name_scope.png\">\n",
    "\n",
    "由打印结果可以看出，在`name_scope`作用域中由`get_variable`获得的于`variable_scope`作用域中创建变量并不在`name_scope`作用域中，但也不在`variable_scope`作用域中，`get_var1`和`get_var2`之间仅仅是共享取值与名称；同样在`name_scope`作用域中由`get_variable`创建的变量也不在`name_scope`作用域中；此外，尽管有打印结果可以看出常量位于`name_scope`中，但在计算图中仍将其表示在了`name_scope`外"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variable_scope & name_scope\n",
    "\n",
    "`tf.name_scope()`主要是用于管理命名空间，以使整个模型更加有条理；而`tf.variable_scope()`主要为实现变量共享，其与`tf.get_variable()`来完成变量共享的功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一些总结\n",
    "\n",
    "### Tensorflow 中的 name 与\n",
    "\n",
    "TensorFlow 命名空间中的命名实际上是属于任何 TensorFlow 变量的**真实属性**，而Python 命名空间中的命名只是在脚本运行期间指向 TensorFlow 变量的临时指针；进而在保存和恢复变量时，可以只使用 TensorFlow 变量名称进行操作，因为脚本终止后 Python 命名空间不再存在，但 Tensorflow 命名空间仍然存在于保存的文件中；有关 Python 命名空间[here](http://localhost:8888/notebooks/Help_Viewers/Python_Basic/Namespace%20and%20Scope.ipynb)\n",
    "\n",
    "其中名字后面的’:’之后接数字为EndPoints索引值（An operation allocates memory for its outputs, which are available on endpoints :0, :1, etc, and you can think of each of these endpoints as a Tensor.），通常情况下为0，因为大部分operation都只有一个输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.17977  11.200977]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"input_const\"):\n",
    "    c1 = tf.constant([10.8, 1.2], name=\"c1\")\n",
    "    c2 = tf.constant([2.4, 1.6], name=\"c2\")\n",
    "\n",
    "with tf.variable_scope(\"input_var\"):\n",
    "    v1 = tf.Variable(tf.ones([2]), name=\"v1\")\n",
    "    v2 = tf.get_variable(\"v2\", shape=[2], initializer=tf.random_normal_initializer(0, 1))\n",
    "\n",
    "v3 = tf.Variable([3.2, 6.4], name=\"v3\")\n",
    "output = tf.add_n([c1, c2, v1, v2, v3], name=\"add\")\n",
    "writer = tf.summary.FileWriter(\"../009_Tensorboard/graphs/scopes_and_names_summary1\", tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pass\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(output)\n",
    "    # 可以使用 name 作为索引\n",
    "    print(sess.run(\"add:0\"))\n",
    "    print(sess.run(\"input_var/v1:0\"))\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
