{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import enum\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=10>初始化</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.empty(3, 4, dtype=torch.float)\n",
    "x = torch.zeros(5, 3, dtype=torch.int8)\n",
    "x = torch.ones_like(x)\n",
    "x = torch.ones(3, 4)\n",
    "x = torch.tensor([3, 4, 6], dtype=torch.int32, device=\"cpu\")\n",
    "x = x.new_ones(3, 4)\n",
    "# print(x)\n",
    "x = torch.rand(4, 4)  # U(0, 1)\n",
    "x = torch.randn(4, 4)\n",
    "x = torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: `x = x.new_ones()`\n",
    "\n",
    "new_ones(size, dtype=None, device=None, requires_grad=False) $\\rightarrow$ Tensor\n",
    "\n",
    "By default, the returned Tensor has the same `torch.dtype` and `torch.device` as this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_usqz_1 = torch.unsqueeze(x, dim=1)\n",
    "x_usqz_0 = torch.unsqueeze(x, dim=0)\n",
    "x_usqz_m1 = torch.unsqueeze(x, dim=-1)\n",
    "x_usqz_m2 = torch.unsqueeze(x, dim=-2)\n",
    "# 可以从shape看出不同dim的区别\n",
    "print(x.shape)\n",
    "print(x_usqz_1.size())\n",
    "print(x_usqz_0.shape)\n",
    "print(x_usqz_m1.size())\n",
    "print(x_usqz_m2.size())\n",
    "print(x)\n",
    "print(x_usqz_1)\n",
    "print(x_usqz_0)\n",
    "print(x_usqz_m1)\n",
    "print(x_usqz_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor = torch.FloatTensor(x)  # 转换成float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((3, 4))\n",
    "y = torch.ones_like(x)\n",
    "# print(x + y)\n",
    "# print(torch.add(x, y))\n",
    "# print(x.add_(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.sum()\n",
    "`sumsum(input, dim, keepdim=False, dtype=None)`\n",
    "\n",
    "**参数**\n",
    "\n",
    "- input, dtype: 略\n",
    "- dim: 求和时，求和索引所在的维度标号\n",
    "- keepdim: 为``True``时，输出张量与原张量形状维度相同，否则会比原张量低至少 1 维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones((3, 4, 5))\n",
    "print(torch.sum(x, dim=0, keepdim=True).shape)\n",
    "print(torch.sum(x, dim=1, keepdim=True).shape)\n",
    "\n",
    "print(x.sum((0, 1), keepdim=True))\n",
    "print(x.sum((0, 2), keepdim=True))\n",
    "print(x.sum((1, 0), keepdim=True))\n",
    "print(x.sum((1, 2), keepdim=True))\n",
    "print(x.sum((2, 0), keepdim=True))\n",
    "print(x.sum((2, 1), keepdim=True))\n",
    "\n",
    "print(x.sum(2, keepdim=True).sum(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch & NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_data = np.arange(16).reshape((4, 4))\n",
    "\n",
    "# np to torch\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "\n",
    "# torch to np\n",
    "torch_2_np = torch_data.numpy()\n",
    "\n",
    "print(\n",
    "    \"np_data:\\n\", np_data,\n",
    "    \"\\n\\ntorch_data:\\n\", torch_data,\n",
    "    \"\\n\\ntorch_2_np:\\n\", torch_2_np\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 函数区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch_data = torch_data.flatten()\n",
    "print(\n",
    "    \"np:\\n\", np_data.dot(np_data),\n",
    "    \"\\n\\ntorch:\\n\", torch_data.dot(torch_data)  # Torch的dot只能用于一维数组\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=8>常用函数</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `x = y.view(*shape)`\n",
    "\n",
    "返回一个与 y 数值相同，但形状为指定形状的 tensor x，而 x 必须与 y 的形状和 stride 兼容，Otherwise, `contiguous` needs to be called before the tensor can be\n",
    "viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(range(24)).reshape(3, 8)\n",
    "print(x.view(3, 2, 4))\n",
    "print(x.view(3, 2, 4, 1))\n",
    "print(x.view(-1, 6))  # ‘-1’可自动推断维度\n",
    "print(x.view(1, -1))  # .view(1, n) 返回拉直的二维数组\n",
    "print(x.view(-1))  # 仅有 -1 时返回拉直的一维数组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### `contiguous(memory_format=torch.contiguous_format)`\n",
    "\n",
    "Returns a contiguous in memory tensor containing the same data as `self` tensor. If `self` tensor is already in the specified memory format, this function returns the `self` tensor.\n",
    "\n",
    "Args:\n",
    "    memory_format (class:`torch.memory_format`): the desired memory format of returned Tensor.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.expand()\n",
    "`<tensor>.expand(*sizes)`\n",
    "\n",
    "返回一个由`<tensor>`张成的新张量。\n",
    "\n",
    "当`<tensor>`有一个维度为 1 时，例如 dim=1 的维度上，则可以在 dim=1 的纬度上扩展为更**大**的维度，即原 dim=1 维度值可以由 1 增大至任何整数；此外可以将`<tensor>`扩展为更**高**维的张量，但新张量的 -1, ..., -n, ( n=`len(<tensor>.shape)` )维上的维度值中非 1 的维度值应与 `<tensor>` 的维度中对应值相等\n",
    "\n",
    "当`<tensor>`没有维度为 1 时，其只能被扩展为更**高**维的张量，其原则与上相同；\n",
    "\n",
    "`*sizes`某一维为 -1 时，表示不改变该维的值，当把`<tensor>`扩展为更**高**维的张量时，第一个维数的值不能为 -1；\n",
    "\n",
    "扩展一个张量不会分配新的内存，而只会在现有张量上创建一个新的视图(new view)，在这个视图中通过将`stride`设为0，来将尺寸为1的维度扩展为更大的尺寸。大小为 1 的任何维度都可以扩展为任意值，而不需要分配新的内存，故在对变量进行赋值等操作时，**注意名词绑定的问题**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([17])\n",
    "print(x.expand(7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1]])\n",
    "print(x.expand(7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 3, 4])\n",
    "print(x.expand(7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2, 3, 4])\n",
    "print(x.expand(2, 7, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[2], [3], [4]])\n",
    "print(x.expand(2, 2, 3, 7))  # 必须为 expand(n1, n1, 3, n3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]])\n",
    "print(x.expand(2, 2, 3, 5))  # 必须为 expand(n1, n1, 3, 5)\n",
    "print(x.expand(3, -1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor.expand_as()\n",
    "`expand_as(other)`\n",
    "\n",
    "**说明**\n",
    "\n",
    "返回一个和`other`形状相同的tensor，更多有关``expand``详见`torch.Tensor.expand`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3, 4], [3, 4, 5, 6], [4, 5, 7, 7]])\n",
    "b = torch.tensor([[1, 3, 3, 1], [4, 4, 5, 6], [4, 5, 6, 9]])\n",
    "c = a.eq(b)\n",
    "# c = c.view(-1).float().sum(0, keepdim=True)\n",
    "c = c.view(-1).float()\n",
    "print(type(c))\n",
    "print(c[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA张量\n",
    "可以利用`x.to(<device>)`来制定将张量移动至特定设备上运行\n",
    "\n",
    "以下代码在GPU可用时的预期输出为：\n",
    "```\n",
    "tensor([-0.4181], device='cuda:0')\n",
    "tensor([-0.4181], dtype=torch.float64)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)  # or to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))  # `.to()` can also change dtype together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = [[[[1, 0.23], [4, 0.24]], 0.6],\n",
    "     [[[1, 0.23], [4, 0.24]], 0.8],\n",
    "     [[[1, 0.23], [4, 0.24]], 0.8]]\n",
    "x = np.array(x)\n",
    "print(type(x[:2, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
