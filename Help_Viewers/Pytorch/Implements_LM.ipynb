{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        self.P = self.get_positional_matrix(max_len, num_hiddens)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        assert X.shape[-1] == self.P.shape[-1]\n",
    "        X = X + self.P[:X.shape[-2]]\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "    \n",
    "    def get_positional_matrix(self, max_len, d):\n",
    "        # J = torch.tensor(range(0, d, 2))\n",
    "        # I = torch.tensor(range(max_len)).reshape(-1, 1).expand(max_len, J.shape[0])\n",
    "        # matrix = torch.true_divide(I, 10000**(torch.true_divide(J, d)))\n",
    "        matrix = [[i / 10000**(2*j/d) for j in range(0, d, 2)] for i in range(max_len)]\n",
    "        matrix = torch.tensor(matrix)\n",
    "        P = torch.empty([max_len, d])\n",
    "        P[:, 0::2] = torch.sin(matrix)\n",
    "        P[:, 1::2] = torch.cos(matrix)\n",
    "        return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PFFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self, ffn_num_hiddens, pw_num_outputs, **kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(pw_num_outputs, ffn_num_hiddens)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, pw_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.dense2(X) \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add&Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        X = self.dropout(Y) + X\n",
    "        X = self.ln(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # `query`: (`batch_size`, #queries, `d`)\n",
    "    # `key`: (`batch_size`, #kv_pairs, `d`)\n",
    "    # `value`: (`batch_size`, #kv_pairs, `dim_v`)\n",
    "    def forward(self, query, key, value):\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        \n",
    "        scores = torch.bmm(query, key.transpose(2, 1)) / math.sqrt(query.shape[-1])\n",
    "        attention_weights = self.dropout(self.mask_softmax(scores, dim=-1))\n",
    "        return torch.bmm(attention_weights, value)\n",
    "    \n",
    "    def mask_softmax(self, X, dim=-1):\n",
    "        mask = X.eq(0).int()\n",
    "        X = -1e6 * mask + X * (1 - mask)\n",
    "        return F.softmax(X, dim=dim)\n",
    "            \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=False)  # qW_q^T\n",
    "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=False)  # kW_k^T\n",
    "        self.W_v = nn.Linear(num_hiddens, num_hiddens, bias=False)  # vW_v^T\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=False)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\" For self-attention, `query`, `key`, and `value` shape: (`batch_size`, `seq_len`, `dim`),\n",
    "            where `seq_len` is the length of input sequence.\n",
    "            `valid_len` shape is either (`batch_size`, ) or (`batch_size`, `seq_len`).\"\"\"\n",
    "\n",
    "        key = transpose_qkv(self.W_k(key), self.num_heads)\n",
    "        value = transpose_qkv(self.W_v(value), self.num_heads)\n",
    "        query = transpose_qkv(self.W_q(query), self.num_heads)\n",
    "        \n",
    "        output = self.attention(query, key, value)\n",
    "        output = transpose_output(output, self.num_heads)\n",
    "        return self.W_o(output)\n",
    "\n",
    "\n",
    "def transpose_qkv(X, num_heads):\n",
    "    # `X_in.size() == (batch_size, seq_len, num_hiddens)`\n",
    "    # `X_out.size() == (batch_size * num_heads, seq_len, num_hiddens / num_heads)`\"\"\"\n",
    "    num_heads = int(num_heads)\n",
    "    (batch_size, seq_len, num_hidden) = tuple(X.size())\n",
    "    X = X.view(batch_size, seq_len, num_heads, -1)\n",
    "    X = X.view(batch_size, num_heads, seq_len, -1)\n",
    "    X = X.view(batch_size * num_heads, seq_len, -1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    num_heads = int(num_heads)\n",
    "    (bs_mul_nh, seq_len, nhid_div_nheads) = tuple(X.size())\n",
    "    X = X.view(-1, seq_len, nhid_div_nheads * num_heads)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.attention(X, X, X)\n",
    "        Y = self.addnorm1(X, Y)\n",
    "        Z = self.ffn(Y)\n",
    "        Z = self.addnorm2(Y, Z)\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for _ in range(num_layers):\n",
    "            self.blocks.add_module(\"Encoder\",\n",
    "                                   EncoderBlock(num_hiddens,\n",
    "                                                ffn_num_hiddens,\n",
    "                                                num_heads,\n",
    "                                                dropout))\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X) * math.sqrt(self.num_hiddens)\n",
    "        X = self.pos_encoding(X)  # X.shape == (2, 100, 24)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 100\n",
    "vocab_size = 200\n",
    "dropout = 0.5\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "num_hiddens = 24\n",
    "ffn_num_hiddens = 48\n",
    "encoder = TransformerEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "X = encoder(torch.ones([batch_size, seq_len], dtype=torch.int64))\n",
    "print(X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
